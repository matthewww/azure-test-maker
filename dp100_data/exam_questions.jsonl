{"question": "You are a data scientist working with an Azure Machine Learning workspace. You need to train a machine learning model using a Python script that requires specific libraries including scikit-learn v1.2.0 and pandas v1.5.2. The script will be executed on a compute cluster named \"training-cluster\" as part of a production pipeline.\n\nWhich approach should you take to ensure the script runs successfully with the required dependencies?", "options": {"A": "Install the libraries directly on the compute cluster using pip install commands in your training script", "B": "Create an Azure Machine Learning environment with the required dependencies specified, then reference this environment when submitting the job", "C": "Upload the library files to the workspace's default datastore and import them at runtime", "D": "Use the default Azure Machine Learning environment and install packages using subprocess calls in your script"}, "correct_answer": "B", "explanation": "Azure Machine Learning environments specify software packages, environment variables, and software settings needed to run scripts. Creating an environment with the required dependencies ensures that the necessary packages are installed on any compute target before executing the script, making your code robust and reusable across compute targets. The environment is stored as an image in the Azure Container Registry and can be versioned for consistency across different job runs.", "learning_path": "Explore and configure the Azure Machine Learning workspace", "module": "Explore Azure Machine Learning workspace resources and assets", "additional_notes": "Option A would be unreliable as compute clusters are ephemeral, Option C is not the intended use of datastores, and Option D could lead to inconsistent environments and potential conflicts."}
{"question": "Your organization has an Azure Machine Learning workspace where multiple data science teams work on different projects. You need to assign appropriate access levels to ensure data scientists can train models and register them, but cannot create or delete compute resources or modify workspace settings.\n\nWhich built-in role should you assign to the data scientists?", "options": {"A": "Owner", "B": "Contributor", "C": "AzureML Data Scientist", "D": "AzureML Compute Operator"}, "correct_answer": "C", "explanation": "The AzureML Data Scientist role is specifically designed for data scientists who need to perform all actions within the workspace except for creating or deleting compute resources or editing workspace settings. This role allows them to create and run jobs to train models, register models, and work with data assets while maintaining proper separation of responsibilities.", "learning_path": "Explore and configure the Azure Machine Learning workspace", "module": "Explore Azure Machine Learning workspace resources and assets", "additional_notes": "Owner and Contributor roles provide too broad access, while AzureML Compute Operator is specifically for managing compute resources, not for data science work."}
{"question": "You are setting up data access for an Azure Machine Learning project. Your training data is stored in an Azure Data Lake Storage Gen2 account external to the workspace. You need to enable secure access to this data from your machine learning jobs.\n\nWhat should you create in the Azure Machine Learning workspace?", "options": {"A": "A data asset that directly references the external storage URL", "B": "A datastore that contains the connection information to the Azure Data Lake Storage Gen2", "C": "A compute instance with direct network access to the storage account", "D": "A pipeline component that downloads the data during job execution"}, "correct_answer": "B", "explanation": "A datastore in Azure Machine Learning contains references to Azure data services and stores the connection information securely in Azure Key Vault. Creating a datastore for the external Azure Data Lake Storage Gen2 account provides secure, authenticated access to the data from machine learning jobs without exposing credentials.", "learning_path": "Explore and configure the Azure Machine Learning workspace", "module": "Explore Azure Machine Learning workspace resources and assets", "additional_notes": "Data assets reference specific files/folders but require a datastore for the connection. Direct URL access doesn't provide secure authentication, and downloading data during execution is inefficient."}
{"question": "You need to run a machine learning training script that performs hyperparameter tuning by testing different combinations of learning rates and batch sizes. The script should automatically try multiple parameter combinations and track the results.\n\nWhich type of Azure Machine Learning job should you use?", "options": {"A": "Command job", "B": "Sweep job", "C": "Pipeline job", "D": "Automated Machine Learning job"}, "correct_answer": "B", "explanation": "A Sweep job is specifically designed for hyperparameter tuning scenarios where you want to execute a single script multiple times with different parameter combinations. It automatically manages the parameter search space and tracks results from each run, making it ideal for finding optimal hyperparameters.", "learning_path": "Explore and configure the Azure Machine Learning workspace", "module": "Explore Azure Machine Learning workspace resources and assets", "additional_notes": "Command jobs execute a single script once, Pipeline jobs run multiple connected steps, and Automated ML automatically selects algorithms and parameters for you."}
{"question": "Your team is developing a machine learning solution where different team members work on data preprocessing, model training, and model evaluation steps. You want to create reusable code components that can be shared across projects and combined into workflows.\n\nWhat Azure Machine Learning feature should you use?", "options": {"A": "Jupyter notebooks in the workspace", "B": "Components that can be used in pipelines", "C": "Automated Machine Learning experiments", "D": "Individual training scripts as jobs"}, "correct_answer": "B", "explanation": "Components in Azure Machine Learning are designed for code reusability and sharing across projects. Each component specifies the name, version, code, and environment needed to run the code. Components can be combined into pipelines where each component represents a step in the workflow, such as data preprocessing, model training, or evaluation.", "learning_path": "Explore and configure the Azure Machine Learning workspace", "module": "Explore Azure Machine Learning workspace resources and assets", "additional_notes": "Notebooks are great for development but not for production reusability. Individual scripts don't provide the packaging and versioning benefits of components."}
{"question": "You are deploying an Azure Machine Learning workspace for a production environment. The workspace will store sensitive model artifacts and connection information to external data sources. Which Azure services are automatically created to support security and monitoring requirements?", "options": {"A": "Only Azure Storage Account and Azure Container Registry", "B": "Azure Key Vault, Application Insights, and Azure Storage Account", "C": "Azure Key Vault, Application Insights, Azure Storage Account, and Azure Container Registry", "D": "Only Azure Key Vault and Application Insights"}, "correct_answer": "C", "explanation": "When an Azure Machine Learning workspace is provisioned, four supporting Azure services are automatically created: Azure Storage Account (for files, notebooks, and metadata), Azure Key Vault (for secure secret management), Application Insights (for monitoring predictive services), and Azure Container Registry (created when needed for storing environment images).", "learning_path": "Explore and configure the Azure Machine Learning workspace", "module": "Explore Azure Machine Learning workspace resources and assets", "additional_notes": "All four services work together to provide comprehensive storage, security, monitoring, and containerization capabilities for the workspace."}
{"question": "Your data science team needs a development environment to write and test machine learning code using Jupyter notebooks. The environment should be managed by the workspace and provide access to workspace assets and datastores.\n\nWhich compute option should you choose?", "options": {"A": "Compute cluster", "B": "Kubernetes cluster", "C": "Compute instance", "D": "Serverless compute"}, "correct_answer": "C", "explanation": "Compute instances are designed as development environments and are similar to virtual machines in the cloud, managed by the workspace. They are ideal for running Jupyter notebooks and provide direct access to workspace assets and datastores, making them perfect for interactive development and experimentation.", "learning_path": "Explore and configure the Azure Machine Learning workspace", "module": "Explore Azure Machine Learning workspace resources and assets", "additional_notes": "Compute clusters are for production workloads, Kubernetes clusters are for model deployment, and serverless compute is for automated training jobs without interactive access."}
{"question": "You have created multiple versions of a machine learning model during your experimentation process. You want to deploy version 3 of your model to a production endpoint while keeping track of all previous versions for potential rollback scenarios.\n\nWhat Azure Machine Learning feature enables this model versioning capability?", "options": {"A": "Data assets with version numbers", "B": "Model registration in the workspace with name and version", "C": "Environment versioning with different package specifications", "D": "Component versioning in pipeline workflows"}, "correct_answer": "B", "explanation": "When you register a model in the Azure Machine Learning workspace, you specify both the name and version. This versioning system allows you to track different iterations of your models, deploy specific versions to endpoints, and maintain a history of model evolution for rollback scenarios and auditability.", "learning_path": "Explore and configure the Azure Machine Learning workspace", "module": "Explore Azure Machine Learning workspace resources and assets", "additional_notes": "While other assets can be versioned, model registration specifically addresses the need to track and deploy different versions of trained models in production scenarios."}
{"question": "You are working with the Azure Machine Learning Python SDK v2 to submit training jobs programmatically. You need to establish a connection to your workspace to create and manage assets.\n\nWhich three parameters are required to authenticate and connect to the workspace?", "options": {"A": "workspace_name, location, and display_name", "B": "subscription_id, resource_group, and workspace_name", "C": "subscription_id, location, and workspace_name", "D": "resource_group, workspace_name, and tenant_id"}, "correct_answer": "B", "explanation": "To connect to an Azure Machine Learning workspace using the Python SDK v2, you need three essential parameters: subscription_id (your Azure subscription ID), resource_group (the name of the resource group containing the workspace), and workspace_name (the name of your workspace). These are used with MLClient and DefaultAzureCredential for authentication.", "learning_path": "Explore and configure the Azure Machine Learning workspace", "module": "Explore developer tools for workspace interaction", "additional_notes": "Location and display_name are used when creating a workspace, while tenant_id is handled automatically by DefaultAzureCredential for authentication."}