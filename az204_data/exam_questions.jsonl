{"question": "You are deploying a web application to Azure App Service that requires .NET 8.0 and must run continuously without any downtime. The application experiences variable traffic patterns with peak loads during business hours. You need to ensure the application can scale automatically based on CPU usage while maintaining at least two instances for high availability.\n\nWhich App Service plan tier should you choose?", "options": {"A": "Free tier with manual scaling", "B": "Shared tier with autoscale enabled", "C": "Basic tier (B2) with manual scale-out to 2 instances", "D": "Standard tier (S1) with autoscale rules configured"}, "correct_answer": "D", "explanation": "The Standard tier is the minimum tier that supports autoscaling capabilities. It runs on dedicated compute instances and allows you to configure autoscale rules based on metrics like CPU usage. The Standard tier also supports multiple instances for high availability and runs apps continuously without the limitations of Free and Shared tiers.", "learning_path": "Implement Azure App Service web apps", "module": "Explore Azure App Service", "additional_notes": "Free and Shared tiers run on shared resources with CPU quotas and cannot scale out. Basic tier supports manual scale-out up to 3 instances but does not support autoscaling. Only Standard tier and above support autoscale rules."}
{"question": "Your company is migrating multiple web applications to Azure App Service. To optimize costs, you plan to host several .NET applications on the same App Service plan. One of the applications is resource-intensive and occasionally causes performance issues for other applications sharing the plan.\n\nWhat should you do to isolate the resource-intensive application while maintaining cost efficiency for the other applications?", "options": {"A": "Keep all applications on the same plan and enable Application Insights to monitor performance", "B": "Move the resource-intensive application to a separate App Service plan in the same region", "C": "Upgrade the existing App Service plan to a higher pricing tier", "D": "Configure deployment slots for the resource-intensive application"}, "correct_answer": "B", "explanation": "Moving the resource-intensive application to a separate App Service plan provides compute isolation while allowing the other applications to remain together and share resources cost-effectively. Each App Service plan has its own set of compute resources (VM instances), so separating the applications prevents the resource-intensive app from impacting others. This is the recommended approach when one app needs to be scaled independently or is consuming too many resources.", "learning_path": "Implement Azure App Service web apps", "module": "Explore Azure App Service", "additional_notes": "Application Insights provides monitoring but doesn't solve resource contention issues. Upgrading the tier increases costs for all apps without isolating the problematic application. Deployment slots are for staging deployments, not resource isolation."}
{"question": "You are developing a containerized web application that needs to be deployed to Azure App Service. The application uses a custom Ubuntu-based Docker image stored in Azure Container Registry that includes specific Python libraries and configuration files not available in the built-in App Service images.\n\nWhich deployment approach should you use?", "options": {"A": "Deploy to App Service on Linux using the custom container from Azure Container Registry", "B": "Extract the application code from the container and deploy using Git deployment", "C": "Deploy to App Service on Windows with a custom runtime stack", "D": "Use Web App for Containers on the Shared pricing tier"}, "correct_answer": "A", "explanation": "App Service on Linux supports custom Docker containers from Azure Container Registry or Docker Hub through the Web App for Containers feature. This allows you to deploy your custom Ubuntu-based image with all your specific libraries and configurations intact. The container runs natively on Linux compute resources in App Service, giving you full control over the runtime environment while still benefiting from App Service's platform features like scaling and deployment slots.", "learning_path": "Implement Azure App Service web apps", "module": "Explore Azure App Service", "additional_notes": "Option B would lose the containerization benefits and custom environment. Option C is incorrect because Windows doesn't support Linux containers. Option D is incorrect because the Shared tier doesn't support Linux apps or containers."}
{"question": "Your team is implementing a CI/CD pipeline for an Azure App Service web app. You need to deploy new releases to a staging environment for testing before swapping to production. The solution must ensure that the staging slot is warmed up and ready to handle traffic before the swap, and must allow for quick rollback if issues are detected after deployment.\n\nWhat should you implement?", "options": {"A": "Deploy directly to the production slot using Azure DevOps with manual approval gates", "B": "Create a deployment slot named 'staging', deploy to it, test, then swap with production slot", "C": "Use blue-green deployment with two separate App Service plans", "D": "Deploy to a separate App Service and use Traffic Manager to redirect users"}, "correct_answer": "B", "explanation": "Deployment slots are the recommended solution for this scenario. When you create a staging slot and deploy to it, you can test the new release in an isolated environment. The slot swap operation automatically warms up the staging slot by sending HTTP requests to the root before swapping, ensuring zero downtime. If issues are detected after the swap, you can quickly roll back by swapping again, as the previous production version is now in the staging slot. This feature is available in Standard tier and above.", "learning_path": "Implement Azure App Service web apps", "module": "Explore Azure App Service", "additional_notes": "Option A deploys directly to production without staging. Option C is unnecessarily complex and expensive. Option D uses external routing instead of App Service's built-in slot swap feature, which doesn't provide the same warm-up and instant rollback capabilities."}
{"question": "You need to configure authentication for an Azure App Service web application. The application must allow users to sign in using their organizational Microsoft Entra ID accounts without writing custom authentication code. Anonymous users should be redirected automatically to the Microsoft Entra ID login page.\n\nWhich configuration should you implement?", "options": {"A": "Enable App Service Authentication with Microsoft Entra ID as the identity provider and set 'Require authentication' with redirect to /.auth/login/aad", "B": "Implement custom authentication middleware using Microsoft Authentication Library (MSAL) in your application code", "C": "Configure a reverse proxy in front of App Service to handle authentication", "D": "Enable App Service Authentication with 'Allow unauthenticated requests' and implement authorization in application code"}, "correct_answer": "A", "explanation": "App Service provides built-in authentication and authorization capabilities. By enabling App Service Authentication with Microsoft Entra ID as the identity provider and configuring 'Require authentication', all unauthenticated requests are automatically redirected to /.auth/login/aad for authentication. This is handled by the platform middleware before requests reach your application code, requiring zero code changes. The authentication module validates, stores, and refreshes tokens automatically.", "learning_path": "Implement Azure App Service web apps", "module": "Explore Azure App Service", "additional_notes": "Option B requires writing and maintaining custom authentication code, which is unnecessary. Option C adds complexity with an external component. Option D allows anonymous requests to reach the application, requiring custom authorization code instead of using the platform's capabilities."}
{"question": "Your organization's web application on Azure App Service needs to restrict inbound traffic to only allow access from your corporate network IP ranges (203.0.113.0/24 and 198.51.100.0/24) and Azure Front Door. Outbound traffic to specific Azure Storage accounts must also be controlled.\n\nWhich combination of networking features should you configure?", "options": {"A": "Configure IP restrictions in App Service networking settings for inbound control and use App Service VNet integration for outbound control", "B": "Place the App Service in an App Service Environment (ASE) with Network Security Groups (NSGs)", "C": "Use Azure Firewall to control both inbound and outbound traffic", "D": "Configure service endpoints on the App Service and use Azure Policy for traffic control"}, "correct_answer": "A", "explanation": "IP restrictions (also called access restrictions) in App Service networking settings allow you to define an allow or deny list of IP addresses, IP ranges, and service tags for inbound traffic. For outbound traffic control, VNet integration connects your app to a virtual network, allowing you to use service endpoints, private endpoints, or route tables to control egress traffic to Azure services like Storage. This combination provides fine-grained control over both inbound and outbound traffic without requiring the full isolation and cost of an ASE.", "learning_path": "Implement Azure App Service web apps", "module": "Explore Azure App Service", "additional_notes": "Option B requires App Service Environment which is expensive and provides more isolation than needed. Option C is external to App Service and adds unnecessary complexity. Option D doesn't provide the granular traffic control needed for this scenario."}
{"question": "You are configuring an Azure App Service to host a production web application. The application requires the following environment-specific values: a database connection string, an API key for a third-party service, and a feature flag to enable beta features. These values differ between development, staging, and production environments.\n\nHow should you manage these configuration values?", "options": {"A": "Store all values in the web.config file and deploy different versions to each environment", "B": "Use Application Settings in the App Service configuration for all values, and mark sensitive values as slot settings", "C": "Store values in Azure Key Vault and hard-code the Key Vault references in the application", "D": "Use Azure App Configuration for feature flags and hard-code connection strings in the application"}, "correct_answer": "B", "explanation": "Application Settings in App Service configuration are the recommended way to manage environment-specific values. Connection strings and API keys should be configured as Application Settings, which override values from configuration files. Sensitive values can be marked as 'slot settings' to prevent them from being swapped when deploying between slots. Application Settings are exposed as environment variables to your application at runtime, keeping secrets out of source control. For especially sensitive values like API keys, you can use Key Vault references in the Application Settings.", "learning_path": "Implement Azure App Service web apps", "module": "Configure web app settings", "additional_notes": "Option A stores sensitive data in source control and requires separate deployments. Option C requires code changes for different environments and doesn't leverage App Service's built-in configuration management. Option D splits configuration across multiple systems and still includes hard-coded secrets."}
{"question": "Your web application on Azure App Service uses slot settings for a database connection string that should not be swapped between staging and production slots. After performing a slot swap, you discover that the production slot is still connecting to the staging database.\n\nWhat is the most likely cause?", "options": {"A": "The connection string was marked as a slot setting in the staging slot but not in the production slot", "B": "The application is caching the connection string value from before the swap", "C": "Slot settings only work for Application Settings, not for connection strings", "D": "The swap operation is still in progress and hasn't fully completed"}, "correct_answer": "B", "explanation": "When a slot swap occurs, App Service adds the WEBSITE_SWAP_WARMUP_PING_PATH and WEBSITE_SWAP_WARMUP_PING_STATUSES settings to warm up the slot before swapping. However, if your application caches configuration values (like connection strings) during startup or initialization, it may continue using the old cached value even after new environment variables are set. After a swap with slot settings, the application receives the configuration values from its new slot position, but cached values need to be cleared. You should ensure your application reads connection strings fresh from configuration when needed or restarts properly after swaps.", "learning_path": "Implement Azure App Service web apps", "module": "Configure web app settings", "additional_notes": "Option A is incorrect because slot settings are configured at the setting level, not separately per slot. Option C is incorrect because slot settings work for both Application Settings and connection strings. Option D is incorrect because swaps complete almost instantly."}
{"question": "You need to enable diagnostic logging for an Azure App Service web application to troubleshoot intermittent errors reported by users. The logs must include detailed application trace messages, failed request traces, and web server logs. The solution must retain logs for 7 days and minimize storage costs.\n\nWhich logging configuration should you implement?", "options": {"A": "Enable Application Logging (Filesystem) and Web Server Logging (Filesystem) with a retention period of 7 days", "B": "Enable Application Logging (Blob) and Web Server Logging (Blob) in Azure Storage with a 7-day retention policy", "C": "Enable only Application Insights for all logging requirements", "D": "Enable Application Logging (Filesystem) for application logs and Web Server Logging (Blob) with a 7-day retention policy for web server logs"}, "correct_answer": "D", "explanation": "Application Logging to Filesystem is suitable for detailed application trace messages and is automatically disabled after 12 hours to prevent filling the disk, making it ideal for temporary debugging. However, Web Server Logging and Failed Request Tracing can generate large volumes of data, so storing them in Blob storage with a retention policy is more cost-effective and allows for longer retention periods. This combination provides the detailed logging needed while managing costs and storage appropriately. Failed Request Tracing must also be enabled separately if detailed HTTP trace information is needed.", "learning_path": "Implement Azure App Service web apps", "module": "Configure web app settings", "additional_notes": "Option A retains web server logs on filesystem which has limited space and higher costs. Option B stores application logs in Blob storage which may not be necessary for short-term debugging. Option C using only Application Insights doesn't provide the raw web server logs and failed request traces needed for this scenario."}
{"question": "Your organization requires that all TLS/SSL certificates used by Azure App Service web applications must be stored in Azure Key Vault and automatically renewed. The web application must use a custom domain (www.contoso.com) with HTTPS enabled.\n\nWhat should you configure?", "options": {"A": "Upload a .pfx certificate file directly to App Service and configure a custom domain binding", "B": "Create an App Service Managed Certificate for the custom domain", "C": "Store the certificate in Key Vault, grant the App Service managed identity access, and import the Key Vault certificate into App Service", "D": "Use Azure Front Door with its certificate management and point it to the App Service"}, "correct_answer": "C", "explanation": "To use Key Vault-stored certificates with App Service, you must first ensure the certificate is stored in Key Vault, then grant the App Service's system-assigned or user-assigned managed identity the 'Key Vault Secrets User' role or appropriate Key Vault access policy. You can then import the certificate from Key Vault into App Service, which creates a reference to the Key Vault certificate. When the certificate is updated in Key Vault (manually or through auto-renewal), App Service automatically syncs the new version, providing automatic certificate rotation without application downtime.", "learning_path": "Implement Azure App Service web apps", "module": "Configure web app settings", "additional_notes": "Option A doesn't integrate with Key Vault or support automatic renewal. Option B creates an App Service-managed certificate which isn't stored in your Key Vault. Option D adds unnecessary complexity and doesn't fulfill the requirement to use Key Vault for certificate storage."}
{"question": "You are configuring autoscale for an Azure App Service plan hosting multiple web applications. During business hours (8 AM to 6 PM Monday-Friday), the applications experience high load and require at least 5 instances. Outside business hours, 2 instances are sufficient. The plan should also scale out when average CPU usage exceeds 70% and scale in when it drops below 30%.\n\nWhich autoscale configuration should you implement?", "options": {"A": "Create a single autoscale rule with CPU-based scale-out at 70% and scale-in at 30%, with instance limits of 2-10", "B": "Create two autoscale profiles: a schedule-based profile for business hours (5-10 instances) with CPU rules, and a default profile for off-hours (2-5 instances) with CPU rules", "C": "Create a CPU-based autoscale rule and manually change the instance count at the start and end of each business day", "D": "Enable autoscale with only schedule-based rules for business hours (5 instances) and off-hours (2 instances)"}, "correct_answer": "B", "explanation": "Autoscale profiles allow you to define different scaling behaviors for different time periods. Creating a schedule-based profile that activates during business hours (with minimum 5, maximum 10 instances and CPU-based rules) and a default profile for off-hours (with minimum 2, maximum 5 instances and CPU-based rules) provides both time-based and metric-based scaling. The CPU rules within each profile allow the application to respond to actual load while the profiles ensure appropriate minimum instance counts for each time period. This combination provides optimal resource utilization and cost management.", "learning_path": "Implement Azure App Service web apps", "module": "Scale apps in Azure App Service", "additional_notes": "Option A doesn't account for the different minimum instance requirements during business hours. Option C requires manual intervention which defeats the purpose of autoscale. Option D only uses schedule-based rules without responding to actual CPU load, which could lead to performance issues during unexpected traffic spikes."}
{"question": "Your web application on Azure App Service Standard tier experiences unpredictable traffic spikes that cause performance degradation. You have configured autoscale rules to scale out when average CPU exceeds 75% across all instances. However, by the time new instances are added and warmed up, some user requests have already failed.\n\nWhat should you do to improve the scaling responsiveness?", "options": {"A": "Change the autoscale rule to use a lower CPU threshold of 50%", "B": "Upgrade to Premium tier and configure always-ready instances", "C": "Reduce the autoscale rule's time window from 10 minutes to 5 minutes and decrease the cool-down period", "D": "Configure multiple autoscale rules with different thresholds (scale out at 65%, 75%, and 85%)"}, "correct_answer": "C", "explanation": "Autoscale evaluates rules based on the metrics aggregated over the time window. By reducing the time window, autoscale can react more quickly to traffic changes. The cool-down period prevents flapping by specifying how long to wait after a scale operation before evaluating rules again. Reducing both allows autoscale to respond faster to load increases. Additionally, you should review the scale-out rule to ensure it adds enough instances per scale action (instance count increment). For highly variable traffic, consider using Premium tier with pre-warmed workers, but Standard tier with optimized autoscale settings can still be effective.", "learning_path": "Implement Azure App Service web apps", "module": "Scale apps in Azure App Service", "additional_notes": "Option A may cause premature scaling and increased costs. Option B requires a tier upgrade which increases costs and may not be necessary if autoscale settings are optimized. Option D creates complexity without addressing the timing issue, and may cause conflicting scale operations."}
{"question": "You need to scale in an Azure App Service plan to reduce costs during a planned maintenance window. The plan currently has 5 running instances and you want to reduce to 2 instances. However, the application should not terminate any in-progress HTTP requests during the scale-in operation.\n\nWhat behavior can you expect from Azure App Service during scale-in?", "options": {"A": "App Service immediately terminates instances, which may interrupt in-progress requests", "B": "App Service provides a 60-minute grace period for instances to complete in-progress requests before shutting down", "C": "App Service waits indefinitely for all requests to complete before scaling in", "D": "App Service only scales in instances that have no active connections"}, "correct_answer": "B", "explanation": "When scaling in, Azure App Service provides a grace period of 60 minutes for Premium plan instances (and 10 minutes for Dedicated plan instances during platform updates) to allow them to complete in-progress requests before shutdown. During this period, the load balancer stops routing new requests to the instances being removed, but existing requests are allowed to complete. If requests don't complete within the grace period, the instances are terminated. This behavior helps minimize disruption during scale-in operations. For long-running operations that exceed this grace period, consider using Azure Functions with Durable Functions or implementing request cancellation handling.", "learning_path": "Implement Azure App Service web apps", "module": "Scale apps in Azure App Service", "additional_notes": "Option A is incorrect because App Service does provide a grace period. Option C is incorrect as there is a finite grace period. Option D is incorrect because scale-in is time-based with a grace period, not connection-based."}
{"question": "Your production web application is deployed to Azure App Service with a deployment slot named 'staging'. You want to test a new version by routing 20% of production traffic to the staging slot to validate performance and monitor for errors before completing the full deployment.\n\nWhich feature should you use?", "options": {"A": "Perform a slot swap with the 'Swap with preview' option", "B": "Configure traffic routing percentage in the deployment slot settings to route 20% to staging", "C": "Use Azure Traffic Manager with weighted routing", "D": "Create a custom routing rule in Application Gateway"}, "correct_answer": "B", "explanation": "App Service deployment slots support traffic routing (also called testing in production) that allows you to route a percentage of user traffic to non-production slots. You can configure this directly in the Azure portal by setting the traffic percentage for each slot. When configured for 20% to staging, the App Service load balancer automatically routes 20% of requests to the staging slot while 80% go to production. Users are then pinned to their assigned slot through a cookie (x-ms-routing-name), ensuring a consistent experience. This allows you to test new features with real production traffic before fully deploying.", "learning_path": "Implement Azure App Service web apps", "module": "Explore Azure App Service deployment slots", "additional_notes": "Option A swaps the slots which would deploy staging to all users, not just 20%. Option C uses external routing which requires additional resources and doesn't leverage App Service's built-in capability. Option D requires Application Gateway which adds complexity and cost when the feature is available natively in App Service."}
{"question": "You are about to perform a deployment slot swap from staging to production for an Azure App Service. The application in staging uses different application settings and connection strings than production. You need to ensure that after the swap, each slot maintains its own specific configuration values for database connections and API endpoints.\n\nWhat should you configure before performing the swap?", "options": {"A": "Mark the application settings and connection strings as 'deployment slot settings' (slot settings)", "B": "Create a backup of the production slot before swapping", "C": "Disable the application settings in staging before performing the swap", "D": "Use the 'Swap with preview' option and manually copy settings between slots"}, "correct_answer": "A", "explanation": "Deployment slot settings (also called 'sticky' settings) are configuration values that remain with the slot during a swap operation. By marking specific application settings and connection strings as slot settings, you ensure they don't move with the application code during a swap. For example, if production has a connection string to the production database marked as a slot setting, it will remain with the production slot even after swapping in the code from staging. This allows each environment to maintain its own configuration while the application code moves between slots.", "learning_path": "Implement Azure App Service web apps", "module": "Explore Azure App Service deployment slots", "additional_notes": "Option B creates a backup but doesn't prevent configuration values from swapping. Option C would break the application by removing necessary settings. Option D is a manual process that is error-prone and doesn't use the built-in slot settings feature designed for this purpose."}
{"question": "Your team performs a deployment slot swap from staging to production. Shortly after the swap, users report errors accessing the application. You need to quickly rollback to the previous version while you investigate the issue.\n\nWhat should you do?", "options": {"A": "Perform another slot swap from production back to staging", "B": "Redeploy the previous version of the application code from source control to production", "C": "Stop the production slot and restart it to reload the previous configuration", "D": "Delete the production slot and recreate it from a backup"}, "correct_answer": "A", "explanation": "After a slot swap, the previous production version is now running in the staging slot. To rollback, you simply perform another swap operation, which immediately moves the previous production version back to the production slot. This is near-instantaneous (typically completing in seconds) because both versions are already deployed and warmed up. This is one of the key benefits of using deployment slots - they provide instant rollback capability. The swap operation also performs the warm-up process to ensure the rolled-back version is ready to handle traffic before completing the swap.", "learning_path": "Implement Azure App Service web apps", "module": "Explore Azure App Service deployment slots", "additional_notes": "Option B requires redeployment which takes time and may require building and packaging the application again. Option C doesn't change the deployed application version. Option D is destructive, causes downtime, and is unnecessarily complex when slot swap provides instant rollback."}
{"question": "You have deployed a web application to Azure App Service with a staging slot. The application connects to Azure SQL Database and Azure Storage. Before swapping to production, you want to validate that the staging slot will function correctly with the production configurations without actually performing the swap.\n\nWhich swap option should you use?", "options": {"A": "Standard slot swap", "B": "Swap with preview (multi-phase swap)", "C": "Auto swap", "D": "Traffic routing to staging slot"}, "correct_answer": "B", "explanation": "Swap with preview is a multi-phase swap operation that allows you to validate your application with production settings before completing the swap. During phase 1, production slot settings (non-slot settings) are applied to the staging slot and the staging instances are warmed up. You can then test the staging slot with these production settings to ensure everything works correctly. If validation succeeds, you complete phase 2 to finish the swap. If issues are found, you can cancel the swap and the staging slot reverts to its original settings. This provides a safe validation step before exposing the new version to production traffic.", "learning_path": "Implement Azure App Service web apps", "module": "Explore Azure App Service deployment slots", "additional_notes": "Option A performs an immediate swap without validation opportunity. Option C automatically swaps after deployment, which doesn't allow for validation. Option D routes some traffic but doesn't apply production settings to staging for validation purposes."}
{"question": "Your organization wants to implement continuous deployment from GitHub to an Azure App Service staging slot. After successful deployment to staging, the code should be automatically tested, and if tests pass, automatically swapped to production. The solution must integrate with the existing GitHub Actions workflow.\n\nWhich deployment feature should you configure?", "options": {"A": "Enable auto swap on the staging slot to production", "B": "Configure continuous deployment from GitHub and use a GitHub Actions workflow to perform the swap", "C": "Use Azure DevOps instead of GitHub for CI/CD capabilities", "D": "Configure a webhook from GitHub to trigger Azure Logic Apps for deployment and testing"}, "correct_answer": "B", "explanation": "While App Service supports auto swap feature, the recommended approach for CI/CD with validation is to use GitHub Actions workflow that deploys to staging, runs automated tests, and then performs the slot swap using the Azure CLI or Azure App Service action if tests pass. This provides full control over the deployment pipeline, including running tests and implementing approval gates if needed. The GitHub Actions workflow can use the azure/webapps-deploy action to deploy and azure/CLI action with 'az webapp deployment slot swap' command to perform the swap after successful testing.", "learning_path": "Implement Azure App Service web apps", "module": "Explore Azure App Service deployment slots", "additional_notes": "Option A (auto swap) swaps automatically after deployment without running tests. Option C abandons the existing GitHub workflow unnecessarily. Option D adds unnecessary complexity with Logic Apps when GitHub Actions already supports this scenario natively."}
{"question": "You need to deploy an Azure App Service that can connect to Azure SQL Database using managed identity. The application must not store any database credentials in configuration files or environment variables. The database is configured to allow Microsoft Entra ID authentication.\n\nWhat should you configure?", "options": {"A": "Store the database connection string with username and password in App Service application settings", "B": "Enable system-assigned managed identity on the App Service, grant it access to the database, and use the managed identity in the connection string", "C": "Create a service principal, store its credentials in Azure Key Vault, and reference the Key Vault secret in the connection string", "D": "Use SQL authentication with the sa account and store the password as an App Service secret"}, "correct_answer": "B", "explanation": "Managed identities provide Azure resources with an automatically managed identity in Microsoft Entra ID. By enabling system-assigned managed identity on the App Service, the service gets an identity that can be granted access to the Azure SQL Database. The application code then uses the managed identity token to authenticate to the database without storing credentials. The connection string can specify 'Authentication=Active Directory Managed Identity' or 'Authentication=Active Directory Default' (which uses managed identity when available). This eliminates credential management and follows the principle of least privilege.", "learning_path": "Implement Azure App Service web apps", "module": "Configure web app settings", "additional_notes": "Option A stores credentials which violates the requirement. Option C still involves managing credentials through the service principal, though Key Vault improves security. Option D uses SQL authentication with a highly privileged account and stores secrets, which is a security anti-pattern."}
{"question": "Your company has an Azure App Service plan in the East US region running at maximum capacity with three web applications. You need to add a fourth web application that requires resources in the West Europe region for data residency compliance.\n\nWhat should you do?", "options": {"A": "Add the new web application to the existing App Service plan in East US", "B": "Scale up the existing App Service plan to a higher tier to accommodate the fourth application", "C": "Create a new App Service plan in the West Europe region for the fourth application", "D": "Create the app in East US and use Azure Traffic Manager to route European traffic through West Europe"}, "correct_answer": "C", "explanation": "An App Service plan is region-specific - it defines compute resources in a specific Azure region. You cannot deploy apps to different regions using the same App Service plan. To deploy an application to West Europe, you must create a new App Service plan in that region. Each App Service plan provisions compute resources (VMs) in its designated region, so applications in that plan run on those regional resources. This architecture also ensures data residency compliance by keeping application execution and data in the required geographic location.", "learning_path": "Implement Azure App Service web apps", "module": "Explore Azure App Service", "additional_notes": "Option A is impossible because the plan is in East US. Option B doesn't solve the regional requirement. Option D doesn't meet data residency compliance since the app still runs in East US with only traffic routing through West Europe."}
{"question": "You are developing a serverless application on Azure that needs to process messages from an Azure Service Bus queue. The processing function should be triggered automatically whenever a new message arrives. The solution must minimize infrastructure management and automatically scale based on queue depth.\n\nWhich Azure service should you use?", "options": {"A": "Azure Logic Apps with Service Bus connector", "B": "Azure Functions with Service Bus trigger on a Consumption plan", "C": "Azure WebJobs on an App Service Dedicated plan", "D": "Azure App Service with a background worker process"}, "correct_answer": "B", "explanation": "Azure Functions with a Service Bus trigger provides native integration to automatically process queue messages as they arrive. The Consumption plan offers true serverless capabilities with automatic scaling based on queue depth (number of messages) and pay-per-execution pricing with no charges when idle. The Functions runtime handles the queue polling, message retrieval, and automatic scaling, allowing you to focus on the processing logic. The Service Bus trigger also provides built-in poison message handling and message lock renewal for long-running operations.", "learning_path": "Implement Azure Functions", "module": "Explore Azure Functions", "additional_notes": "Option A provides workflow orchestration but is designer-first and may be more complex than needed for simple message processing. Option C and D require managing App Service infrastructure and don't provide the same serverless, event-driven scaling capabilities."}
{"question": "Your organization has multiple Azure Functions apps running on Consumption plans. One function app processes critical business transactions and must run continuously with predictable performance and sub-second response times. Another function app is causing high costs due to millions of small, frequent executions.\n\nWhich hosting plan changes should you recommend?", "options": {"A": "Move both function apps to Premium plan with always-ready instances", "B": "Keep both apps on Consumption plan and optimize the code to reduce execution time", "C": "Move the critical function app to Premium plan with always-ready instances, and keep the high-volume app on Consumption plan", "D": "Move both function apps to a Dedicated (App Service) plan for predictable costs"}, "correct_answer": "C", "explanation": "Different workloads have different hosting requirements. The critical function app benefits from the Premium plan's always-ready instances which eliminate cold starts and provide predictable performance through pre-warmed workers that run continuously. The high-volume app with many small executions is ideal for Consumption plan's pay-per-execution model, where you only pay for actual execution time. Moving it to Premium or Dedicated plan would increase costs since you'd pay for continuous compute regardless of execution patterns. This hybrid approach optimizes both performance and cost.", "learning_path": "Implement Azure Functions", "module": "Explore Azure Functions", "additional_notes": "Option A increases costs for the high-volume app unnecessarily. Option B doesn't address the performance requirements for the critical app. Option D may provide cost predictability but doesn't offer the performance optimization needed for the critical app and would likely cost more overall."}
{"question": "You have deployed an Azure Function App with multiple functions on a Consumption plan. The function app includes an HTTP-triggered function that occasionally takes 8 minutes to complete processing large data files. Users report that the function times out before completing.\n\nWhat should you do to resolve this issue?", "options": {"A": "Increase the functionTimeout value in host.json to 10 minutes on the Consumption plan", "B": "Move the function app to a Premium plan and configure functionTimeout to unbounded", "C": "Split the long-running function into smaller functions that each process part of the data", "D": "Implement retry logic in the client application to handle timeouts"}, "correct_answer": "B", "explanation": "The Consumption plan has a maximum timeout of 10 minutes for function execution, and HTTP-triggered functions have an additional Azure Load Balancer constraint of 230 seconds (about 4 minutes) for responding to requests. For processing that takes 8 minutes, you need to move to a Premium or Dedicated plan where the functionTimeout can be set to unbounded. Additionally, for long-running HTTP operations, you should implement the asynchronous pattern where the function immediately returns with a status URL that clients can poll for completion. Consider using Durable Functions for orchestrating long-running processes.", "learning_path": "Implement Azure Functions", "module": "Explore Azure Functions", "additional_notes": "Option A won't work because even though Consumption plan supports up to 10 minutes, HTTP functions are limited to 230 seconds by the load balancer. Option C may not be practical depending on the data processing requirements. Option D doesn't solve the timeout issue, just handles the failure."}
{"question": "Your Azure Function App on a Consumption plan in the West US region is experiencing throttling during peak loads. The function processes events from Event Hub and occasionally you see the message 'Function host is not running' when checking the platform metrics. The function app has exceeded 100 instances in a single region.\n\nWhat is the most likely cause and solution?", "options": {"A": "The function code is inefficient; optimize the code to reduce execution time", "B": "The Consumption plan has reached its maximum instance count of 100 for Linux apps; distribute load across multiple regions", "C": "The function app is out of memory; increase the memory allocation in the plan settings", "D": "The Event Hub is sending too many events; reduce the event throughput"}, "correct_answer": "B", "explanation": "On the Consumption plan, Linux function apps can scale to a maximum of 100 instances per region, while Windows can scale to 200 instances. Once this limit is reached, new instances cannot be created and the function may not be able to process all incoming events, leading to throttling. The solution is to either deploy the function app to multiple regions for geographic distribution of load, or migrate to a Flex Consumption or Premium plan which offers higher scale limits. The 'Function host is not running' message can appear when the platform is at maximum scale and cannot create additional instances.", "learning_path": "Implement Azure Functions", "module": "Explore Azure Functions", "additional_notes": "Option A may help reduce the need for instances but doesn't address the scaling limit. Option C is not possible as Consumption plan has fixed memory per instance. Option D addresses the symptom rather than the architectural scaling limitation."}
{"question": "You are comparing Azure Functions and Azure Logic Apps for a workflow that needs to integrate with multiple SaaS applications (Salesforce, ServiceNow, and Office 365) to automate a business approval process. The workflow requires conditional logic, parallel branches, and loops based on data from these services. Your team has limited coding experience but strong process design skills.\n\nWhich service should you choose?", "options": {"A": "Azure Functions because it provides better performance and scalability", "B": "Azure Logic Apps because it offers a designer-first approach with extensive SaaS connectors", "C": "Azure Functions with Durable Functions extension for orchestration capabilities", "D": "Azure WebJobs because it integrates well with App Service"}, "correct_answer": "B", "explanation": "Azure Logic Apps is designed for this exact scenario - it provides a designer-first (declarative) approach with a visual designer in the Azure portal, making it accessible to users without deep coding experience. Logic Apps includes a large collection of pre-built connectors for SaaS applications like Salesforce, ServiceNow, and Office 365, eliminating the need to write integration code. The workflow designer supports conditionals, loops, parallel branches, and other control flow patterns through a visual interface. For business process automation with multiple service integrations, Logic Apps is generally the better choice over code-first Azure Functions.", "learning_path": "Implement Azure Functions", "module": "Explore Azure Functions", "additional_notes": "Option A (Functions) requires coding skills for integration logic. Option C (Durable Functions) is code-first and more complex for users without coding experience. Option D (WebJobs) doesn't provide the same level of SaaS integration or visual design capabilities."}
{"question": "You need to create an Azure Function that triggers whenever a new blob is uploaded to a specific container in Azure Storage named 'invoices'. The function should read the blob content, extract data, and write the results to an Azure Cosmos DB collection.\n\nWhich triggers and bindings should you configure in the function.json file?", "options": {"A": "Trigger: HTTP, Input binding: Blob Storage, Output binding: Cosmos DB", "B": "Trigger: Blob Storage, Input binding: None (read blob in code), Output binding: Cosmos DB", "C": "Trigger: Blob Storage, Input binding: Blob Storage, Output binding: Cosmos DB", "D": "Trigger: Timer, Input binding: Blob Storage, Output binding: Cosmos DB"}, "correct_answer": "C", "explanation": "The Blob Storage trigger automatically detects new blobs in the specified container and invokes the function. While the trigger provides blob metadata, using a Blob Storage input binding allows the function to automatically receive the blob content as a parameter, simplifying the code. The Cosmos DB output binding enables the function to write to Cosmos DB by simply setting values on a binding parameter, without needing to manually create a Cosmos DB client. Bindings handle the connection management, serialization, and resource disposal, allowing you to focus on the business logic.", "learning_path": "Implement Azure Functions", "module": "Develop Azure Functions", "additional_notes": "Option A uses HTTP trigger which doesn't automatically respond to blob uploads. Option B works but doesn't leverage input bindings to simplify blob reading. Option D uses Timer trigger which polls on a schedule rather than responding to blob events."}
{"question": "Your Azure Function needs to connect to an Azure SQL Database. You want to follow best practices by using managed identity for authentication instead of connection strings with passwords. The function is written in C# and uses dependency injection.\n\nHow should you configure the database connection?", "options": {"A": "Store the connection string with SQL authentication in Azure Key Vault and use Key Vault references in application settings", "B": "Enable system-assigned managed identity on the Function App, configure the connection string with Authentication=Active Directory Default, and use Azure.Identity library", "C": "Use environment variables to pass SQL username and password to the function at runtime", "D": "Create a service principal and store its credentials in the function's local.settings.json file"}, "correct_answer": "B", "explanation": "Enabling system-assigned managed identity on the Function App gives it an identity in Microsoft Entra ID that can be granted access to Azure SQL Database. The connection string should specify 'Authentication=Active Directory Default' (or 'Active Directory Managed Identity'), which automatically uses the function app's managed identity. The Azure.Identity library (via DefaultAzureCredential) handles acquiring the token automatically. This approach eliminates stored credentials entirely and follows security best practices. The managed identity should be granted appropriate database permissions using SQL commands or Azure RBAC.", "learning_path": "Implement Azure Functions", "module": "Develop Azure Functions", "additional_notes": "Option A improves security by using Key Vault but still relies on SQL authentication with passwords. Option C stores credentials insecurely in environment variables. Option D stores credentials in a file which is insecure and won't work when deployed to Azure."}
{"question": "You are developing an Azure Function locally using Visual Studio Code and the Azure Functions Core Tools. The function connects to Azure Storage and Azure Service Bus using connection strings. You need to configure these connection strings for local development without committing them to source control.\n\nWhere should you store these configuration values?", "options": {"A": "In the function.json file in the function project", "B": "In the host.json file in the project root", "C": "In the local.settings.json file in the project root", "D": "In the appsettings.json file in the project root"}, "correct_answer": "C", "explanation": "The local.settings.json file is specifically designed for local development settings in Azure Functions. It stores app settings and connection strings that are used when running functions locally using Azure Functions Core Tools. This file should be added to .gitignore to prevent committing secrets to source control. When you publish the function to Azure, settings from local.settings.json are not automatically deployed - you must manually configure them as Application Settings in the Azure Function App. The local.settings.json file mirrors the structure of Application Settings in Azure, making it easy to sync settings between local and cloud environments.", "learning_path": "Implement Azure Functions", "module": "Develop Azure Functions", "additional_notes": "Option A (function.json) defines function configuration like bindings, not connection strings. Option B (host.json) contains runtime configuration, not secrets. Option D (appsettings.json) is used in ASP.NET Core applications but not the standard approach for Azure Functions local development."}
{"question": "Your Azure Function App contains 15 functions written in Python. All functions share common configuration including the default timeout duration, logging level, and retry policies. You need to configure a function timeout of 15 minutes for all functions in the app.\n\nWhere should you configure this setting?", "options": {"A": "In each individual function.json file", "B": "In the host.json file at the project root", "C": "In the Application Settings of the Function App in the Azure portal", "D": "In the local.settings.json file"}, "correct_answer": "B", "explanation": "The host.json file is a metadata file that contains global configuration options that affect all functions in a function app. The functionTimeout property in host.json specifies the timeout duration for all functions in the app. Settings in host.json apply equally to all functions, making it the appropriate place for app-wide configuration like timeouts, logging levels, and retry policies. The host.json file is deployed with your function app code and should be in source control. Changes to host.json require redeploying or restarting the function app.", "learning_path": "Implement Azure Functions", "module": "Develop Azure Functions", "additional_notes": "Option A would require configuring each function individually which is inefficient and error-prone. Option C (Application Settings) is for connection strings and app-specific settings, not runtime configuration. Option D is only for local development settings, not runtime behavior."}
{"question": "You need to implement an Azure Function that processes orders from a queue and sends confirmation emails. The function should automatically retry failed attempts with exponential backoff up to 3 times. If all retries fail, the message should be moved to a poison queue for manual investigation.\n\nHow should you configure this behavior?", "options": {"A": "Implement retry logic manually in the function code with try-catch blocks and loop counter", "B": "Configure maxDequeueCount in host.json for the queue trigger to 3, which automatically handles poison messages", "C": "Use Durable Functions with automatic retry policies", "D": "Configure an Azure Logic App to handle retries and call the function"}, "correct_answer": "B", "explanation": "Azure Functions queue triggers provide built-in poison message handling through the maxDequeueCount setting in host.json. When a message processing fails, the queue trigger automatically retries the function execution. After the number of retries exceeds maxDequeueCount (set to 3 in this case), the message is automatically moved to a poison queue named {original-queue-name}-poison. This built-in behavior eliminates the need for custom retry logic and ensures failed messages don't block queue processing. The exponential backoff is handled automatically by the Azure Storage Queue visibility timeout mechanism.", "learning_path": "Implement Azure Functions", "module": "Develop Azure Functions", "additional_notes": "Option A requires manual implementation and doesn't leverage the platform's built-in capabilities. Option C is more complex than necessary for this scenario. Option D adds unnecessary external orchestration when the capability is built into Functions."}
{"question": "Your company has an Azure Function App that must be deployed across multiple Azure regions for high availability and disaster recovery. Each region has its own Azure SQL Database instance. The function must connect to the database in its local region.\n\nHow should you configure the database connection strings?", "options": {"A": "Store all regional connection strings in host.json and use conditional logic in the function code to select the correct one based on region", "B": "Create separate Function Apps for each region with region-specific Application Settings for the connection string", "C": "Use Azure Traffic Manager to route requests and have the function detect the region using an HTTP header", "D": "Store connection strings in a global Azure Key Vault and use region tags to filter the correct one"}, "correct_answer": "B", "explanation": "The recommended approach for multi-region deployment is to create separate Function App instances in each region, each with its own Application Settings containing the region-specific connection string. This ensures each function instance connects to its local database and provides true regional isolation. You can deploy the same function code to all regions using CI/CD pipelines, while managing region-specific configuration through Application Settings. This architecture also supports independent scaling and configuration per region, and simplifies failover scenarios using services like Azure Front Door or Traffic Manager at the HTTP layer.", "learning_path": "Implement Azure Functions", "module": "Develop Azure Functions", "additional_notes": "Option A clutters the code with infrastructure concerns and doesn't scale well. Option C relies on external routing and makes functions dependent on HTTP headers. Option D is complex and requires additional logic to determine the region and retrieve the correct connection string."}
{"question": "You are developing an HTTP-triggered Azure Function that needs to return a response to the client immediately while continuing to process data asynchronously in the background. The background processing may take 10-15 minutes.\n\nWhich pattern should you implement?", "options": {"A": "Return HTTP 202 Accepted immediately with a status URL, and use Durable Functions to handle the long-running workflow", "B": "Increase the functionTimeout in host.json to 20 minutes and process everything synchronously", "C": "Use an async/await pattern in the function code to process in the background while keeping the HTTP connection open", "D": "Return success immediately and use a Timer-triggered function to process the data later"}, "correct_answer": "A", "explanation": "The HTTP 202 Accepted pattern with Durable Functions is the recommended approach for long-running operations triggered by HTTP requests. The function immediately returns HTTP 202 with a status URL (provided by Durable Functions' built-in HTTP APIs) that clients can poll to check completion status. Durable Functions orchestrations can run for extended periods without timeout constraints and survive function app restarts. This pattern decouples the HTTP response from the processing workflow, prevents timeout issues with the Azure Load Balancer (230-second limit for HTTP functions), and provides built-in status tracking and history.", "learning_path": "Implement Azure Functions", "module": "Develop Azure Functions", "additional_notes": "Option B violates the HTTP load balancer constraint and keeps connections open too long. Option C doesn't actually allow the HTTP request to complete because async operations within the same execution context still block the response. Option D loses the connection between the request and processing, making status tracking difficult."}
{"question": "Your Azure Function App is using a Consumption plan and you notice cold start latency of 3-5 seconds when the function hasn't been invoked recently. The function is customer-facing and requires sub-second response times consistently. Budget is a consideration but performance is the priority.\n\nWhat should you do?", "options": {"A": "Optimize the function code to reduce initialization time and package size", "B": "Migrate to a Premium plan with at least 1 always-ready instance configured", "C": "Migrate to a Dedicated (App Service) plan and enable Always On", "D": "Increase the memory allocation for the Consumption plan to reduce cold starts"}, "correct_answer": "B", "explanation": "Cold starts occur when a function app hasn't been invoked recently and the platform needs to provision and initialize a new instance. The Premium plan eliminates cold starts by maintaining pre-warmed worker instances that are always ready to handle requests immediately. By configuring always-ready instances (minimum instance count), you ensure that at least that number of instances are continuously running and warmed up. This provides consistent sub-second response times while still allowing automatic scaling for additional load. Premium plan also offers better performance with faster instance provisioning when scaling out beyond the always-ready instances.", "learning_path": "Implement Azure Functions", "module": "Explore Azure Functions", "additional_notes": "Option A can help reduce cold start duration but doesn't eliminate cold starts on Consumption plan. Option C provides consistent performance but is more expensive and requires managing the App Service plan. Option D is not possible - Consumption plan has fixed memory per instance and doesn't allow memory configuration."}
{"question": "You need to develop a function locally using Visual Studio Code and deploy it to Azure. The function should trigger when files are added to Azure Blob Storage container named 'uploads' and write processed results to a Cosmos DB collection named 'results'. You want to test the function end-to-end locally before deploying.\n\nWhat should you configure for local development?", "options": {"A": "Use Azurite local storage emulator for Blob Storage and Cosmos DB emulator for Cosmos DB, with connection strings in local.settings.json", "B": "Connect directly to Azure Storage and Azure Cosmos DB in the cloud using connection strings in local.settings.json", "C": "Mock both services using in-memory implementations in your function code", "D": "Deploy to Azure immediately and test in the cloud environment"}, "correct_answer": "A", "explanation": "For local development, using emulators provides the most isolated and cost-effective testing environment. Azurite is the official storage emulator that replicates Azure Blob, Queue, and Table storage locally. The Cosmos DB emulator provides local Cosmos DB functionality. Both emulators run on your development machine and provide connection strings that you add to local.settings.json. This allows you to develop and test functions end-to-end without incurring cloud costs or requiring internet connectivity. The Azure Functions Core Tools automatically work with these emulators, providing an experience nearly identical to running in Azure.", "learning_path": "Implement Azure Functions", "module": "Develop Azure Functions", "additional_notes": "Option B works but incurs cloud costs, requires internet connectivity, and risks polluting production data during development. Option C requires significant extra code and doesn't test actual binding behavior. Option D eliminates local testing which slows development and debugging."}
{"question": "Your Azure Function uses a Service Bus Queue trigger. During high load, you notice that the function is processing messages too slowly and the queue depth keeps growing. The function is currently scaling to 10 instances but each instance is only processing one message at a time.\n\nHow can you improve throughput?", "options": {"A": "Increase the maxConcurrentCalls setting in host.json to allow each instance to process multiple messages in parallel", "B": "Upgrade to a Premium plan for better performance", "C": "Change the Service Bus Queue to have multiple partitions", "D": "Increase the batchSize setting to process messages in larger batches"}, "correct_answer": "A", "explanation": "The maxConcurrentCalls setting in host.json controls how many messages each function instance can process concurrently. The default is often conservative (e.g., 16 for Service Bus). Increasing this value allows each instance to process multiple messages in parallel using asynchronous processing, significantly improving throughput when the function's processing is I/O-bound. This is often the most effective way to improve throughput without increasing costs. However, you must ensure your function code is thread-safe and that downstream systems can handle the increased concurrency. You should also monitor memory and CPU usage to ensure instances don't become overloaded.", "learning_path": "Implement Azure Functions", "module": "Develop Azure Functions", "additional_notes": "Option B may help but doesn't address the concurrency limitation. Option C - Service Bus Queues don't have partitions (Topics/Subscriptions support partitioning, but that's different). Option D - Service Bus trigger processes messages individually, not in batches like Event Hub."}
{"question": "You are creating an Azure Function that needs to run on a schedule every 15 minutes to aggregate data from multiple sources and generate reports. The function must run reliably at the scheduled time and should not be affected by incoming events or HTTP requests.\n\nWhich trigger type should you use?", "options": {"A": "HTTP trigger with a scheduled task calling the endpoint", "B": "Timer trigger with a CRON expression", "C": "Queue trigger with Azure Logic Apps scheduling messages", "D": "Event Grid trigger with a scheduled publisher"}, "correct_answer": "B", "explanation": "Timer triggers are specifically designed for scheduled execution in Azure Functions. They use CRON expressions to define the schedule (e.g., '0 */15 * * * *' for every 15 minutes). The timer trigger runs reliably based on the schedule and doesn't depend on external callers or events. The Functions runtime manages the schedule and ensures the function executes at the specified times. Timer triggers also handle timezone configuration and can be temporarily disabled without code changes by updating function.json or using app settings.", "learning_path": "Implement Azure Functions", "module": "Develop Azure Functions", "additional_notes": "Option A adds dependency on an external scheduler and HTTP endpoint exposure. Option C is unnecessarily complex and adds latency. Option D misuses Event Grid which is for event-driven scenarios, not scheduled tasks."}
{"question": "Your Azure Function needs to read configuration values that differ between development, staging, and production environments. The configuration includes database connection strings, API endpoints, and feature flags. You want to avoid hardcoding these values in the function code.\n\nHow should you manage these environment-specific configurations?", "options": {"A": "Use if-else statements in the code to check the environment and set values accordingly", "B": "Store values in Application Settings in the Azure Function App and access them using Environment.GetEnvironmentVariable() or IConfiguration", "C": "Create separate function projects for each environment with embedded configuration", "D": "Store all configuration in the function.json file"}, "correct_answer": "B", "explanation": "Application Settings in Azure Function Apps are the recommended way to manage environment-specific configuration. Settings configured in the Azure portal are exposed as environment variables that can be accessed in code using Environment.GetEnvironmentVariable() or through dependency injection with IConfiguration in .NET. For local development, these settings are stored in local.settings.json. This approach keeps sensitive configuration out of source control, allows easy configuration changes without code deployment, and follows the twelve-factor app methodology. For especially sensitive values, you can use Key Vault references in Application Settings.", "learning_path": "Implement Azure Functions", "module": "Develop Azure Functions", "additional_notes": "Option A hardcodes logic for environment detection and doesn't separate configuration from code. Option C requires maintaining separate codebases or build configurations. Option D mixes function behavior configuration with application settings, and function.json is not designed for environment-specific app configuration."}
{"question": "You have an Azure Function that processes IoT device telemetry data from Azure Event Hub. During testing, you notice that some messages are being processed multiple times. The function must guarantee that each message is processed exactly once even if the function fails partway through processing.\n\nWhat should you implement?", "options": {"A": "Use Event Hub trigger with manual checkpoint management and implement idempotent processing logic", "B": "Disable the Event Hub trigger's automatic checkpointing and manually commit after every message", "C": "Use Service Bus Queue instead of Event Hub for exactly-once delivery guarantees", "D": "Configure the function to run synchronously and disable scaling to ensure single processing"}, "correct_answer": "A", "explanation": "Event Hub provides at-least-once delivery semantics, meaning messages may be delivered multiple times in failure scenarios. The recommended approach is to implement idempotent processing logic, where processing the same message multiple times produces the same result as processing it once. This can be achieved by checking for duplicates using message identifiers, using upsert operations instead of inserts, or tracking processed message IDs in a database. While you can manage checkpoints manually for more control, idempotency is essential for handling retry scenarios. Event Hub's checkpoint mechanism saves the position in the stream, but doesn't guarantee exactly-once processing.", "learning_path": "Implement Azure Functions", "module": "Develop Azure Functions", "additional_notes": "Option B doesn't solve the duplicate processing issue and adds complexity. Option C is a major architecture change; Service Bus Queue also provides at-least-once, not exactly-once semantics. Option D severely limits scalability and still doesn't guarantee exactly-once processing in all failure scenarios."}
{"question": "Your Azure Function App has 10 functions with different resource requirements. Some functions are CPU-intensive while others are I/O-bound. You want to ensure that the CPU-intensive functions don't monopolize resources and prevent other functions from executing.\n\nWhat is the recommended approach?", "options": {"A": "Configure different maxDegreeOfParallelism settings in host.json for each function", "B": "Deploy CPU-intensive functions to a separate Function App with its own hosting plan", "C": "Use function-level authorization to rate-limit CPU-intensive functions", "D": "Configure different pricing tiers for different functions within the same Function App"}, "correct_answer": "B", "explanation": "When functions in the same Function App have significantly different resource profiles or scaling requirements, the recommended approach is to separate them into different Function Apps. Each Function App can have its own hosting plan with appropriate configuration and scaling settings. This provides resource isolation, prevents one function from impacting others, and allows independent scaling and configuration. For example, CPU-intensive functions might benefit from a Premium or Dedicated plan with larger instances, while I/O-bound functions might work well on Consumption plan with higher concurrency settings.", "learning_path": "Implement Azure Functions", "module": "Develop Azure Functions", "additional_notes": "Option A - maxDegreeOfParallelism is a host-level setting, not per-function. Option C uses authorization for resource management which is not its intended purpose. Option D is not possible - all functions in a Function App share the same hosting plan and pricing tier."}
{"question": "You are implementing an Azure Function with an HTTP trigger that accepts file uploads up to 100MB. The function processes the file and returns results. Users report that large file uploads fail with timeout errors.\n\nWhat should you recommend?", "options": {"A": "Increase the functionTimeout setting in host.json to allow more processing time", "B": "Implement an asynchronous pattern: accept the file upload, return HTTP 202 with a status URL, and process the file in a Durable Function orchestration", "C": "Configure the function to use a Premium plan with larger memory allocation", "D": "Split large files into smaller chunks on the client side before uploading"}, "correct_answer": "B", "explanation": "For large file uploads and processing, the recommended pattern is to decouple the upload acceptance from the processing. The function should immediately accept the file (storing it in Blob Storage), return HTTP 202 Accepted with a status URL, and trigger asynchronous processing using Durable Functions or a queue-triggered function. This prevents HTTP timeout issues (230-second Azure Load Balancer limit for HTTP functions) and provides a better user experience with progress tracking. Clients can poll the status URL to check completion. This pattern is scalable and resilient to failures, as the processing can continue even if the HTTP connection is lost.", "learning_path": "Implement Azure Functions", "module": "Develop Azure Functions", "additional_notes": "Option A doesn't solve the HTTP Load Balancer timeout constraint. Option C may improve processing speed but doesn't address the fundamental timeout issue. Option D adds complexity on the client side and still doesn't eliminate the timeout risk for processing."}
{"question": "You are developing an application that stores user-uploaded documents in Azure Blob Storage. The documents are frequently accessed for the first 30 days, occasionally accessed between 30-90 days, and rarely accessed after 90 days. You need to optimize storage costs while maintaining appropriate access performance.\n\nWhich lifecycle management policy should you configure?", "options": {"A": "Transition to Cool tier after 30 days, then Archive tier after 90 days", "B": "Keep all blobs in Hot tier and manually move old blobs to Archive", "C": "Transition to Cold tier after 30 days, then Cool tier after 90 days", "D": "Transition to Archive tier immediately after upload"}, "correct_answer": "A", "explanation": "Azure Blob Storage lifecycle management policies allow you to automatically transition blobs to cooler storage tiers based on access patterns. The Cool tier is optimized for data that is infrequently accessed and stored for a minimum of 30 days, while the Archive tier is for data that can tolerate retrieval latency and is stored for at least 180 days. This policy transitions data appropriately based on the access pattern, optimizing costs while maintaining required access performance.", "learning_path": "Develop solutions that use Blob storage", "module": "Manage the Azure Blob storage lifecycle", "additional_notes": "Option B requires manual intervention which is inefficient and error-prone. Option C has the tier progression backwards (Cold is cooler than Cool). Option D would make recently uploaded documents inaccessible for hours, which doesn't match the frequent access requirement."}
{"question": "Your application needs to store virtual machine backup files in Azure Storage. These VHD files can be up to 4 TB in size and require random read/write access for incremental backups. The files must support page-level operations.\n\nWhich type of blob should you use?", "options": {"A": "Block blobs with large block size", "B": "Append blobs with sequential write support", "C": "Page blobs optimized for random access", "D": "Block blobs with snapshot versioning"}, "correct_answer": "C", "explanation": "Page blobs are specifically designed for random read/write operations and can store files up to 8 TB in size. They are optimized for storing VHD files and serving as disks for Azure virtual machines. Page blobs support page-level operations (512-byte pages), making them ideal for incremental backup scenarios where only modified pages need to be updated rather than rewriting the entire file.", "learning_path": "Develop solutions that use Blob storage", "module": "Explore Azure Blob storage", "additional_notes": "Block blobs are made up of blocks that must be managed individually and don't support efficient random access at the page level. Append blobs are optimized for append operations only, not random access. Option D doesn't address the random access requirement for VHD files."}
{"question": "You are implementing a logging solution that writes application logs continuously to Azure Blob Storage. The logs are written sequentially and never modified after creation. The solution must optimize write performance and minimize cost.\n\nWhich blob type and storage account configuration should you use?", "options": {"A": "Block blobs in a Premium block blob storage account", "B": "Append blobs in a Standard general-purpose v2 storage account", "C": "Page blobs in a Premium page blob storage account", "D": "Block blobs in a Standard general-purpose v2 storage account with Hot tier"}, "correct_answer": "B", "explanation": "Append blobs are specifically optimized for append operations, making them ideal for logging scenarios where data is continuously written sequentially and never modified. Standard general-purpose v2 storage accounts provide the best balance of features and cost for this scenario. Append blobs consist of blocks optimized for append operations, ensuring efficient sequential writes without the overhead of managing individual blocks as required with block blobs.", "learning_path": "Develop solutions that use Blob storage", "module": "Explore Azure Blob storage", "additional_notes": "Option A (Premium block blobs) provides higher performance but at significantly higher cost, which isn't necessary for logging. Option C (Page blobs) is designed for random access, not sequential append operations. Option D (Block blobs) requires more complex block management compared to append blobs for sequential writes."}
{"question": "Your organization requires that all data stored in Azure Blob Storage must be encrypted using company-managed encryption keys stored in an on-premises hardware security module (HSM). The keys must never leave the on-premises HSM, but Azure must be able to encrypt and decrypt data.\n\nHow should you configure encryption for the storage account?", "options": {"A": "Use customer-managed keys stored in Azure Key Vault with keys imported from the on-premises HSM", "B": "Use customer-provided keys sent with each request, generated from the on-premises HSM", "C": "Use Microsoft-managed keys with additional client-side encryption using on-premises keys", "D": "This requirement cannot be met; Azure requires keys to be stored in Azure Key Vault or Key Vault HSM"}, "correct_answer": "D", "explanation": "Azure Storage server-side encryption with customer-managed keys requires that the keys be stored in either Azure Key Vault or Azure Key Vault Managed HSM. While keys can be initially generated in an on-premises HSM and imported to Azure Key Vault HSM, they must reside in Azure for encryption/decryption operations. Customer-provided keys (option B) allow keys to be provided per-request, but the keys must still be accessible to Azure for each operation, which doesn't meet the requirement that keys never leave the on-premises HSM. For true on-premises key retention, client-side encryption before upload is the only option.", "learning_path": "Develop solutions that use Blob storage", "module": "Explore Azure Storage security features", "additional_notes": "Options A and B both require keys to be accessible to Azure for encryption operations. Option C mentions client-side encryption which could work, but the question specifically asks about storage account encryption configuration. The requirement as stated (keys never leaving on-premises) is incompatible with Azure's server-side encryption model."}
{"question": "You need to create a storage container that will store financial reports accessible only to specific users. The container name must follow Azure naming conventions and support efficient URL-based access.\n\nWhich container name is valid?", "options": {"A": "Financial_Reports_2024", "B": "financial-reports-2024", "C": "FinancialReports2024", "D": "financial.reports.2024"}, "correct_answer": "B", "explanation": "Azure Blob Storage container names must follow specific naming rules: they must be between 3 and 63 characters long, start with a letter or number, and contain only lowercase letters, numbers, and the dash (-) character. Consecutive dashes are not permitted. The container name forms part of the URI used to access the container and blobs, so it must be a valid DNS name. Option B 'financial-reports-2024' follows all these rules.", "learning_path": "Develop solutions that use Blob storage", "module": "Explore Azure Blob storage", "additional_notes": "Option A contains uppercase letters and underscores (not allowed). Option C contains uppercase letters (must be lowercase). Option D contains periods which are not permitted in container names, though they are allowed in blob names within containers."}
{"question": "Your application stores user profile images in Azure Blob Storage with a Premium block blob storage account. Users report slow image loading times during peak hours. The storage account is in the East US region, and most users are in Europe.\n\nWhat should you do to improve performance?", "options": {"A": "Change to a Standard general-purpose v2 storage account for better throughput", "B": "Enable geo-redundancy (GRS) to replicate data to Europe", "C": "Create a separate Premium block blob storage account in West Europe and replicate images using lifecycle policies", "D": "Implement Azure CDN with the storage account as the origin and enable caching"}, "correct_answer": "D", "explanation": "Azure Content Delivery Network (CDN) caches frequently accessed content at edge locations closer to users, dramatically reducing latency for content delivery. Since the images are user profile photos accessed from Europe, CDN will cache them at European edge locations, providing much faster access than retrieving from the East US storage account. CDN is specifically designed for this scenario and integrates seamlessly with Azure Blob Storage.", "learning_path": "Develop solutions that use Blob storage", "module": "Explore Azure Blob storage", "additional_notes": "Option A would decrease performance (Standard is slower than Premium). Option B provides disaster recovery but doesn't improve read performance (secondary region is read-only and not automatically accessed). Option C adds complexity and doesn't provide the edge caching benefits that CDN offers globally."}
{"question": "You are implementing encryption for sensitive data stored in Azure Blob Storage. The application must have complete control over the encryption process, including key management and encryption/decryption logic. Encrypted data must be uploaded to and downloaded from Blob Storage.\n\nWhich encryption approach should you implement?", "options": {"A": "Azure Storage service-side encryption with customer-managed keys", "B": "Client-side encryption using Azure Blob Storage client library with customer-provided keys", "C": "Azure Storage service-side encryption with Microsoft-managed keys", "D": "Transport-level encryption using HTTPS only"}, "correct_answer": "B", "explanation": "Client-side encryption using the Azure Blob Storage client libraries (.NET, Java, Python) allows the application to encrypt data on the client before uploading to Azure Storage and decrypt it after downloading. This gives complete control over the encryption process, key management, and ensures that Azure never has access to unencrypted data or encryption keys. The client libraries support AES encryption in Galois/Counter Mode (GCM) for version 2 client-side encryption.", "learning_path": "Develop solutions that use Blob storage", "module": "Explore Azure Storage security features", "additional_notes": "Options A and C use server-side encryption where Azure handles encryption/decryption, which doesn't provide the complete control required. Option D only encrypts data in transit, not at rest, and doesn't give the application control over encryption logic."}
{"question": "Your organization has a Standard general-purpose v2 storage account with 500 TB of data distributed across multiple containers. You need to ensure business continuity by protecting against regional disasters while minimizing costs. The solution must provide read access to data if the primary region becomes unavailable.\n\nWhich redundancy option should you configure?", "options": {"A": "Zone-redundant storage (ZRS)", "B": "Locally redundant storage (LRS) with manual backups to another region", "C": "Geo-redundant storage (GRS)", "D": "Read-access geo-redundant storage (RA-GRS)"}, "correct_answer": "D", "explanation": "Read-access geo-redundant storage (RA-GRS) replicates data to a secondary region hundreds of miles away from the primary region and provides read access to the data in the secondary region. This meets both requirements: protection against regional disasters and read access during primary region outages. With RA-GRS, applications can be configured to read from the secondary endpoint if the primary region becomes unavailable, ensuring business continuity.", "learning_path": "Develop solutions that use Blob storage", "module": "Explore Azure Storage security features", "additional_notes": "Option A (ZRS) only protects against datacenter failures within a region, not regional disasters. Option B requires manual intervention and doesn't provide automatic failover. Option C (GRS) replicates to a secondary region but doesn't provide read access to the secondary until Microsoft initiates a failover."}
{"question": "You are implementing a data archival solution where blobs older than 365 days must be moved to the Archive tier. After moving to Archive, blobs must remain in the Archive tier for at least 180 days before they can be deleted. You need to configure a lifecycle management policy that enforces this requirement.\n\nWhat should you include in the policy definition?", "options": {"A": "Set daysAfterModificationGreaterThan to 365 for tierToArchive, then 545 for delete", "B": "Set daysAfterModificationGreaterThan to 365 for tierToArchive with daysAfterLastTierChangeGreaterThan set to 180 for delete", "C": "Set daysAfterCreationGreaterThan to 365 for tierToArchive, then use a separate rule with daysAfterModificationGreaterThan set to 545 for delete", "D": "Archive tier blobs cannot be deleted automatically; manual deletion is required after 180 days"}, "correct_answer": "A", "explanation": "The lifecycle management policy should use daysAfterModificationGreaterThan to track blob age. Setting tierToArchive action at 365 days moves blobs to Archive after one year. The delete action should be set to 545 days (365 + 180) to ensure blobs remain in Archive for at least 180 days before deletion. The policy evaluates conditions based on last modification time, and both actions use the same reference point, making the calculation straightforward: total days = days before archive + minimum retention in archive.", "learning_path": "Develop solutions that use Blob storage", "module": "Manage the Azure Blob storage lifecycle", "additional_notes": "Option B incorrectly uses daysAfterLastTierChangeGreaterThan which is specifically for controlling rehydration from Archive, not for delete actions. Option C unnecessarily complicates the solution with separate rules when one rule can handle both actions. Option D is incorrect; Archive blobs can be automatically deleted via lifecycle policies."}
{"question": "Your application needs to store both structured JSON metadata and large binary files (images and videos) together as a single logical unit. Each file can be up to 50 MB. The solution must support querying metadata efficiently while maintaining the association between metadata and binary content.\n\nHow should you architect the storage solution?", "options": {"A": "Store JSON metadata as blob metadata properties and binary files as block blobs", "B": "Store JSON metadata in Azure Table Storage and binary files in Blob Storage with blob URI in Table Storage", "C": "Store JSON metadata in Azure Cosmos DB and binary files in Blob Storage with blob URI in Cosmos DB", "D": "Store both JSON and binary files as separate blobs in the same container using blob index tags for association"}, "correct_answer": "C", "explanation": "Azure Cosmos DB provides efficient querying capabilities for JSON data while Azure Blob Storage is optimized for storing large binary files. Storing the blob URI in Cosmos DB maintains the association between metadata and content. This architecture separates concerns: Cosmos DB handles fast queries on structured metadata with its indexing capabilities, while Blob Storage efficiently stores large files. This is a recommended pattern for applications requiring both rich querying and binary storage.", "learning_path": "Develop solutions that use Blob storage", "module": "Explore Azure Blob storage", "additional_notes": "Option A (blob metadata) is limited to 8 KB and doesn't support complex queries. Option B (Table Storage) could work but lacks the rich querying capabilities of Cosmos DB and doesn't support as complex JSON structures. Option D stores everything in Blob Storage but blob index tags are limited and not designed for complex metadata queries."}
{"question": "You are designing a globally distributed e-commerce application using Azure Cosmos DB. The application requires strong consistency for inventory management to prevent overselling, but can tolerate eventual consistency for user reviews and ratings. Different geographic regions have different read and write patterns.\n\nHow should you configure consistency levels?", "options": {"A": "Configure Strong consistency at the account level for all operations", "B": "Configure Session consistency at the account level and override to Strong for inventory operations per-request", "C": "Configure Eventual consistency at the account level to maximize performance", "D": "Create separate Cosmos DB accounts for inventory (Strong) and reviews (Eventual)"}, "correct_answer": "B", "explanation": "Azure Cosmos DB allows you to set a default consistency level at the account level and override it to a weaker consistency level on a per-request basis. By setting Session consistency as the default (which provides read-your-writes guarantees and is suitable for most scenarios), you can override to Strong consistency for critical inventory operations that require linearizability. This provides the flexibility to use appropriate consistency levels for different operations without the overhead and cost of maintaining separate accounts.", "learning_path": "Develop solutions that use Azure Cosmos DB", "module": "Explore Azure Cosmos DB", "additional_notes": "Option A forces Strong consistency for all operations, increasing latency and costs unnecessarily for reviews. Option C risks inconsistencies in inventory management. Option D adds significant complexity and cost with multiple accounts when Cosmos DB supports per-request consistency level relaxation."}
{"question": "Your application stores IoT sensor data in Azure Cosmos DB with millions of new documents written daily. Queries primarily filter by deviceId and timestamp. The application experiences hot partition issues where certain devices generate significantly more data than others.\n\nWhat should you do to resolve the hot partition issue?", "options": {"A": "Change the partition key from deviceId to timestamp", "B": "Use a composite partition key combining deviceId with a hash or time-based suffix", "C": "Increase the provisioned RU/s for the container", "D": "Enable serverless mode for automatic scaling"}, "correct_answer": "B", "explanation": "Hot partitions occur when data is unevenly distributed across partition key values, causing some physical partitions to become overloaded. Using a composite partition key that combines deviceId with a hash or time-based suffix (e.g., deviceId + hour or deviceId + hash(timestamp)) distributes data from high-volume devices across multiple physical partitions. This maintains the ability to query by device while improving throughput and avoiding throttling caused by hot partitions.", "learning_path": "Develop solutions that use Azure Cosmos DB", "module": "Explore Azure Cosmos DB", "additional_notes": "Option A using timestamp would distribute data evenly but makes device-specific queries inefficient as they would scatter across many partitions. Option C only temporarily masks the problem by providing more capacity but doesn't resolve the uneven distribution. Option D doesn't address the fundamental partition design issue."}
{"question": "You need to implement a transaction that updates multiple documents across different partition keys in Azure Cosmos DB. The operation must either complete entirely or roll back completely. All affected documents are in the same container.\n\nWhich approach should you use?", "options": {"A": "Use Cosmos DB stored procedures with transactional batch support", "B": "Use Cosmos DB change feed to implement saga pattern for distributed transactions", "C": "Use TransactionalBatch with multiple partition keys", "D": "Implement application-level two-phase commit"}, "correct_answer": "B", "explanation": "Azure Cosmos DB transactions (including stored procedures and TransactionalBatch) are scoped to a single logical partition. When you need to update documents across different partition keys, you must use a distributed transaction pattern like the saga pattern. The change feed can trigger compensating transactions if any step fails, providing eventual consistency across partitions. This is the recommended approach for cross-partition transactions in Cosmos DB.", "learning_path": "Develop solutions that use Azure Cosmos DB", "module": "Work with Azure Cosmos DB", "additional_notes": "Option A (stored procedures) and Option C (TransactionalBatch) only support transactions within a single partition key. Option D requires complex coordination logic and doesn't leverage Cosmos DB's built-in features like change feed for implementing saga patterns."}
{"question": "Your Cosmos DB container has a partition key of /userId and stores user profiles with size ranging from 1 KB to 2 MB. You need to query all users who registered in the last 30 days, ordered by registration date. The container has 10 million documents across 100 physical partitions.\n\nWhat should you do to optimize this query?", "options": {"A": "Add a composite index on (registrationDate, userId)", "B": "Create a secondary container with partition key /registrationDate", "C": "Enable analytical store and query using Azure Synapse Link", "D": "Use continuation tokens to paginate results efficiently"}, "correct_answer": "B", "explanation": "This query is a cross-partition query that must fan out to all partitions, which is expensive in terms of RU consumption. Creating a secondary container partitioned by registrationDate (or registrationDate with a suffix for distribution) allows efficient querying for recent registrations. This container can be kept in sync using change feed processing. This pattern of maintaining multiple containers with different partition keys optimized for different query patterns is a recommended approach in Cosmos DB when specific queries are critical and frequent.", "learning_path": "Develop solutions that use Azure Cosmos DB", "module": "Work with Azure Cosmos DB", "additional_notes": "Option A helps with ordering within partitions but doesn't eliminate the need to query across all partitions. Option C is good for analytics but adds complexity for operational queries. Option D helps with pagination but doesn't reduce the cost of the cross-partition query itself."}
{"question": "You are configuring indexing for a Cosmos DB container that stores user activity logs. The logs have 50+ fields but queries only use 3 specific fields: userId, eventType, and timestamp. Write operations outnumber read operations 10:1.\n\nHow should you configure the indexing policy?", "options": {"A": "Use automatic indexing with default policy (index all properties)", "B": "Set indexingMode to 'none' to maximize write performance", "C": "Set indexingMode to 'consistent' with includedPaths for only userId, eventType, timestamp, and exclude all others", "D": "Set indexingMode to 'lazy' to balance write performance and query capability"}, "correct_answer": "C", "explanation": "By explicitly including only the three fields used in queries and excluding all others, you minimize indexing overhead during writes while maintaining efficient query performance for the fields you actually query. This is especially important when writes far outnumber reads (10:1 ratio). Cosmos DB charges RUs for both writes and index updates, so reducing indexed properties significantly improves write performance and reduces costs. The indexingMode should be 'consistent' to ensure queries have up-to-date index information.", "learning_path": "Develop solutions that use Azure Cosmos DB", "module": "Work with Azure Cosmos DB", "additional_notes": "Option A indexes all 50+ fields unnecessarily, wasting RUs on write operations. Option B completely disables indexing, making queries inefficient. Option D ('lazy' mode) is deprecated and not recommended; it can cause queries to return stale results."}
{"question": "Your application uses Azure Cosmos DB with Session consistency level. A user updates their profile in the US East region, and the application immediately redirects them to read their profile from the US West region. The user reports seeing old profile data.\n\nWhat is the cause and solution?", "options": {"A": "Session consistency is broken; upgrade to Bounded Staleness consistency", "B": "Session tokens are not being passed across regions; ensure the session token from the write is included in the read request", "C": "US West region has replication lag; wait for automatic synchronization", "D": "Session consistency doesn't work across regions; use Strong consistency"}, "correct_answer": "B", "explanation": "Session consistency guarantees read-your-writes, but only when the session token from the write operation is passed to subsequent read operations. When reading from a different region, the application must explicitly pass the session token received from the write response to the read request. This allows Cosmos DB to ensure the read sees at least the updates from that session, even across regions. The SDK automatically handles this within a single client instance, but cross-region scenarios may require explicit token management.", "learning_path": "Develop solutions that use Azure Cosmos DB", "module": "Explore Azure Cosmos DB", "additional_notes": "Option A and D suggest changing consistency level unnecessarily, which increases cost and latency. Option C is incorrect; with Session consistency and proper token passing, reads should see writes immediately. Session consistency works across regions when session tokens are properly propagated."}
{"question": "You need to migrate 500 GB of data from an on-premises MongoDB database to Azure Cosmos DB for MongoDB API. The migration must minimize downtime and ensure data consistency. The application can tolerate brief read-only mode during cutover.\n\nWhich migration strategy should you use?", "options": {"A": "Use Azure Data Factory to copy data in a single bulk operation", "B": "Use Azure Database Migration Service with online migration to sync data continuously", "C": "Export MongoDB data to JSON files and import using Cosmos DB Data Migration Tool", "D": "Use mongodump and mongorestore commands for direct database copy"}, "correct_answer": "B", "explanation": "Azure Database Migration Service (DMS) supports online migration for MongoDB to Cosmos DB for MongoDB API. Online migration performs an initial full data copy followed by continuous synchronization of changes (CDC - change data capture) until you're ready to cutover. This minimizes downtime as the application can continue running against the source during most of the migration. When ready, you can briefly switch the application to read-only mode, let final changes sync, then point the application to Cosmos DB.", "learning_path": "Develop solutions that use Azure Cosmos DB", "module": "Explore Azure Cosmos DB", "additional_notes": "Option A causes extended downtime during the bulk copy. Option C requires manual export/import and doesn't handle changes during migration. Option D (mongodump/mongorestore) requires significant downtime for 500 GB of data and doesn't provide continuous synchronization."}
{"question": "Your Cosmos DB container uses manual provisioned throughput with 40,000 RU/s. During peak hours, you see 429 (Too Many Requests) throttling errors. During off-peak hours, you use only 10,000 RU/s. The peak usage pattern is predictable (daily 2-hour peak).\n\nWhat should you do to optimize costs while preventing throttling?", "options": {"A": "Switch to autoscale provisioned throughput with max 40,000 RU/s", "B": "Increase manual throughput to 60,000 RU/s to provide buffer", "C": "Switch to serverless mode", "D": "Implement application-level request retry logic with exponential backoff"}, "correct_answer": "A", "explanation": "Autoscale provisioned throughput automatically scales between 10% of max RU/s and the configured max based on actual usage. With autoscale set to max 40,000 RU/s, it scales from 4,000 to 40,000 RU/s as needed. During off-peak (10,000 RU/s usage), you'll be charged for ~10,000 RU/s rather than the full 40,000. During peak, it scales up to 40,000 RU/s to prevent throttling. For predictable peak patterns with significant variance, autoscale is more cost-effective than manual provisioning at peak capacity.", "learning_path": "Develop solutions that use Azure Cosmos DB", "module": "Work with Azure Cosmos DB", "additional_notes": "Option B increases costs 24/7 to handle 2-hour peaks. Option C (serverless) has limitations (max 5000 RU/s per partition, 50 GB storage limit) that may not suit this scenario. Option D addresses the symptom but doesn't prevent throttling or optimize costs."}
{"question": "You are implementing a real-time analytics dashboard that displays aggregated user activity from Azure Cosmos DB. The aggregations are complex and involve scanning millions of documents. Running these queries on the transactional store is affecting application performance.\n\nWhat should you configure?", "options": {"A": "Create materialized views using change feed and store aggregations in a separate container", "B": "Enable Azure Synapse Link and query the analytical store", "C": "Increase the RU/s allocation to support analytics queries", "D": "Create secondary indexes on all fields used in aggregations"}, "correct_answer": "B", "explanation": "Azure Synapse Link provides a seamless integration between Cosmos DB and Azure Synapse Analytics. When enabled, it creates a column-store analytical store that is automatically synchronized with the transactional row-store. Analytics queries run against the analytical store without consuming RU/s from the transactional workload and without impacting operational performance. This is the recommended solution for running complex analytics and aggregations on Cosmos DB data without affecting transactional operations.", "learning_path": "Develop solutions that use Azure Cosmos DB", "module": "Work with Azure Cosmos DB", "additional_notes": "Option A works but requires custom code and maintenance for the change feed processor and aggregation logic. Option C wastes money by provisioning high RU/s for analytics when Synapse Link provides isolated analytics at lower cost. Option D doesn't help with scan-heavy aggregation queries."}
{"question": "Your application stores order documents in Cosmos DB with partition key /customerId. Each order contains an array of line items. You need to query for all orders containing a specific product SKU across all customers. Current queries are slow and expensive.\n\nHow should you optimize the query?", "options": {"A": "Change the partition key to /productSKU", "B": "Flatten the data model: create a separate document for each line item with partition key /productSKU", "C": "Use a composite index on (customerId, lineItems.sku)", "D": "Use the ARRAY_CONTAINS function with a specific partition key"}, "correct_answer": "B", "explanation": "When you frequently query by a property that is not the partition key (especially arrays like line items), consider flattening the data model. Creating a separate container (or additional denormalized documents) where each line item is a document with partition key /productSKU allows efficient queries for orders by product. This is a common denormalization pattern in Cosmos DB: storing data in multiple formats optimized for different query patterns. You can use change feed to keep the two representations synchronized.", "learning_path": "Develop solutions that use Azure Cosmos DB", "module": "Work with Azure Cosmos DB", "additional_notes": "Option A breaks existing customer-based queries. Option C doesn't help with cross-partition queries as indexes optimize within-partition queries. Option D still requires a cross-partition query across all customerId partitions, which is expensive. Data modeling for query patterns is essential in Cosmos DB."}
