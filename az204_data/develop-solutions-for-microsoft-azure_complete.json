{
  "url": "https://learn.microsoft.com/en-us/training/courses/az-204t00",
  "title": "Develop Solutions for Microsoft Azure",
  "description": "",
  "learning_paths": [
    {
      "title": "Implement Azure App Service web apps",
      "url": "https://learn.microsoft.com/en-us/training/paths/create-azure-app-service-web-apps/",
      "learn_uid": "learn.wwl.create-azure-app-service-web-apps",
      "modules": [
        {
          "title": "Explore Azure App Service",
          "url": "https://learn.microsoft.com/en-us/training/modules/introduction-to-azure-app-service/",
          "description": "",
          "learning_objectives": [],
          "prerequisites": [],
          "units": [
            {
              "number": 1,
              "title": "Introduction",
              "url": "https://learn.microsoft.com/en-us/training/modules/introduction-to-azure-app-service/1-introduction",
              "href": "1-introduction",
              "content": "Read in English\nAdd\nAdd to plan\nIntroduction\nCompleted\n3 minutes\nYour company is considering moving their web apps to the cloud and you're asked to evaluate Azure App Service.\nLearning objectives\nAfter completing this module, you'll be able to:\nDescribe Azure App Service key components and value.\nExplain how Azure App Service manages authentication and authorization.\nIdentify methods to control inbound and outbound traffic to your web app.\nDeploy a containerized app to App Service.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Introduction"
                },
                {
                  "level": 2,
                  "text": "Learning objectives"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 2,
              "title": "Examine Azure App Service",
              "url": "https://learn.microsoft.com/en-us/training/modules/introduction-to-azure-app-service/2-azure-app-service",
              "href": "2-azure-app-service",
              "content": "Read in English\nAdd\nAdd to plan\nExamine Azure App Service\nCompleted\n3 minutes\nAzure App Service is a fully managed platform designed to simplify the deployment and scaling of web apps, mobile back ends, and RESTful APIs. It abstracts away infrastructure management, letting you focus on writing code and shipping features faster.\nYou can build using your preferred stackâwhether itâs .NET, Java (Java SE, Tomcat, JBoss), Node.js, Python, or PHPâand deploy to either Windows or Linux environments. If you're working with containers, App Service also supports custom container deployments, giving you full control over your runtime.\nBuilt-in auto scale support\nThe ability to scale up/down or scale out/in is baked into the Azure App Service. Depending on the usage of the web app, you can scale the resources of the underlying machine that is hosting your web app up/down. Resources include the number of cores or the amount of RAM available. Scaling out/in is the ability to increase, or decrease, the number of machine instances that are running your web app.\nContainer support\nWith Azure App Service, you can deploy and run containerized web apps on Windows and Linux. You can pull container images from a private Azure Container Registry or Docker Hub. Azure App Service also supports multi-container apps, Windows containers, and Docker Compose for orchestrating container instances.\nContinuous integration/deployment support\nThe Azure portal provides out-of-the-box continuous integration and deployment with Azure DevOps Services, GitHub, Bitbucket, FTP, or a local Git repository on your development machine. Connect your web app with any of the above sources and App Service can automatically sync your code and apply changes as theyâre pushed to the connected repository. Continuous integration and deployment for containerized web apps is also supported using either Azure Container Registry or Docker Hub.\nDeployment slots\nWhen deploying a web app, you can use a separate deployment slot instead of the default production slot when you're running in the Standard App Service pricing tier or better. Deployment slots are live apps with their own host names. App content and configurations elements can be swapped between two deployment slots, including the production slot.\nApp Service on Linux\nApp Service can also host web apps natively on Linux for supported application stacks. It can also run custom Linux containers (also known as\nWeb App for Containers\n). App Service on Linux supports many language specific built-in images. Just deploy your code. Supported languages and frameworks include: .NET Core, Java (Tomcat, JBoss EAP, or Java SE with an embedded web server), Node.js, Python, and PHP. If the runtime your application requires isn't supported in the built-in images, you can deploy it with a custom container.\nThe languages, and their supported versions, are updated regularly. You can retrieve the current list by using the following command in the Cloud Shell.\naz webapp list-runtimes --os-type linux\nLimitations\nApp Service on Linux does have some limitations:\nApp Service on Linux isn't supported on Shared pricing tier.\nThe Azure portal shows only features that currently work for Linux apps. As features are enabled, they're activated on the portal.\nWhen deployed to built-in images, your code and content are allocated as a storage volume for web content, backed by Azure Storage. The disk latency of this volume is higher and more variable than the latency of the container filesystem. Apps that require heavy read-only access to content files might benefit from the custom container option, which places files in the container filesystem instead of on the content volume.\nApp Service Environment\nApp Service Environment is an Azure App Service feature that provides a fully isolated and dedicated environment for running App Service apps. It offers improved security at high scale.\nUnlike the App Service offering, where supporting infrastructure is shared, with App Service Environment, compute is dedicated to a single customer. For more information on the differences between App Service Environment and App Service, see the\ncomparison\n.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Examine Azure App Service"
                },
                {
                  "level": 2,
                  "text": "Built-in auto scale support"
                },
                {
                  "level": 2,
                  "text": "Container support"
                },
                {
                  "level": 2,
                  "text": "Continuous integration/deployment support"
                },
                {
                  "level": 2,
                  "text": "Deployment slots"
                },
                {
                  "level": 2,
                  "text": "App Service on Linux"
                },
                {
                  "level": 3,
                  "text": "Limitations"
                },
                {
                  "level": 2,
                  "text": "App Service Environment"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [
                "az webapp list-runtimes --os-type linux",
                "az webapp list-runtimes --os-type linux"
              ],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                },
                {
                  "url": "/en-us/azure/app-service/environment/ase-multi-tenant-comparison",
                  "text": "comparison"
                }
              ]
            },
            {
              "number": 3,
              "title": "Examine Azure App Service plans",
              "url": "https://learn.microsoft.com/en-us/training/modules/introduction-to-azure-app-service/3-azure-app-service-plans",
              "href": "3-azure-app-service-plans",
              "content": "Read in English\nAdd\nAdd to plan\nExamine Azure App Service plans\nCompleted\n4 minutes\nIn App Service, an app always runs in an\nApp Service plan\n. An App Service plan defines a set of compute resources for a web app to run. One or more apps can be configured to run on the same computing resources (or in the same App Service plan).\nWhen you create an App Service plan in a certain region (for example, West Europe), a set of compute resources is created for that plan in that region. Whatever apps you put into this App Service plan run on these compute resources as defined by your App Service plan. Each App Service plan defines:\nOperating System (Windows, Linux)\nRegion (West US, East US, etc.)\nNumber of VM instances\nSize of VM instances (for example, P1v3, P2v3, based on pricing tier)\nPricing tier (Free, Shared, Basic, Standard, Premium, PremiumV2, PremiumV3, IsolatedV2)\nThe\npricing tier\nof an App Service plan determines what App Service features you get and how much you pay for the plan. There are a few categories of pricing tiers:\nShared compute\n:\nFree\nand\nShared\n, the two base tiers, runs an app on the same Azure VM as other App Service apps, including apps of other customers. These tiers allocate CPU quotas to each app that runs on the shared resources, and the resources can't scale out.\nDedicated compute\n: The\nBasic\n,\nStandard\n,\nPremium\n,\nPremiumV2\n, and\nPremiumV3\ntiers run apps on dedicated Azure VMs. Only apps in the same App Service plans share the same compute resources. The higher the tier, the more VM instances are available to you for scale-out.\nIsolated\n: The\nIsolated\nand\nIsolatedV2\ntiers run dedicated Azure VMs on dedicated Azure Virtual Networks. It provides network isolation on top of compute isolation to your apps. It provides the maximum scale-out capabilities.\nNote\nApp Service Free and Shared hosting plans are base tiers that run on the same Azure virtual machines as other App Service apps. Some apps might belong to other customers. These tiers are intended to be used only for development and testing purposes.\nHow does my app run and scale?\nIn the\nFree\nand\nShared\ntiers, an app receives CPU minutes on a shared VM instance and can't scale out. In other tiers, an app runs and scales as follows:\nAn app runs on all the VM instances configured in the App Service plan.\nIf multiple apps are in the same App Service plan, they all share the same VM instances.\nIf you have multiple deployment slots for an app, all deployment slots also run on the same VM instances.\nIf you enable diagnostic logs, perform backups, or run WebJobs, they also use CPU cycles and memory on these VM instances.\nIn this way, the App Service plan is the\nscale unit\nof the App Service apps. If the plan is configured to run five VM instances, then all apps in the plan run on all five instances. If the plan is configured for autoscaling, then all apps in the plan are scaled out together based on the autoscale settings.\nWhat if my app needs more capabilities or features?\nYour App Service plan can be scaled up and down at any time. It's as simple as changing the pricing tier of the plan. If your app is in the same App Service plan with other apps, you might want to improve the app's performance by isolating the compute resources. You can do it by moving the app into a separate App Service plan.\nYou can potentially save money by putting multiple apps into one App Service plan. However, since apps in the same App Service plan all share the same compute resources you need to understand the capacity of the existing App Service plan and the expected load for the new app.\nIsolate your app into a new App Service plan when:\nThe app is resource-intensive.\nYou want to scale the app independently from the other apps in the existing plan.\nThe app needs resources in a different geographical region.\nThis approach gives you a dedicated resource pool and greater control over your appâs performance and scaling.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Examine Azure App Service plans"
                },
                {
                  "level": 2,
                  "text": "How does my app run and scale?"
                },
                {
                  "level": 2,
                  "text": "What if my app needs more capabilities or features?"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 4,
              "title": "Deploy to App Service",
              "url": "https://learn.microsoft.com/en-us/training/modules/introduction-to-azure-app-service/4-deploy-code-to-app-service",
              "href": "4-deploy-code-to-app-service",
              "content": "Read in English\nAdd\nAdd to plan\nDeploy to App Service\nCompleted\n3 minutes\nEvery development team has unique requirements that can make implementing an efficient deployment pipeline difficult on any cloud service. App Service supports both automated and manual deployment.\nAutomated deployment\nAutomated deployment, or continuous deployment, is a process used to push out new features and bug fixes in a fast and repetitive pattern with minimal effect on end users.\nAzure App Service supports automated deployment from several source control systems as part of a continuous integration and deployment (CI/CD) pipeline. The following options are available:\nAzure DevOps Services\n: You can push your code to Azure DevOps Services, build your code in the cloud, run the tests, generate a release from the code, and finally, push your code to an Azure Web App.\nGitHub\n: Azure supports automated deployment directly from GitHub. When you connect your GitHub repository to Azure for automated deployment, any changes you push to your production branch on GitHub are automatically deployed for you.\nBitbucket\n: Bitbucket is supported, though GitHub and Azure DevOps are more commonly used and better integrated.\nManual deployment\nThere are a few options that you can use to manually push your code to Azure:\nGit\n: App Service web apps feature a Git URL that you can add as a remote repository. Pushing to the remote repository deploys your app.\nCLI\n: The\naz webapp up\nis a feature of the\naz\ncommand-line interface that packages your app and deploys it. Unlike other deployment methods,\naz webapp up\ncan create a new App Service web app for you.\nZip deploy\n: Use\ncurl\nor a similar HTTP utility to send a ZIP of your application files to App Service.\nFTP/S\n: FTP or FTPS is a traditional way of pushing your code to many hosting environments, including App Service.\nNote\nApp Service uses Kudu for Git and zip-based deployments. Kudu handles file syncing and deployment triggers.\nUse deployment slots\nWhenever possible, use deployment slots when deploying a new production build. When using a Standard App Service Plan tier or better, you can deploy your app to a staging environment and then swap your staging and production slots. The swap operation warms up the necessary worker instances to match your production scale, thus eliminating downtime.\nContinuously deploy code\nIf your project designates branches for testing, QA, and staging, then each of those branches should be continuously deployed to a staging slot. This allows your stakeholders to easily access and test the deployed branch.\nContinuously deploy containers\nFor custom containers from Azure Container Registry or other container registries, deploy the image into a staging slot and swap into production to prevent downtime. The automation is more complex than code deployment because you must push the image to a container registry and update the image tag on the webapp.\nBuild and tag the image\n: As part of the build pipeline, tag the image with the git commit ID, timestamp, or other identifiable information. Itâs best not to use the default \"latest\" tag. Otherwise, itâs difficult to trace back what code is currently deployed, which makes debugging far more difficult.\nPush the tagged image\n: Once the image is built and tagged, the pipeline pushes the image to your container registry. In the next step, the deployment slot will pull the tagged image from the container registry.\nUpdate the deployment slot with the new image tag\n: When this property is updated, the site automatically restarts and pulls the new container image.\nSidecar containers\nIn Azure App Service, you can add up to nine sidecar containers for each sidecar-enabled custom container app. Sidecar containers are supported for Linux-based custom container apps and enable deploying extra services and features without making them tightly coupled to your main application container. For example, you can add monitoring, logging, configuration, and networking services as sidecar containers.\nYou can add a sidecar container through the\nDeployment Center\nin the app's management page.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Deploy to App Service"
                },
                {
                  "level": 2,
                  "text": "Automated deployment"
                },
                {
                  "level": 2,
                  "text": "Manual deployment"
                },
                {
                  "level": 2,
                  "text": "Use deployment slots"
                },
                {
                  "level": 3,
                  "text": "Continuously deploy code"
                },
                {
                  "level": 3,
                  "text": "Continuously deploy containers"
                },
                {
                  "level": 2,
                  "text": "Sidecar containers"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [
                "az webapp up",
                "az webapp up"
              ],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 5,
              "title": "Explore authentication and authorization in App Service",
              "url": "https://learn.microsoft.com/en-us/training/modules/introduction-to-azure-app-service/5-authentication-authorization-app-service",
              "href": "5-authentication-authorization-app-service",
              "content": "Read in English\nAdd\nAdd to plan\nExplore authentication and authorization in App Service\nCompleted\n6 minutes\nAzure App Service provides built-in authentication and authorization support. You can sign in users and access data by writing minimal or no code in your web app, RESTful API, mobile back end, or Azure Functions.\nWhy use the built-in authentication?\nYou're not required to use App Service for authentication and authorization. Many web frameworks are bundled with security features, and you can use them if you like. If you need more flexibility than App Service provides, you can also write your own utilities.\nThe built-in authentication feature for App Service and Azure Functions can save you time and effort by providing out-of-the-box authentication with federated identity providers, allowing you to focus on the rest of your application.\nAzure App Service allows you to integrate various auth capabilities into your web app or API without implementing them yourself.\nAuth is built directly into the platform and doesnât require any particular language, SDK, security expertise, or code.\nYou can integrate with multiple login providers. For example, Microsoft Entra ID, Facebook, Google, X.\nIdentity providers\nApp Service uses federated identity, in which a third-party identity provider manages the user identities and authentication flow for you. The following identity providers are available by default:\nProvider\nSign-in endpoint\nHow-To guidance\nMicrosoft Entra\n/.auth/login/aad\nApp Service Microsoft Entra platform login\nFacebook\n/.auth/login/facebook\nApp Service Facebook login\nGoogle\n/.auth/login/google\nApp Service Google login\nX\n/.auth/login/x\nApp Service X login\nAny OpenID Connect provider\n/.auth/login/<providerName>\nApp Service OpenID Connect login\nGitHub\n/.auth/login/github\nApp Service GitHub login\nApple\n/.auth/login/apple\n/azure/app-service/configure-authentication-provider-apple\nWhen you configure this feature with one of these providers, its sign-in endpoint is available for user authentication and for validation of authentication tokens from the provider. You can provide your users with any number of these sign-in options.\nHow it works\nThe authentication and authorization middleware component is a feature of the platform that runs on the same VM as your application. When enabled, every incoming HTTP request passes through it before being handled by your application.\nThe platform middleware handles several things for your app:\nAuthenticates users and clients with the specified identity provider\nValidates, stores, and refreshes OAuth tokens issued by the configured identity provider\nManages the authenticated session\nInjects identity information into HTTP request headers\nThe module runs separately from your application code and can be configured using Azure Resource Manager settings or using a configuration file. No SDKs, specific programming languages, or changes to your application code are required.\nNote\nIn Linux and containers the authentication and authorization module runs in a separate container, isolated from your application code. Because it doesn't run in-process, no direct integration with specific language frameworks is possible.\nAuthentication flow\nThe authentication flow is the same for all providers, but differs depending on whether you want to sign in with the provider's SDK.\nWithout provider SDK: The application delegates federated sign-in to App Service. This delegation is typically the case with browser apps, which can present the provider's login page to the user. The server code manages the sign-in process and is referred to as\nserver-directed flow\nor\nserver flow\n.\nWith provider SDK: The application signs users in to the provider manually and then submits the authentication token to App Service for validation. This is typically the case with browser-less apps, which can't present the provider's sign-in page to the user. The application code manages the sign-in process and is referred to as\nclient-directed flow\nor\nclient flow\n. This applies to REST APIs, Azure Functions, JavaScript browser clients, and native mobile apps that sign users in using the provider's SDK.\nThe following table shows the steps of the authentication flow.\nStep\nWithout provider SDK\nWith provider SDK\nSign user in\nRedirects client to\n/.auth/login/<provider>\n.\nClient code signs user in directly with provider's SDK and receives an authentication token. For information, see the provider's documentation.\nPost-authentication\nProvider redirects client to\n/.auth/login/<provider>/callback\n.\nClient code posts token from provider to\n/.auth/login/<provider>\nfor validation.\nEstablish authenticated session\nApp Service adds authenticated cookie to response.\nApp Service returns its own authentication token to client code.\nServe authenticated content\nClient includes authentication cookie in subsequent requests (automatically handled by browser).\nClient code presents authentication token in\nX-ZUMO-AUTH\nheader (automatically handled by Mobile Apps client SDKs).\nFor client browsers, App Service can automatically direct all unauthenticated users to\n/.auth/login/<provider>\n. You can also present users with one or more\n/.auth/login/<provider>\nlinks to sign in to your app using their provider of choice.\nAuthorization behavior\nIn the Azure portal, you can configure App Service with many behaviors when an incoming request isn't authenticated.\nAllow unauthenticated requests:\nThis option defers authorization of unauthenticated traffic to your application code. For authenticated requests, App Service also passes along authentication information in the HTTP headers. This option provides more flexibility in handling anonymous requests. It lets you present multiple sign-in providers to your users.\nRequire authentication:\nThis option rejects any unauthenticated traffic to your application. This rejection can be a redirect action to one of the configured identity providers. In these cases, a browser client is redirected to\n/.auth/login/<provider>\nfor the provider you choose. If the anonymous request comes from a native mobile app, the returned response is an\nHTTP 401 Unauthorized\n. You can also configure the rejection to be an\nHTTP 401 Unauthorized\nor\nHTTP 403 Forbidden\nfor all requests.\nCaution\nRestricting access in this way applies to all calls to your app, which may not be desirable for apps wanting a publicly available home page, as in many single-page applications.\nToken store\nApp Service provides a built-in token store, which is a repository of tokens that are associated with the users of your web apps, APIs, or native mobile apps. When you enable authentication with any provider, this token store is immediately available to your app.\nNote\nThe token store is only available when using the built-in authentication feature, and tokens can be accessed via environment variables or HTTP headers.\nLogging and tracing\nIf you enable application logging, authentication and authorization traces are collected directly in your log files. If you see an authentication error that you didn't expect, you can conveniently find all the details by looking in your existing application logs.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Explore authentication and authorization in App Service"
                },
                {
                  "level": 2,
                  "text": "Why use the built-in authentication?"
                },
                {
                  "level": 2,
                  "text": "Identity providers"
                },
                {
                  "level": 2,
                  "text": "How it works"
                },
                {
                  "level": 3,
                  "text": "Authentication flow"
                },
                {
                  "level": 3,
                  "text": "Authorization behavior"
                },
                {
                  "level": 3,
                  "text": "Token store"
                },
                {
                  "level": 3,
                  "text": "Logging and tracing"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [
                "/.auth/login/aad",
                "/.auth/login/facebook",
                "/.auth/login/google",
                "/.auth/login/x",
                "/.auth/login/<providerName>",
                "/.auth/login/github",
                "/.auth/login/apple",
                "/.auth/login/<provider>",
                "/.auth/login/<provider>/callback",
                "/.auth/login/<provider>",
                "X-ZUMO-AUTH",
                "/.auth/login/<provider>",
                "/.auth/login/<provider>",
                "/.auth/login/<provider>",
                "HTTP 401 Unauthorized",
                "HTTP 401 Unauthorized",
                "HTTP 403 Forbidden"
              ],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                },
                {
                  "url": "/en-us/azure/app-service/configure-authentication-provider-aad",
                  "text": "App Service Microsoft Entra platform login"
                },
                {
                  "url": "/en-us/azure/app-service/configure-authentication-provider-facebook",
                  "text": "App Service Facebook login"
                },
                {
                  "url": "/en-us/azure/app-service/configure-authentication-provider-google",
                  "text": "App Service Google login"
                },
                {
                  "url": "/en-us/azure/app-service/configure-authentication-provider-twitter",
                  "text": "App Service X login"
                },
                {
                  "url": "/en-us/azure/app-service/configure-authentication-provider-openid-connect",
                  "text": "App Service OpenID Connect login"
                },
                {
                  "url": "/en-us/azure/app-service/configure-authentication-provider-github",
                  "text": "App Service GitHub login"
                }
              ]
            },
            {
              "number": 6,
              "title": "Discover App Service networking features",
              "url": "https://learn.microsoft.com/en-us/training/modules/introduction-to-azure-app-service/6-network-features",
              "href": "6-network-features",
              "content": "Read in English\nAdd\nAdd to plan\nDiscover App Service networking features\nCompleted\n4 minutes\nBy default, apps hosted in App Service are accessible directly through the internet and can reach internet-hosted endpoints. For many applications, you need to control the inbound and outbound network traffic.\nThere are two main deployment types for Azure App Service:\nThe multitenant public service hosts App Service plans in the Free, Shared, Basic, Standard, Premium, PremiumV2, and PremiumV3 pricing SKUs.\nThe single-tenant App Service Environment (ASE) hosts Isolated SKU App Service plans directly in your Azure virtual network.\nMultitenant App Service networking features\nAzure App Service is a distributed system. The roles that handle incoming HTTP or HTTPS requests are called\nfront ends\n. The roles that host the customer workload are called\nworkers\n. All the roles in an App Service deployment exist in a multitenant network. Because there are many different customers in the same App Service scale unit, you can't connect the App Service network directly to your network.\nInstead of connecting the networks, you need features to handle the various aspects of application communication. The features that handle requests to your app can't be used to solve problems when you're making calls from your app. Likewise, the features that solve problems for calls from your app can't be used to solve problems to your app.\nInbound features\nOutbound features\nApp-assigned address\nHybrid Connections\nAccess restrictions\nGateway-required virtual network integration\nService endpoints\nVirtual network integration\nPrivate endpoints\nYou can mix the features to solve your problems with a few exceptions. The following inbound use cases are examples of how to use App Service networking features to control traffic inbound to your app.\nInbound use case\nFeature\nSupport IP-based SSL needs for your app\nApp-assigned address\nSupport unshared dedicated inbound address for your app\nApp-assigned address\nRestrict access to your app from a set of well-defined addresses\nAccess restrictions\nDefault networking behavior\nAzure App Service scale units support many customers in each deployment. The Free and Shared SKU plans host customer workloads on multitenant workers. The Basic and higher plans host customer workloads that are dedicated to only one App Service plan. If you have a Standard App Service plan, all the apps in that plan run on the same worker. If you scale out the worker, all the apps in that App Service plan are replicated on a new worker for each instance in your App Service plan.\nOutbound addresses\nThe worker virtual machines are broken down in large part by the App Service plans. The Free, Shared, Basic, Standard, and Premium plans all use the same worker virtual machine type. The PremiumV2 plan uses another virtual machine type. PremiumV3 uses yet another virtual machine type. When you change the virtual machine family, you get a different set of outbound addresses.\nThere are many addresses that are used for outbound calls. The outbound addresses used by your app for making outbound calls are listed in the properties for your app. These addresses are shared by all the apps running on the same worker virtual machine family in the App Service deployment. If you want to see all the addresses that your app might use in a scale unit, there's a property called\npossibleOutboundIpAddresses\nthat lists them.\nFind outbound IPs\nTo find the outbound IP addresses currently used by your app in the Azure portal, select\nProperties\nin your app's left-hand navigation.\nYou can find the same information by running the following Azure CLI command in the Cloud Shell. They're listed in the\nAdditional Outbound IP Addresses\nfield.\naz webapp show \\\n    --resource-group <group_name> \\\n    --name <app_name> \\ \n    --query outboundIpAddresses \\\n    --output tsv\nTo find all possible outbound IP addresses for your app, regardless of pricing tiers, run the following command in the Cloud Shell.\naz webapp show \\\n    --resource-group <group_name> \\ \n    --name <app_name> \\ \n    --query possibleOutboundIpAddresses \\\n    --output tsv\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Discover App Service networking features"
                },
                {
                  "level": 2,
                  "text": "Multitenant App Service networking features"
                },
                {
                  "level": 2,
                  "text": "Default networking behavior"
                },
                {
                  "level": 3,
                  "text": "Outbound addresses"
                },
                {
                  "level": 3,
                  "text": "Find outbound IPs"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [
                "possibleOutboundIpAddresses",
                "az webapp show \\\n    --resource-group <group_name> \\\n    --name <app_name> \\ \n    --query outboundIpAddresses \\\n    --output tsv",
                "az webapp show \\\n    --resource-group <group_name> \\\n    --name <app_name> \\ \n    --query outboundIpAddresses \\\n    --output tsv",
                "az webapp show \\\n    --resource-group <group_name> \\ \n    --name <app_name> \\ \n    --query possibleOutboundIpAddresses \\\n    --output tsv",
                "az webapp show \\\n    --resource-group <group_name> \\ \n    --name <app_name> \\ \n    --query possibleOutboundIpAddresses \\\n    --output tsv"
              ],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 7,
              "title": "Exercise: Deploy a containerized app to Azure App Service",
              "url": "https://learn.microsoft.com/en-us/training/modules/introduction-to-azure-app-service/7-exercise-deploy-containerized-app",
              "href": "7-exercise-deploy-containerized-app",
              "content": "Read in English\nAdd\nAdd to plan\nExercise: Deploy a containerized app to Azure App Service\nCompleted\n15 minutes\nIn this exercise, you deploy a containerized app to Azure App Service.\nTasks performed in this exercise:\nCreate an Azure App Service resource and deploy a containerized app\nView the results\nClean up resources\nBefore you start\nTo complete the exercise, you need:\nAn Azure subscription. If you don't already have one, you can\nsign up for one\n.\nGetting started\nSelect the\nLaunch Exercise\nbutton, it opens the exercise instructions in a new browser window. When you're finished with the exercise, return here for:\nA quick knowledge check\nA summary of what you've learned\nTo earn a badge for completing this module\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Exercise: Deploy a containerized app to Azure App Service"
                },
                {
                  "level": 2,
                  "text": "Before you start"
                },
                {
                  "level": 2,
                  "text": "Getting started"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [
                {
                  "src": "../../wwl-azure/introduction-to-azure-app-service/media/launch-exercise.png",
                  "absolute_url": "https://learn.microsoft.com/en-us/wwl-azure/introduction-to-azure-app-service/media/launch-exercise.png",
                  "alt_text": "Button to launch exercise.",
                  "title": "",
                  "filename": "launch-exercise.png",
                  "image_type": "icon",
                  "context": {
                    "preceding_heading": "Getting started",
                    "following_text": "Was this page helpful?",
                    "figure_caption": "",
                    "parent_section": ""
                  }
                }
              ],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                },
                {
                  "url": "https://azure.microsoft.com/",
                  "text": "sign up for one"
                }
              ]
            },
            {
              "number": 8,
              "title": "Module assessment",
              "url": "https://learn.microsoft.com/en-us/training/modules/introduction-to-azure-app-service/8-knowledge-check",
              "href": "8-knowledge-check",
              "content": "Read in English\nAdd\nAdd to plan\nModule assessment\nCompleted\n3 minutes\nChoose the best response for each of the questions below.\nCheck your knowledge\n1.\nWhich of the following App Service plan categories provides the maximum scale-out capabilities?\nDedicated compute\nIsolated\nShared compute\n2.\nWhich of the following networking features of App Service can be used to control outbound network traffic?\nApp-assigned address\nHybrid Connections\nService endpoints\n3.\nWhat is the purpose of the Azure App Service Environment feature?\nIt provides a shared infrastructure for running App Service apps.\nIt allows for the deployment and running of containerized web apps on Windows and Linux.\nIt provides a fully isolated and dedicated environment for running App Service apps with improved security at high scale.\n4.\nWhat determines the set of compute resources for a web app to run in Azure App Service?\nThe geographical region where the app is deployed\nThe pricing tier of the app\nThe App Service plan\n5.\nWhat is the purpose of using deployment slots in Azure App Service?\nTo deploy an app to a staging environment and then swap staging and production slots, eliminating downtime\nTo increase the storage capacity of the application\nTo add up to nine sidecar containers for each sidecar-enabled custom container app\nYou must answer all questions before checking your work.\nYou must answer all questions before checking your work.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Module assessment"
                },
                {
                  "level": 2,
                  "text": "Check your knowledge"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 9,
              "title": "Summary",
              "url": "https://learn.microsoft.com/en-us/training/modules/introduction-to-azure-app-service/9-summary",
              "href": "9-summary",
              "content": "Read in English\nAdd\nAdd to plan\nSummary\nCompleted\n3 minutes\nIn this module, you learned how to:\nDescribe Azure App Service key components and value.\nExplain how Azure App Service manages authentication and authorization.\nIdentify methods to control inbound and outbound traffic to your web app.\nDeploy a containerized app to App Service.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Summary"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            }
          ]
        },
        {
          "title": "Configure web app settings",
          "url": "https://learn.microsoft.com/en-us/training/modules/configure-web-app-settings/",
          "description": "",
          "learning_objectives": [],
          "prerequisites": [],
          "units": [
            {
              "number": 1,
              "title": "Introduction",
              "url": "https://learn.microsoft.com/en-us/training/modules/configure-web-app-settings/1-introduction",
              "href": "1-introduction",
              "content": "Read in English\nAdd\nAdd to plan\nIntroduction\nCompleted\n3 minutes\nIn App Service, app settings are variables passed as environment variables to the application code.\nLearning objectives\nAfter completing this module, you'll be able to:\nCreate application settings that are bound to deployment slots.\nExplain the options for installing SSL/TLS certificates for your app.\nEnable diagnostic logging for your app to aid in monitoring and debugging.\nCreate virtual app to directory mappings.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Introduction"
                },
                {
                  "level": 2,
                  "text": "Learning objectives"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 2,
              "title": "Configure application settings",
              "url": "https://learn.microsoft.com/en-us/training/modules/configure-web-app-settings/2-configure-application-settings",
              "href": "2-configure-application-settings",
              "content": "Read in English\nAdd\nAdd to plan\nConfigure application settings\nCompleted\n5 minutes\nIn App Service, app settings are variables passed as environment variables to the application code. For Linux apps and custom containers, App Service passes app settings to the container using the\n--env\nflag to set the environment variable in the container. In either case, they're injected into your app environment at app startup. When you add, remove, or edit app settings, App Service triggers an app restart.\nFor ASP.NET and ASP.NET Core developers, setting app settings in App Service is like setting them in\n<appSettings>\nin\nWeb.config\nor\nappsettings.json\n, but the values in App Service override the ones in\nWeb.config\nor\nappsettings.json\n. You can keep development settings (for example, local MySQL password) in\nWeb.config\nor\nappsettings.json\nand production secrets (for example, Azure MySQL database password) safely in App Service. The same code uses your development settings when you debug locally, and it uses your production secrets when deployed to Azure.\nApp settings are always encrypted when stored (encrypted-at-rest). App settings names can only contain letters, numbers (0-9), periods (\".\"), and underscores (\"_\")\nSpecial characters in the value of an App Setting must be escaped as needed by the target OS.\nApplication settings can be accessed by navigating to your app's management page and selecting\nEnvironment variables > Application settings\n.\nAdding and editing settings\nTo add a new app setting, select\n+ Add\n. If you're using deployment slots, you can specify if your setting is swappable or not. In the dialog, you can stick the setting to the current slot.\nWhen finished, select\nApply\n. Don't forget to select\nApply\nback in the\nEnvironment variables\npage.\nNote\nIn a default Linux app service or a custom Linux container, any nested JSON key structure in the app setting name like\nApplicationInsights:InstrumentationKey\nneeds to be configured in App Service as\nApplicationInsights__InstrumentationKey\nfor the key name. In other words, replace any\n:\nwith\n__\n(double underscore). Any periods in the app setting name are replaced with a\n_\n(single underscore).\nEditing application settings in bulk\nTo add or edit app settings in bulk, select the\nAdvanced edit\nbutton. When finished, select\nOK\n. Don't forget to select Apply back in the Environment variables page. App settings have the following JSON formatting:\n[\n  {\n    \"name\": \"<key-1>\",\n    \"value\": \"<value-1>\",\n    \"slotSetting\": false\n  },\n  {\n    \"name\": \"<key-2>\",\n    \"value\": \"<value-2>\",\n    \"slotSetting\": false\n  },\n  ...\n]\nConfigure connection strings\nFor ASP.NET and ASP.NET Core developers, setting connection strings in App Service are like setting them in\n<connectionStrings>\nin\nWeb.config\n, but the values you set in App Service override the ones in\nWeb.config\n. For other language stacks, it's better to use app settings instead, because connection strings require special formatting in the variable keys in order to access the values.\nTip\nThere's one case where you may want to use connection strings instead of app settings for non-.NET languages: certain Azure database types are backed up along with the app\nonly\nif you configure a connection string for the database in your App Service app.\nAdding and editing connection strings follow the same principles as other app settings and they can also be tied to deployment slots. An example of connection strings in JSON formatting that you would use for bulk adding or editing:\n[\n  {\n    \"name\": \"name-1\",\n    \"value\": \"conn-string-1\",\n    \"type\": \"SQLServer\",\n    \"slotSetting\": false\n  },\n  {\n    \"name\": \"name-2\",\n    \"value\": \"conn-string-2\",\n    \"type\": \"PostgreSQL\",\n    \"slotSetting\": false\n  },\n  ...\n]\nNote\n.NET apps targeting PostgreSQL should set the connection string to\nCustom\nas work around for a known issue in .NET\nEnvironmentVariablesConfigurationProvider\n.\nAt runtime, connection strings are available as environment variables, prefixed with the following connection types:\nSQLServer:\nSQLCONNSTR_\nMySQL:\nMYSQLCONNSTR_\nSQLAzure:\nSQLAZURECONNSTR_\nCustom:\nCUSTOMCONNSTR_\nPostgreSQL:\nPOSTGRESQLCONNSTR_\nNotification Hub:\nNOTIFICATIONHUBCONNSTR_\nService Bus:\nSERVICEBUSCONNSTR_\nEvent Hub:\nEVENTHUBCONNSTR_\nDocument DB:\nDOCDBCONNSTR_\nRedis Cache:\nREDISCACHECONNSTR_\nFor example, a MySQL connection string named\nconnectionstring1\ncan be accessed as the environment variable\nMYSQLCONNSTR_connectionString1\n.\nConfigure environment variables for custom containers\nYour custom container might use environment variables that need to be supplied externally. You can pass them in via the Cloud Shell. In Bash:\naz webapp config appsettings set --resource-group <group-name> --name <app-name> --settings key1=value1 key2=value2\nIn PowerShell:\nSet-AzWebApp -ResourceGroupName <group-name> -Name <app-name> -AppSettings @{\"DB_HOST\"=\"myownserver.mysql.database.azure.com\"}\nWhen your app runs, the App Service app settings are injected into the process as environment variables automatically. You can verify container environment variables with the URL\nhttps://<app-name>.scm.azurewebsites.net/Env\n.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Configure application settings"
                },
                {
                  "level": 2,
                  "text": "Adding and editing settings"
                },
                {
                  "level": 3,
                  "text": "Editing application settings in bulk"
                },
                {
                  "level": 2,
                  "text": "Configure connection strings"
                },
                {
                  "level": 2,
                  "text": "Configure environment variables for custom containers"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [
                "<appSettings>",
                "ApplicationInsights:InstrumentationKey",
                "ApplicationInsights__InstrumentationKey",
                "[\n  {\n    \"name\": \"<key-1>\",\n    \"value\": \"<value-1>\",\n    \"slotSetting\": false\n  },\n  {\n    \"name\": \"<key-2>\",\n    \"value\": \"<value-2>\",\n    \"slotSetting\": false\n  },\n  ...\n]",
                "[\n  {\n    \"name\": \"<key-1>\",\n    \"value\": \"<value-1>\",\n    \"slotSetting\": false\n  },\n  {\n    \"name\": \"<key-2>\",\n    \"value\": \"<value-2>\",\n    \"slotSetting\": false\n  },\n  ...\n]",
                "<connectionStrings>",
                "[\n  {\n    \"name\": \"name-1\",\n    \"value\": \"conn-string-1\",\n    \"type\": \"SQLServer\",\n    \"slotSetting\": false\n  },\n  {\n    \"name\": \"name-2\",\n    \"value\": \"conn-string-2\",\n    \"type\": \"PostgreSQL\",\n    \"slotSetting\": false\n  },\n  ...\n]",
                "[\n  {\n    \"name\": \"name-1\",\n    \"value\": \"conn-string-1\",\n    \"type\": \"SQLServer\",\n    \"slotSetting\": false\n  },\n  {\n    \"name\": \"name-2\",\n    \"value\": \"conn-string-2\",\n    \"type\": \"PostgreSQL\",\n    \"slotSetting\": false\n  },\n  ...\n]",
                "EnvironmentVariablesConfigurationProvider",
                "SQLCONNSTR_",
                "MYSQLCONNSTR_",
                "SQLAZURECONNSTR_",
                "CUSTOMCONNSTR_",
                "POSTGRESQLCONNSTR_",
                "NOTIFICATIONHUBCONNSTR_",
                "SERVICEBUSCONNSTR_",
                "EVENTHUBCONNSTR_",
                "DOCDBCONNSTR_",
                "REDISCACHECONNSTR_",
                "MYSQLCONNSTR_connectionString1",
                "az webapp config appsettings set --resource-group <group-name> --name <app-name> --settings key1=value1 key2=value2",
                "az webapp config appsettings set --resource-group <group-name> --name <app-name> --settings key1=value1 key2=value2",
                "Set-AzWebApp -ResourceGroupName <group-name> -Name <app-name> -AppSettings @{\"DB_HOST\"=\"myownserver.mysql.database.azure.com\"}",
                "Set-AzWebApp -ResourceGroupName <group-name> -Name <app-name> -AppSettings @{\"DB_HOST\"=\"myownserver.mysql.database.azure.com\"}",
                "https://<app-name>.scm.azurewebsites.net/Env"
              ],
              "images": [
                {
                  "src": "../../wwl-azure/configure-web-app-settings/media/configure-app-settings.png",
                  "absolute_url": "https://learn.microsoft.com/en-us/wwl-azure/configure-web-app-settings/media/configure-app-settings.png",
                  "alt_text": "Screenshot of Navigating to Environment variables > Application settings.",
                  "title": "",
                  "filename": "configure-app-settings.png",
                  "image_type": "screenshot",
                  "context": {
                    "preceding_heading": "Configure application settings",
                    "following_text": "To add a new app setting, select+ Add. If you're using deployment slots, you can specify if your setting is swappable or not. In the dialog, you can stick the setting to the current slot.",
                    "figure_caption": "",
                    "parent_section": ""
                  }
                },
                {
                  "src": "../../wwl-azure/configure-web-app-settings/media/app-configure-slotsetting.png",
                  "absolute_url": "https://learn.microsoft.com/en-us/wwl-azure/configure-web-app-settings/media/app-configure-slotsetting.png",
                  "alt_text": "Selecting deployment slot setting to stick the setting to the current slot.",
                  "title": "",
                  "filename": "app-configure-slotsetting.png",
                  "image_type": "illustration",
                  "context": {
                    "preceding_heading": "Adding and editing settings",
                    "following_text": "When finished, selectApply. Don't forget to selectApplyback in theEnvironment variablespage.",
                    "figure_caption": "",
                    "parent_section": ""
                  }
                }
              ],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 3,
              "title": "Configure general settings",
              "url": "https://learn.microsoft.com/en-us/training/modules/configure-web-app-settings/3-configure-general-settings",
              "href": "3-configure-general-settings",
              "content": "Read in English\nAdd\nAdd to plan\nConfigure general settings\nCompleted\n4 minutes\nIn the\nConfiguration > General settings\nsection you can configure some common settings for your app. Some settings require you to scale up to higher pricing tiers.\nA list of the currently available settings:\nStack settings\n: The software stack to run the app, including the language and SDK versions. For Linux apps and custom container apps, you can also set an optional start-up command or file.\nPlatform settings\n: Lets you configure settings for the hosting platform, including:\nPlatform bitness\n: 32-bit or 64-bit. For Windows apps only.\nFTP state\n: Allow only FTPS or disable FTP altogether.\nHTTP version\n: Set to\n2.0\nto enable support for HTTPS/2 protocol.\nNote\nMost modern browsers support HTTP/2 protocol over TLS only, while nonencrypted traffic continues to use HTTP/1.1. To ensure that client browsers connect to your app with HTTP/2, secure your custom DNS name.\nWeb sockets\n: For ASP.NET SignalR or socket.io, for example.\nAlways On\n: Keeps the app loaded even when there's no traffic. When\nAlways On\nisn't turned on (default), the app is unloaded after 20 minutes without any incoming requests. The unloaded app can cause high latency for new requests because of its warm-up time. When\nAlways On\nis turned on, the front-end load balancer sends a GET request to the application root every five minutes. The continuous ping prevents the app from being unloaded.\nAlways On is required for continuous WebJobs or for WebJobs that are triggered using a CRON expression.\nARR affinity\n: In a multi-instance deployment, ensure that the client is routed to the same instance for the life of the session. You can set this option to\nOff\nfor stateless applications.\nHTTPS Only\n: When enabled, all HTTP traffic is redirected to HTTPS.\nMinimum TLS version\n: Select the minimum TLS encryption version required by your app.\nDebugging\n: Enable remote debugging for ASP.NET, ASP.NET Core, or Node.js apps. This option turns off automatically after 48 hours.\nIncoming client certificates\n: Require client certificates in mutual authentication. TLS mutual authentication is used to restrict access to your app by enabling different types of authentication for it.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Configure general settings"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [
                {
                  "src": "../../wwl-azure/configure-web-app-settings/media/configure-general-settings.png",
                  "absolute_url": "https://learn.microsoft.com/en-us/wwl-azure/configure-web-app-settings/media/configure-general-settings.png",
                  "alt_text": "Screenshot of navigating to Configure > General settings.",
                  "title": "",
                  "filename": "configure-general-settings.png",
                  "image_type": "screenshot",
                  "context": {
                    "preceding_heading": "Configure general settings",
                    "following_text": "A list of the currently available settings:",
                    "figure_caption": "",
                    "parent_section": ""
                  }
                },
                {
                  "src": "../../wwl-azure/configure-web-app-settings/media/open-general-linux.png",
                  "absolute_url": "https://learn.microsoft.com/en-us/wwl-azure/configure-web-app-settings/media/open-general-linux.png",
                  "alt_text": "Establishing the stack settings that include the programming language.",
                  "title": "",
                  "filename": "open-general-linux.png",
                  "image_type": "illustration",
                  "context": {
                    "preceding_heading": "Configure general settings",
                    "following_text": "Platform settings: Lets you configure settings for the hosting platform, including:",
                    "figure_caption": "",
                    "parent_section": ""
                  }
                }
              ],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 4,
              "title": "Configure path mappings",
              "url": "https://learn.microsoft.com/en-us/training/modules/configure-web-app-settings/4-configure-path-mappings",
              "href": "4-configure-path-mappings",
              "content": "Read in English\nAdd\nAdd to plan\nConfigure path mappings\nCompleted\n3 minutes\nIn the\nConfiguration > Path mappings\nsection you can configure handler mappings, and virtual application and directory mappings. The\nPath mappings\npage displays different options based on the OS type.\nWindows apps (uncontainerized)\nFor Windows apps, you can customize the IIS handler mappings and virtual applications and directories.\nHandler mappings let you add custom script processors to handle requests for specific file extensions. To add a custom handler, select\nNew handler mapping\n. Configure the handler as follows:\nExtension\n: The file extension you want to handle, such as *\n.php\nor\nhandler.fcgi\n.\nScript processor\n: The absolute path of the script processor. Requests to files that match the file extension are processed by the script processor. Use the path\nD:\\home\\site\\wwwroot\nto refer to your app's root directory.\nArguments\n: Optional command-line arguments for the script processor.\nEach app has the default root path (\n/\n) mapped to\nD:\\home\\site\\wwwroot\n, where your code is deployed by default. If your app root is in a different folder, or if your repository has more than one application, you can edit or add virtual applications and directories.\nYou can configure virtual applications and directories by specifying each virtual directory and its corresponding physical path relative to the website root (\nD:\\home\n). To mark a virtual directory as a web application, clear the\nDirectory\ncheck box.\nLinux and containerized apps\nYou can add custom storage for your containerized app. Containerized apps include all Linux apps and also the Windows and Linux custom containers running on App Service. Select\nNew Azure Storage Mount\nand configure your custom storage as follows:\nName\n: The display name.\nConfiguration options\n:\nBasic\nor\nAdvanced\n. Select\nBasic\nif the storage account isn't using service endpoints, private endpoints, or Azure Key Vault. Otherwise, select\nAdvanced\n.\nStorage accounts\n: The storage account with the container you want.\nStorage type\n:\nAzure Blobs\nor\nAzure Files\n. Windows container apps only support Azure Files. Azure Blobs only supports read-only access.\nStorage container\n: For basic configuration, the container you want.\nShare name\n: For advanced configuration, the file share name.\nAccess key\n: For advanced configuration, the access key.\nMount path\n: The absolute path in your container to mount the custom storage.\nDeployment slot setting\n: When checked, the storage mount settings also apply to deployment slots.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Configure path mappings"
                },
                {
                  "level": 2,
                  "text": "Windows apps (uncontainerized)"
                },
                {
                  "level": 2,
                  "text": "Linux and containerized apps"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [
                "D:\\home\\site\\wwwroot",
                "D:\\home\\site\\wwwroot"
              ],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 5,
              "title": "Enable diagnostic logging",
              "url": "https://learn.microsoft.com/en-us/training/modules/configure-web-app-settings/5-enable-diagnostic-logging",
              "href": "5-enable-diagnostic-logging",
              "content": "Read in English\nAdd\nAdd to plan\nEnable diagnostic logging\nCompleted\n6 minutes\nThere are built-in diagnostics to assist with debugging an App Service app. In this lesson, you learn how to enable diagnostic logging and add instrumentation to your application, and how to access the information logged by Azure.\nThe following table shows the types of logging, the platforms supported, and where the logs can be stored and located for accessing the information.\nType\nPlatform\nLocation\nDescription\nApplication logging\nWindows, Linux\nApp Service file system and/or Azure Storage blobs\nLogs messages generated by your application code. The messages are generated by the web framework you choose, or from your application code directly using the standard logging pattern of your language. Each message is assigned one of the following categories:\nCritical\n,\nError\n,\nWarning\n,\nInfo\n,\nDebug\n, and\nTrace\n.\nWeb server logging\nWindows\nApp Service file system or Azure Storage blobs\nRaw HTTP request data in the W3C extended log file format. Each log message includes data like the HTTP method, resource URI, client IP, client port, user agent, response code, and so on.\nDetailed error messages\nWindows\nApp Service file system\nCopies of the\n.html\nerror pages that would otherwise be sent to the client browser. For security reasons, detailed error pages shouldn't be sent to clients in production, but App Service can save the error page each time an application error occurs that has HTTP code 400 or greater.\nFailed request tracing\nWindows\nApp Service file system\nDetailed tracing information on failed requests, including a trace of the IIS components used to process the request and the time taken in each component. One folder is generated for each failed request, which contains the XML log file, and the XSL stylesheet to view the log file with.\nDeployment logging\nWindows, Linux\nApp Service file system\nHelps determine why a deployment failed. Deployment logging happens automatically and there are no configurable settings for deployment logging.\nEnable application logging (Windows)\nTo enable application logging for Windows apps in the Azure portal, navigate to your app and select\nApp Service logs\n.\nSelect\nOn\nfor either\nApplication Logging (Filesystem)\nor\nApplication Logging (Blob)\n, or both. The\nFilesystem\noption is for temporary debugging purposes, and turns itself off in 12 hours. The\nBlob\noption is for long-term logging, and needs a blob storage container to write logs to.\nNote\nIf you regenerate your storage account's access keys, you must reset the respective logging configuration to use the updated access keys. To do this turn the logging feature off and then on again.\nYou can also set the\nLevel\nof details included in the log as shown in the following table.\nLevel\nIncluded categories\nDisabled\nNone\nError\nError, Critical\nWarning\nWarning, Error, Critical\nInformation\nInfo, Warning, Error, Critical\nVerbose\nTrace, Debug, Info, Warning, Error, Critical (all categories)\nWhen finished, select\nSave\n.\nEnable application logging (Linux/Container)\nIn\nApp Service logs\nset the\nApplication logging\noption to\nFile System\n.\nIn\nQuota (MB)\n, specify the disk quota for the application logs. In\nRetention Period (Days)\n, set the number of days the logs should be retained.\nWhen finished, select\nSave\n.\nEnable web server logging\nFor\nWeb server logging\n, select\nStorage\nto store logs on blob storage, or\nFile System\nto store logs on the App Service file system.\nIn\nRetention Period (Days)\n, set the number of days the logs should be retained.\nWhen finished, select\nSave\n.\nAdd log messages in code\nIn your application code, you use the usual logging facilities to send log messages to the application logs. For example:\nASP.NET applications can use the\nSystem.Diagnostics.Trace\nclass to log information to the application diagnostics log. For example:\nSystem.Diagnostics.Trace.TraceError(\"If you're seeing this, something bad happened\");\nBy default, ASP.NET Core uses the\nMicrosoft.Extensions.Logging.AzureAppServices\nlogging provider.\nPython applications can use the\nOpenCensus package\nto send logs to the application diagnostics log.\nStream logs\nBefore you stream logs in real time, enable the log type that you want. Any information written to files ending in .txt, .log, or .htm that are stored in the\n/LogFiles\ndirectory (\nd:/home/logfiles\n) is streamed by App Service.\nNote\nSome types of logging buffer write to the log file, which can result in out of order events in the stream. For example, an application log entry that occurs when a user visits a page may be displayed in the stream before the corresponding HTTP log entry for the page request.\nAzure portal - To stream logs in the Azure portal, navigate to your app and select\nLog stream\n.\nAzure CLI - To stream logs live in Cloud Shell, use the following command:\naz webapp log tail --name appname --resource-group myResourceGroup\nLocal console - To stream logs in the local console, install Azure CLI and sign in to your account. Once signed in, follow the instructions shown for Azure CLI.\nAccess log files\nIf you configure the Azure Storage blobs option for a log type, you need a client tool that works with Azure Storage.\nFor logs stored in the App Service file system, the easiest way is to download the ZIP file in the browser at:\nLinux/container apps:\nhttps://<app-name>.scm.azurewebsites.net/api/logs/docker/zip\nWindows apps:\nhttps://<app-name>.scm.azurewebsites.net/api/dump\nFor Linux/container apps, the ZIP file contains console output logs for both the docker host and the docker container. For a scaled-out app, the ZIP file contains one set of logs for each instance. In the App Service file system, these log files are the contents of the\n/home/LogFiles\ndirectory.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Enable diagnostic logging"
                },
                {
                  "level": 2,
                  "text": "Enable application logging (Windows)"
                },
                {
                  "level": 2,
                  "text": "Enable application logging (Linux/Container)"
                },
                {
                  "level": 2,
                  "text": "Enable web server logging"
                },
                {
                  "level": 2,
                  "text": "Add log messages in code"
                },
                {
                  "level": 2,
                  "text": "Stream logs"
                },
                {
                  "level": 2,
                  "text": "Access log files"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [
                "System.Diagnostics.Trace",
                "System.Diagnostics.Trace.TraceError(\"If you're seeing this, something bad happened\");",
                "System.Diagnostics.Trace.TraceError(\"If you're seeing this, something bad happened\");",
                "Microsoft.Extensions.Logging.AzureAppServices",
                "d:/home/logfiles",
                "az webapp log tail --name appname --resource-group myResourceGroup",
                "az webapp log tail --name appname --resource-group myResourceGroup",
                "https://<app-name>.scm.azurewebsites.net/api/logs/docker/zip",
                "https://<app-name>.scm.azurewebsites.net/api/dump"
              ],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                },
                {
                  "url": "/en-us/azure/azure-monitor/app/opencensus-python",
                  "text": "OpenCensus package"
                }
              ]
            },
            {
              "number": 6,
              "title": "Configure security certificates",
              "url": "https://learn.microsoft.com/en-us/training/modules/configure-web-app-settings/6-configure-security-certificates",
              "href": "6-configure-security-certificates",
              "content": "Read in English\nAdd\nAdd to plan\nConfigure security certificates\nCompleted\n4 minutes\nAzure App Service has tools that let you  create, upload, or import a private certificate or a public certificate into App Service.\nA certificate uploaded into an app is stored in a deployment unit that is bound to the app service plan's resource group and region combination (internally called a\nwebspace\n). The certificate is accessible to other apps in the same resource group and region combination.\nThe table below details the options you have for adding certificates in App Service:\nOption\nDescription\nCreate a free App Service managed certificate\nA private certificate that's free of charge and easy to use if you just need to secure your custom domain in App Service.\nPurchase an App Service certificate\nA private certificate managed by Azure. It combines the simplicity of automated certificate management and the flexibility of renewal and export options.\nImport a certificate from Key Vault\nUseful if you use Azure Key Vault to manage your certificates.\nUpload a private certificate\nIf you already have a private certificate from a third-party provider, you can upload it.\nUpload a public certificate\nPublic certificates aren't used to secure custom domains, but you can load them into your code if you need them to access remote resources.\nPrivate certificate requirements\nThe free\nApp Service managed certificate\nand the\nApp Service certificate\nalready satisfy the requirements of App Service. If you want to use a private certificate in App Service, your certificate must meet the following requirements:\nExported as a password-protected PFX file, encrypted using triple DES.\nContains private key at least 2,048 bits long.\nContains all intermediate certificates and the root certificate in the certificate chain.\nTo secure a custom domain in a TLS binding, the certificate has other requirements:\nContains an Extended Key Usage for server authentication (OID = 1.3.6.1.5.5.7.3.1)\nSigned by a trusted certificate authority\nCreating a free managed certificate\nTo create custom TLS/SSL bindings or enable client certificates for your App Service app, your App Service plan must be in the\nBasic\n,\nStandard\n,\nPremium\n, or\nIsolated\ntier.\nThe free App Service managed certificate is a turn-key solution for securing your custom DNS name in App Service. It's a TLS/SSL server certificate fully managed by App Service and renewed continuously and automatically in six-month increments, 45 days before expiration. You create the certificate and bind it to a custom domain, and let App Service do the rest.\nImportant\nBefore you create a free managed certificate, make sure you meet the prerequisites for your app. Free certificates are issued by DigiCert. For some domains, you must explicitly allow DigiCert as a certificate issuer by creating a CAA domain record with the value:\n0 issue digicert.com\n. Azure fully manages the certificates on your behalf, so any aspect of the managed certificate, including the root issuer, can change at any time. These changes are outside your control. Make sure to avoid hard dependencies and \"pinning\" practice certificates to the managed certificate or any part of the certificate hierarchy.\nThe free certificate comes with the following limitations:\nDoesn't support wildcard certificates.\nDoesn't support usage as a client certificate by using certificate thumbprint, which is planned for deprecation and removal.\nDoesn't support private DNS.\nIsn't exportable.\nIsn't supported in an App Service Environment (ASE).\nOnly supports alphanumeric characters, dashes (-), and periods (.).\nOnly custom domains of length up to 64 characters are supported.\nImport an App Service Certificate\nIf you purchase an App Service Certificate from Azure, Azure manages the following tasks:\nTakes care of the purchase process from certificate provider.\nPerforms domain verification of the certificate.\nMaintains the certificate in Azure Key Vault.\nManages certificate renewal.\nSynchronize the certificate automatically with the imported copies in App Service apps.\nIf you already have a working App Service certificate, you can:\nImport the certificate into App Service.\nManage the certificate, such as renew, rekey, and export it.\nNote\nApp Service Certificates aren't supported in Azure National Clouds at this time.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Configure security certificates"
                },
                {
                  "level": 2,
                  "text": "Private certificate requirements"
                },
                {
                  "level": 2,
                  "text": "Creating a free managed certificate"
                },
                {
                  "level": 2,
                  "text": "Import an App Service Certificate"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [
                "0 issue digicert.com"
              ],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 7,
              "title": "Module assessment",
              "url": "https://learn.microsoft.com/en-us/training/modules/configure-web-app-settings/7-knowledge-check",
              "href": "7-knowledge-check",
              "content": "Read in English\nAdd\nAdd to plan\nModule assessment\nCompleted\n3 minutes\nCheck your knowledge\n1.\nIn which of the following app configuration settings categories would you set the stack and SDK version?\nApplication settings\nPath mappings\nGeneral settings\n2.\nWhich of the following types of application logging is supported on the Linux platform?\nWeb server logging\nFailed request tracing\nDeployment logging\nYou must answer all questions before checking your work.\nYou must answer all questions before checking your work.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Module assessment"
                },
                {
                  "level": 2,
                  "text": "Check your knowledge"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 8,
              "title": "Summary",
              "url": "https://learn.microsoft.com/en-us/training/modules/configure-web-app-settings/8-summary",
              "href": "8-summary",
              "content": "Read in English\nAdd\nAdd to plan\nSummary\nCompleted\n2 minutes\nIn this module, you learned how to:\nCreate application settings that are bound to deployment slots.\nExplain the options for installing SSL/TLS certificates for your app.\nEnable diagnostic logging for your app to aid in monitoring and debugging.\nCreate virtual app to directory mappings.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Summary"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            }
          ]
        },
        {
          "title": "Scale apps in Azure App Service",
          "url": "https://learn.microsoft.com/en-us/training/modules/scale-apps-app-service/",
          "description": "",
          "learning_objectives": [],
          "prerequisites": [],
          "units": [
            {
              "number": 1,
              "title": "Introduction",
              "url": "https://learn.microsoft.com/en-us/training/modules/scale-apps-app-service/1-introduction",
              "href": "1-introduction",
              "content": "Read in English\nAdd\nAdd to plan\nIntroduction\nCompleted\n3 minutes\nAutoscaling enables a system to adjust the resources required to meet the varying demand from users, while controlling the costs associated with these resources. You can use autoscaling with many Azure services, including web applications. Autoscaling requires you to configure autoscale rules that specify the conditions under which resources should be added or removed.\nLearning objectives\nAfter completing this module, you'll be able to:\nIdentify scenarios for which autoscaling is an appropriate solution\nCreate autoscaling rules for a web app\nMonitor the effects of autoscaling\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Introduction"
                },
                {
                  "level": 2,
                  "text": "Learning objectives"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 2,
              "title": "Examine scale out options",
              "url": "https://learn.microsoft.com/en-us/training/modules/scale-apps-app-service/2-autoscale-factors",
              "href": "2-autoscale-factors",
              "content": "Read in English\nAdd\nAdd to plan\nExamine scale out options\nCompleted\n4 minutes\nAzure App Service supports manual scaling, and two options for scaling out your web apps automatically:\nAutoscaling with Azure\nautoscale\n. Autoscaling makes scaling decisions based on rules that you define.\nAzure App Service\nautomatic scaling\n. Automatic scaling makes scaling decisions for you based on the parameters that you select.\nThe following table highlights the differences between the two automatic scaling options:\nFactor\nAutoscale\nAutomatic scaling\nAvailable pricing tiers\nStandard and Up\nPremium V2 (P1V2, P2V2, P3V2) and Premium V3 (P0V3, P1V3, P2V3, P3V3, P1MV3, P2MV3, P3MV3, P4MV3, P5MV3) pricing tiers\nRule-based scaling\nYes\nNo, the platform manages the scale-out and in based on HTTP traffic.\nSchedule-based scaling\nYes\nNo\nAlways ready instances\nNo, your web app runs on other instances available during the scale-out operation, based on threshold defined for autoscale rules.\nYes (minimum 1)\nPrewarmed instances\nNo\nYes (default 1)\nPer-app maximum\nNo\nYes\nWhat is autoscaling?\nAutoscaling is a cloud system or process that adjusts available resources based on the current demand. Autoscaling performs scaling\nin and out\n, as opposed to scaling\nup and down\n.\nAutoscaling can be triggered according to a schedule, or by assessing whether the system is running short on resources. For example, autoscaling could be triggered if CPU utilization grows, memory occupancy increases, the number of incoming requests to a service appears to be surging, or some combination of factors.\nAzure App Service autoscaling\nAutoscaling in Azure App Service monitors the resource metrics of a web app as it runs. It detects situations where other resources are required to handle an increasing workload, and ensures those resources are available before the system becomes overloaded.\nAutoscaling responds to changes in the environment by adding or removing web servers and balancing the load between them. Autoscaling doesn't have any effect on the CPU power, memory, or storage capacity of the web servers powering the app, it only changes the number of web servers.\nAutoscaling rules\nAutoscaling makes its decisions based on rules that you define. A rule specifies the threshold for a metric, and triggers an autoscale event when this threshold is crossed. Autoscaling can also deallocate resources when the workload lessens.\nDefine your autoscaling rules carefully. For example, a Denial of Service attack can result in a large-scale influx of incoming traffic. Trying to handle a surge in requests caused by a DoS attack would be fruitless and expensive. These requests aren't genuine, and should be discarded rather than processed. A better solution is to implement detection and filtering of requests that occur during such an attack before they reach your service.\nWhen should you consider autoscaling?\nAutoscaling provides elasticity for your services. For example, you might expect increased/reduced activity for a business app during holidays.\nAutoscaling improves availability and fault tolerance. It can help ensure that client requests to a service aren't denied because an instance is either: not able to acknowledge the request in a timely manner; or because an instance has crashed.\nAutoscaling works by adding or removing web servers. If your web apps perform  resource-intensive processing as part of each request, then autoscaling might not be an effective approach. In these situations, manually scaling up may be necessary. For example, if a request sent to a web app involves performing complex processing over a large dataset, depending on the instance size, this single request could exhaust the processing and memory capacity of the instance.\nAutoscaling isn't the best approach to handling long-term growth. You might have a web app that starts with a few users, but increases in popularity over time. Autoscaling has an overhead associated with monitoring resources and determining whether to trigger a scaling event. In this scenario, if you can anticipate the rate of growth, manually scaling the system over time may be a more cost effective approach.\nThe number of instances of a service is also a factor. You might expect to run only a few instances of a service most of the time. However, in this situation, your service is susceptible to downtime or lack of availability whether autoscaling is enabled or not. The fewer the number of instances initially, the less capacity you have to handle an increasing workload while autoscaling spins up more instances.\nAzure App Service automatic scaling\nYou enable automatic scaling for an App Service Plan and configure a range of instances for each of the web apps. As your web app starts receiving HTTP traffic, App Service monitors the load and adds instances. Resources may be shared when multiple web apps within an App Service Plan are required to scale out simultaneously.\nHere are a few scenarios where you should scale out automatically:\nYou don't want to set up autoscale rules based on resource metrics.\nYou want your web apps within the same App Service Plan to scale differently and independently of each other.\nYour web app is connected to a database or legacy system, which may not scale as fast as the web app. Scaling automatically allows you to set the maximum number of instances your App Service Plan can scale to. This setting helps the web app to not overwhelm the backend.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Examine scale out options"
                },
                {
                  "level": 2,
                  "text": "What is autoscaling?"
                },
                {
                  "level": 2,
                  "text": "Azure App Service autoscaling"
                },
                {
                  "level": 3,
                  "text": "Autoscaling rules"
                },
                {
                  "level": 3,
                  "text": "When should you consider autoscaling?"
                },
                {
                  "level": 2,
                  "text": "Azure App Service automatic scaling"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 3,
              "title": "Identify autoscale factors",
              "url": "https://learn.microsoft.com/en-us/training/modules/scale-apps-app-service/3-app-service-autoscale-conditions-rules",
              "href": "3-app-service-autoscale-conditions-rules",
              "content": "Read in English\nAdd\nAdd to plan\nIdentify autoscale factors\nCompleted\n3 minutes\nAutoscaling enables you to specify the conditions under which a web app should be scaled out, and back in again. Effective autoscaling ensures sufficient resources are available to handle large volumes of requests at peak times, while managing costs when the demand drops.\nYou can configure autoscaling to detect when to scale in and out according to a combination of factors, based on resource usage. You can also configure autoscaling to occur according to a schedule.\nIn this unit, you learn how to specify the factors that can be used to autoscale a service.\nAutoscaling and the App Service Plan\nAutoscaling is a feature of the App Service Plan used by the web app. When the web app scales out, Azure starts new instances of the hardware defined by the App Service Plan to the app.\nTo prevent runaway autoscaling, an App Service Plan has an instance limit. Plans in more expensive pricing tiers have a higher limit. Autoscaling can't create more instances than this limit.\nNote\nNot all App Service Plan pricing tiers support autoscaling.\nAutoscale conditions\nYou indicate how to autoscale by creating autoscale conditions. Azure provides two options for autoscaling:\nScale based on a metric, such as the length of the disk queue, or the number of HTTP requests awaiting processing.\nScale to a specific instance count according to a schedule. For example, you can arrange to scale out at a particular time of day, or on a specific date or day of the week. You also specify an end date, and the system scales back in at this time.\nScaling to a specific instance count only enables you to scale out to a defined number of instances. If you need to scale out incrementally, you can combine metric and schedule-based autoscaling in the same autoscale condition. So, you could arrange for the system to scale out if the number of HTTP requests exceeds some threshold, but only between certain hours of the day.\nYou can create multiple autoscale conditions to handle different schedules and metrics. Azure autoscales your service when any of these conditions apply. An App Service Plan also has a default condition that is used if none of the other conditions are applicable. This condition is always active and doesn't have a schedule.\nMetrics for autoscale rules\nAutoscaling by metric requires that you define one or more autoscale rules. An autoscale rule specifies a metric to monitor, and how autoscaling should respond when this metric crosses a defined threshold. The metrics you can monitor for a web app are:\nCPU Percentage\n. This metric is an indication of the CPU utilization across all instances. A high value shows that instances are becoming CPU-bound, which could cause delays in processing client requests.\nMemory Percentage\n. This metric captures the memory occupancy of the application across all instances. A high value indicates that free memory could be running low, and could cause one or more instances to fail.\nDisk Queue Length\n. This metric is a measure of the number of outstanding I/O requests across all instances. A high value means that disk contention could be occurring.\nHttp Queue Length\n. This metric shows how many client requests are waiting for processing by the web app. If this number is large, client requests might fail with HTTP 408 (Timeout) errors.\nData In\n. This metric is the number of bytes received across all instances.\nData Out\n. This metric is the number of bytes sent by all instances.\nYou can also scale based on metrics for other Azure services.\nHow an autoscale rule analyzes metrics\nAutoscaling works by analyzing trends in metric values over time across all instances. Analysis is a multi-step process.\nIn the first step, an autoscale rule aggregates the values retrieved for a metric for all instances across a period of time known as the\ntime grain\n. Each metric has its own intrinsic time grain, but in most cases this period is 1 minute. The aggregated value is known as the\ntime aggregation\n. The options available are\nAverage\n,\nMinimum\n,\nMaximum\n,\nSum\n,\nLast\n, and\nCount\n.\nAn interval of one minute is a short interval in which to determine whether any change in metric is long-lasting enough to make autoscaling worthwhile. So, an autoscale rule performs a second step that performs a further aggregation of the value calculated by the\ntime aggregation\nover a longer, user-specified period, known as the\nDuration\n. The minimum\nDuration\nis 5 minutes. If the\nDuration\nis set to 10 minutes for example, the autoscale rule aggregates the 10 values calculated for the\ntime grain\n.\nThe aggregation calculation for the\nDuration\ncan be different from the\ntime grain\n. For example, if the\ntime aggregation\nis\nAverage\nand the statistic gathered is\nCPU Percentage\nacross a one-minute\ntime grain\n, each minute the average CPU percentage utilization across all instances for that minute is calculated. If the\ntime grain statistic\nis set to\nMaximum\n, and the\nDuration\nof the rule is set to 10 minutes, the maximum of the 10 average values for the CPU percentage utilization is to determine if the rule threshold was crossed.\nAutoscale actions\nWhen an autoscale rule detects that a metric crossed a threshold, it can perform an autoscale action. An autoscale action can be\nscale-out\nor\nscale-in\n. A scale-out action increases the number of instances, and a scale-in action reduces the instance count. An autoscale action uses an operator (such as\nless than\n,\ngreater than\n,\nequal to\n, and so on) to determine how to react to the threshold. Scale-out actions typically use the\ngreater than\noperator to compare the metric value to the threshold. Scale-in actions tend to compare the metric value to the threshold with the\nless than\noperator. An autoscale action can also set the instance count to a specific level, rather than incrementing or decrementing the number available.\nAn autoscale action has a\ncool down\nperiod, specified in minutes. During this interval, the scale rule can't be triggered again. This is to allow the system to stabilize between autoscale events. Remember that it takes time to start up or shut down instances, and so any metrics gathered might not show any significant changes for several minutes. The minimum cool down period is five minutes.\nPairing autoscale rules\nYou should plan for scaling-in when a workload decreases. Consider defining autoscale rules in pairs in the same autoscale condition. One autoscale rule should indicate how to scale the system out when a metric exceeds an upper threshold. Then other rule should define how to scale the system back in again when the same metric drops below a lower threshold.\nCombining autoscale rules\nA single autoscale condition can contain several autoscale rules (for example, a scale-out rule and the corresponding scale-in rule). However, the autoscale rules in an autoscale condition don't have to be directly related. You could define the following four rules in the same autoscale condition:\nIf the HTTP queue length exceeds 10, scale out by 1\nIf the CPU utilization exceeds 70%, scale out by 1\nIf the HTTP queue length is zero, scale in by 1\nIf the CPU utilization drops below 50%, scale in by 1\nWhen determining whether to scale out, the autoscale action is performed if\nany\nof the scale-out rules are met (HTTP queue length exceeds 10\nor\nCPU utilization exceeds 70%). When scaling in, the autoscale action runs\nonly if all\nof the scale-in rules are met (HTTP queue length drops to zero\nand\nCPU utilization falls below 50%). If you need to scale in if only one of the scale-in rules are met, you must define the rules in separate autoscale conditions.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Identify autoscale factors"
                },
                {
                  "level": 2,
                  "text": "Autoscaling and the App Service Plan"
                },
                {
                  "level": 2,
                  "text": "Autoscale conditions"
                },
                {
                  "level": 2,
                  "text": "Metrics for autoscale rules"
                },
                {
                  "level": 2,
                  "text": "How an autoscale rule analyzes metrics"
                },
                {
                  "level": 2,
                  "text": "Autoscale actions"
                },
                {
                  "level": 2,
                  "text": "Pairing autoscale rules"
                },
                {
                  "level": 2,
                  "text": "Combining autoscale rules"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 4,
              "title": "Enable autoscale in App Service",
              "url": "https://learn.microsoft.com/en-us/training/modules/scale-apps-app-service/4-autoscale-app-service",
              "href": "4-autoscale-app-service",
              "content": "Read in English\nAdd\nAdd to plan\nEnable autoscale in App Service\nCompleted\n3 minutes\nIn this unit, you learn how to enable autoscaling, create autoscale rules, and monitor autoscaling activity\nEnable autoscale\nGet started with autoscaling by navigating to your App Service plan in the Azure portal and select\nScale out (App Service plan)\nin the\nSettings\ngroup in the left navigation pane. Select\nRules Based\nin the\nScale out method\nsection of the page, and then select\nConfigure\n.\nNote\nNot all pricing tiers support autoscaling. The development pricing tiers are either limited to a single instance (the\nF1\nand\nD1\ntiers), or they only provide manual scaling (the\nB1\ntier). If you selected one of these tiers, you must first scale up to the\nS1\nor any of the\nP\nlevel production tiers.\nBy default, an App Service Plan only implements manual scaling. Selecting\nCustom autoscale\nreveals condition groups you can use to manage your scale settings.\nAdd scale conditions\nOnce you enable autoscaling, you can edit the automatically created default scale condition, and you can add your own custom scale conditions. Remember that each scale condition can either scale based on a metric, or scale to a specific instance count.\nThe Default scale condition is executed when none of the other scale conditions are active.\nA metric-based scale condition can also specify the minimum and maximum number of instances to create. The maximum number can't exceed the limits defined by the pricing tier. Additionally, all scale conditions other than the default may include a schedule indicating when the condition should be applied.\nCreate scale rules\nA metric-based scale condition contains one or more scale rules. You use the\nAdd a rule\nlink to add your own custom rules. You define:\nThe criteria that indicate when a rule should trigger an autoscale action.\nThe autoscale action to be performed.\nMonitor autoscaling activity\nThe Azure portal enables you to track when autoscaling occurred through the\nRun history\nchart. This chart shows how the number of instances varies over time, and which autoscale conditions caused each change.\nYou can use the\nRun history\nchart with the metrics shown on the\nOverview\npage to correlate the autoscaling events with resource utilization.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Enable autoscale in App Service"
                },
                {
                  "level": 2,
                  "text": "Enable autoscale"
                },
                {
                  "level": 2,
                  "text": "Add scale conditions"
                },
                {
                  "level": 2,
                  "text": "Create scale rules"
                },
                {
                  "level": 2,
                  "text": "Monitor autoscaling activity"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [
                {
                  "src": "../../wwl-azure/scale-apps-app-service/media/enable-autoscale.png",
                  "absolute_url": "https://learn.microsoft.com/en-us/wwl-azure/scale-apps-app-service/media/enable-autoscale.png",
                  "alt_text": "Screenshot showing the Custom autoscale selection.",
                  "title": "",
                  "filename": "enable-autoscale.png",
                  "image_type": "screenshot",
                  "context": {
                    "preceding_heading": "Enable autoscale",
                    "following_text": "Once you enable autoscaling, you can edit the automatically created default scale condition, and you can add your own custom scale conditions. Remember that each scale condition can either scale based",
                    "figure_caption": "",
                    "parent_section": ""
                  }
                },
                {
                  "src": "../../wwl-azure/scale-apps-app-service/media/autoscale-conditions.png",
                  "absolute_url": "https://learn.microsoft.com/en-us/wwl-azure/scale-apps-app-service/media/autoscale-conditions.png",
                  "alt_text": "Screenshot of the condition page for an App Service Plan showing the default scale condition.",
                  "title": "",
                  "filename": "autoscale-conditions.png",
                  "image_type": "screenshot",
                  "context": {
                    "preceding_heading": "Add scale conditions",
                    "following_text": "A metric-based scale condition can also specify the minimum and maximum number of instances to create. The maximum number can't exceed the limits defined by the pricing tier. Additionally, all scale c",
                    "figure_caption": "",
                    "parent_section": ""
                  }
                },
                {
                  "src": "../../wwl-azure/scale-apps-app-service/media/autoscale-rules.png",
                  "absolute_url": "https://learn.microsoft.com/en-us/wwl-azure/scale-apps-app-service/media/autoscale-rules.png",
                  "alt_text": "Screenshot of the scale rule settings pane.",
                  "title": "",
                  "filename": "autoscale-rules.png",
                  "image_type": "screenshot",
                  "context": {
                    "preceding_heading": "Create scale rules",
                    "following_text": "The Azure portal enables you to track when autoscaling occurred through theRun historychart. This chart shows how the number of instances varies over time, and which autoscale conditions caused each c",
                    "figure_caption": "",
                    "parent_section": ""
                  }
                },
                {
                  "src": "../../wwl-azure/scale-apps-app-service/media/autoscale-run-history.png",
                  "absolute_url": "https://learn.microsoft.com/en-us/wwl-azure/scale-apps-app-service/media/autoscale-run-history.png",
                  "alt_text": "Screenshot of the Run history information for the app.",
                  "title": "",
                  "filename": "autoscale-run-history.png",
                  "image_type": "screenshot",
                  "context": {
                    "preceding_heading": "Monitor autoscaling activity",
                    "following_text": "You can use theRun historychart with the metrics shown on theOverviewpage to correlate the autoscaling events with resource utilization.",
                    "figure_caption": "",
                    "parent_section": ""
                  }
                },
                {
                  "src": "../../wwl-azure/scale-apps-app-service/media/service-plan-metrics.png",
                  "absolute_url": "https://learn.microsoft.com/en-us/wwl-azure/scale-apps-app-service/media/service-plan-metrics.png",
                  "alt_text": "Screenshot of the metrics shown on the App Service Plan overview page.",
                  "title": "",
                  "filename": "service-plan-metrics.png",
                  "image_type": "screenshot",
                  "context": {
                    "preceding_heading": "Monitor autoscaling activity",
                    "following_text": "Was this page helpful?",
                    "figure_caption": "",
                    "parent_section": ""
                  }
                }
              ],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 5,
              "title": "Explore autoscale best practices",
              "url": "https://learn.microsoft.com/en-us/training/modules/scale-apps-app-service/5-autoscale-best-practices",
              "href": "5-autoscale-best-practices",
              "content": "Read in English\nAdd\nAdd to plan\nExplore autoscale best practices\nCompleted\n3 minutes\nIf you're not following good practices when creating autoscale settings, you can create conditions that lead to undesirable results. In this unit, you learn how to avoid creating rules that conflict with each other.\nAutoscale concepts\nAn autoscale setting scales instances horizontally, which is\nout\nby increasing the instances and\nin\nby decreasing the number of instances. An autoscale setting has a maximum, minimum, and default value of instances.\nAn autoscale job always reads the associated metric to scale by, checking if it crossed the configured threshold for scale-out or scale-in.\nAll thresholds are calculated at an instance level. For example, \"scale out by one instance when average CPU > 80% when instance count is 2\", means to scale out when the average CPU across all instances is greater than 80%.\nAll autoscale successes and failures are logged to the Activity Log. You can then configure an activity log alert so that you can be notified via email, short message service, or webhooks whenever there's activity.\nAutoscale best practices\nUse the following best practices as you create your autoscale rules.\nEnsure the maximum and minimum values are different and have an adequate margin between them\nIf you have a setting that has minimum=two, maximum=two and the current instance count is two, no scale action can occur. Keep an adequate margin between the maximum and minimum instance counts, which are inclusive. Autoscale always scales between these limits.\nChoose the appropriate statistic for your diagnostics metric\nFor diagnostics metrics, you can choose among\nAverage\n,\nMinimum\n,\nMaximum\nand\nTotal\nas a metric to scale by. The most common statistic is Average.\nChoose the thresholds carefully for all metric types\nWe recommend carefully choosing different thresholds for scale-out and scale-in based on practical situations.\nWe\ndon't recommend\nautoscale settings like the following examples with the same or similar threshold values for out and in conditions:\nIncrease instances by one count when Thread Count >= 600\nDecrease instances by one count when Thread Count <= 600\nLet's look at an example of what can lead to a behavior that might seem confusing. Consider the following sequence.\nAssume there are two instances to begin with and then the average number of threads per instance grows to 625.\nAutoscale scales out adding a third instance.\nNext, assume that the average thread count across instance falls to 575.\nBefore scaling in, autoscale tries to estimate the final state if it scaled in. For example, 575 x 3 (current instance count) = 1,725 / 2 (final number of instances when scaled in) = 862.5 threads. This means autoscale would have to immediately scale out again even after it scaled in, if the average thread count remains the same or even falls only a small amount. However, if it scaled out again, the whole process would repeat, leading to an infinite loop.\nTo avoid this situation (termed \"flapping\"), autoscale doesn't scale in at all. Instead, it skips and reevaluates the condition again the next time the service's job executes.\nEstimation during a scale-in is intended to avoid \"flapping\" situations, where scale-in and scale out actions continually go back and forth. Keep this behavior in mind when you choose the same thresholds for scale-out and in.\nWe recommend choosing an adequate margin between the scale-out and in thresholds. As an example, consider the following better rule combination.\nIncrease instances by one count when CPU% >= 80\nDecrease instances by one count when CPU% <= 60\nIn this case\nAssume you're starting with two instances.\nIf the average CPU% across instances goes to 80, autoscale scales out adding a third instance.\nNow assume that over time the CPU% falls to 60.\nAutoscale's scale-in rule estimates the final state if it were to scale-in. For example, 60 x 3 (current instance count) = 180 / 2 (final number of instances when scaled in) = 90. So autoscale doesn't scale-in because it would have to scale out again immediately. Instead, it skips scaling in.\nThe next time autoscale checks, the CPU continues to fall to 50. It estimates again - 50 x 3 instance = 150 / 2 instances = 75, which is below the scale-out threshold of 80, so it scales in successfully to 2 instances.\nConsiderations for scaling when multiple rules are configured in a profile\nThere are cases where you might have to set multiple rules in a profile. The following set of autoscale rules are used by services when multiple rules are set.\nOn\nscale-out\n, autoscale runs if any rule is met. On\nscale-in\n, autoscale requires all rules to be met.\nTo illustrate, assume that you have the following four autoscale rules:\nIf CPU < 30%, scale-in by 1\nIf Memory < 50%, scale-in by 1\nIf CPU > 75%, scale out by 1\nIf Memory > 75%, scale out by 1\nThen the following occurs:\nIf CPU is 76% and Memory is 50%, we scale out.\nIf CPU is 50% and Memory is 76% we scale out.\nOn the other hand, if CPU is 25% and memory is 51% autoscale doesn't scale-in. An automatic scale-in would occur if the CPU is 29% and the Memory is 49% since both of the scale-in rules would be true.\nAlways select a safe default instance count\nThe default instance count is important because autoscale scales your service to that count when metrics aren't available. Therefore, select a default instance count that's safe for your workloads.\nConfigure autoscale notifications\nAutoscale posts to the Activity Log if any of the following conditions occur:\nAutoscale issues a scale operation\nAutoscale service successfully completes a scale action\nAutoscale service fails to take a scale action.\nMetrics aren't available for autoscale service to make a scale decision.\nMetrics are available (recovery) again to make a scale decision.\nYou can also use an Activity Log alert to monitor the health of the autoscale engine. In addition to using activity log alerts, you can also configure email or webhook notifications to get notified for successful scale actions via the notifications tab on the autoscale setting.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Explore autoscale best practices"
                },
                {
                  "level": 2,
                  "text": "Autoscale concepts"
                },
                {
                  "level": 2,
                  "text": "Autoscale best practices"
                },
                {
                  "level": 3,
                  "text": "Ensure the maximum and minimum values are different and have an adequate margin between them"
                },
                {
                  "level": 3,
                  "text": "Choose the appropriate statistic for your diagnostics metric"
                },
                {
                  "level": 3,
                  "text": "Choose the thresholds carefully for all metric types"
                },
                {
                  "level": 3,
                  "text": "Considerations for scaling when multiple rules are configured in a profile"
                },
                {
                  "level": 3,
                  "text": "Always select a safe default instance count"
                },
                {
                  "level": 3,
                  "text": "Configure autoscale notifications"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 6,
              "title": "Module assessment",
              "url": "https://learn.microsoft.com/en-us/training/modules/scale-apps-app-service/6-knowledge-check",
              "href": "6-knowledge-check",
              "content": "Read in English\nAdd\nAdd to plan\nModule assessment\nCompleted\n3 minutes\nCheck your knowledge\n1.\nWhich of these statements best describes autoscaling?\nAutoscaling requires an administrator to actively monitor the workload on a system.\nAutoscaling is a scale out/scale in solution.\nScaling up/scale down provides better availability than autoscaling.\n2.\nWhich of these scenarios is a suitable candidate for autoscaling?\nThe number of users requiring access to an application varies according to a regular schedule.\nThe system is subject to a sudden influx of requests that grinds your system to a halt.\nYour organization is running a promotion and expects to see increased traffic to their web site for the next couple of weeks.\nYou must answer all questions before checking your work.\nYou must answer all questions before checking your work.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Module assessment"
                },
                {
                  "level": 2,
                  "text": "Check your knowledge"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 7,
              "title": "Summary",
              "url": "https://learn.microsoft.com/en-us/training/modules/scale-apps-app-service/7-summary",
              "href": "7-summary",
              "content": "Read in English\nAdd\nAdd to plan\nSummary\nCompleted\n3 minutes\nIn this module, you learned how to:\nIdentify scenarios for which autoscaling is an appropriate solution\nCreate autoscaling rules for a web app\nMonitor the effects of autoscaling\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Summary"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            }
          ]
        },
        {
          "title": "Explore Azure App Service deployment slots",
          "url": "https://learn.microsoft.com/en-us/training/modules/understand-app-service-deployment-slots/",
          "description": "",
          "learning_objectives": [],
          "prerequisites": [],
          "units": [
            {
              "number": 1,
              "title": "Introduction",
              "url": "https://learn.microsoft.com/en-us/training/modules/understand-app-service-deployment-slots/1-introduction",
              "href": "1-introduction",
              "content": "Read in English\nAdd\nAdd to plan\nIntroduction\nCompleted\n3 minutes\nThe deployment slot functionality in App Service is a powerful tool that enables you to preview, manage, test, and deploy your different development environments.\nLearning objectives\nAfter completing this module, you'll be able to:\nDescribe the benefits of using deployment slots.\nUnderstand how slot swapping operates in App Service.\nPerform manual swaps and enable auto swap.\nRoute traffic manually and automatically.\nPrerequisites\nExperience using the Azure portal to create and manage App Service web apps\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Introduction"
                },
                {
                  "level": 2,
                  "text": "Learning objectives"
                },
                {
                  "level": 2,
                  "text": "Prerequisites"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 2,
              "title": "Explore staging environments",
              "url": "https://learn.microsoft.com/en-us/training/modules/understand-app-service-deployment-slots/2-app-service-staging-environments",
              "href": "2-app-service-staging-environments",
              "content": "Read in English\nAdd\nAdd to plan\nExplore staging environments\nCompleted\n3 minutes\nWhen you deploy your web app, web app on Linux, mobile back end, or API app to Azure App Service, you can use a separate deployment slot instead of the default production slot. This approach is available if you run in the\nStandard\n,\nPremium\n, or\nIsolated\nApp Service plan tier. Deployment slots are live apps with their own host names. App content and configuration elements can be swapped between two deployment slots, including the production slot.\nDeploying your application to a nonproduction slot has the following benefits:\nValidate app changes in a staging deployment slot before swapping it with the production slot.\nDeploying an app to a slot first and then swapping it into production ensures that all instances of the slot are warmed up before being swapped into production. This eliminates downtime when you deploy your app. The traffic redirection is seamless, and no requests are dropped because of swap operations. You can automate this entire workflow by configuring auto swap when preswap validation isn't needed.\nAfter a swap, the previous production app is located in the staging slot. If the changes swapped into the production slot aren't as you expect, you can perform the same swap immediately to get your \"last known good site\" back.\nEach App Service plan tier supports a different number of deployment slots. There's no extra charge for using deployment slots. To find out the number of slots your app's tier supports, visit\nApp Service limits\n.\nTo scale your app to a different tier, make sure that the target tier supports the number of slots your app already uses. For example, if your app has more than five slots, you can't scale it down to the\nStandard\ntier, because the\nStandard\ntier supports only five deployment slots.\nWhen you create a new deployment slot the new slot has no content, even if you clone the settings from a different slot. You can deploy to the slot from a different repository branch or a different repository.\nThe slot's URL has the format\nhttp://sitename-slotname.azurewebsites.net\n. To keep the URL length within necessary domain name system limits, the site name is truncated at 40 characters. The combined site name and slot name must be fewer than 59 characters.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Explore staging environments"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [
                "http://sitename-slotname.azurewebsites.net"
              ],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                },
                {
                  "url": "/en-us/azure/azure-resource-manager/management/azure-subscription-service-limits#app-service-limits",
                  "text": "App Service limits"
                }
              ]
            },
            {
              "number": 3,
              "title": "Examine slot swapping",
              "url": "https://learn.microsoft.com/en-us/training/modules/understand-app-service-deployment-slots/3-app-service-slot-swapping",
              "href": "3-app-service-slot-swapping",
              "content": "Read in English\nAdd\nAdd to plan\nExamine slot swapping\nCompleted\n5 minutes\nWhen you swap two slots, App Service completes the following process to ensure that the target slot doesn't experience downtime:\nApply the following settings from the target slot (for example, the production slot) to all instances of the source slot:\nSlot-specific app settings and connection strings, if applicable.\nContinuous deployment settings, if enabled.\nApp Service authentication settings, if enabled.\nWhen any of the settings is applied to the source slot, the change triggers all instances in the source slot to restart. During\nswap with preview\n, this marks the end of the first phase. The swap operation is paused, and you can validate that the source slot works correctly with the target slot's settings.\nWait for every instance in the source slot to complete its restart. If any instance fails to restart, the swap operation reverts all changes to the source slot and stops the operation.\nIf local cache is enabled, trigger local cache initialization by making an HTTP request to the application root (\"/\") on each instance of the source slot. Wait until each instance returns any HTTP response. Local cache initialization causes another restart on each instance.\nIf auto swap is enabled with custom warm-up, trigger Application Initiation by making an HTTP request to the application root (\"/\") on each instance of the source slot.\nIf\napplicationInitialization\nisn't specified, trigger an HTTP request to the application root of the source slot on each instance.\nAn instance is considered warmed up if it returns any HTTP response.\nIf all instances on the source slot are warmed up successfully, swap the two slots by switching the routing rules for the two slots. After this step, the target slot (for example, the production slot) has the app previously warmed up in the source slot.\nNow that the source slot has the pre-swap app previously in the target slot, perform the same operation by applying all settings and restarting the instances.\nAt any point of the swap operation, all work of initializing the swapped apps happens on the source slot. The target slot remains online while the source slot is being prepared and warmed up, regardless of where the swap succeeds or fails. To swap a staging slot with the production slot, make sure that the production slot is always the target slot. This way, the swap operation doesn't affect your production app.\nWhen you clone configuration from another deployment slot, the cloned configuration is editable. Some configuration elements follow the content across a swap (not slot specific), whereas other configuration elements stay in the same slot after a swap (slot specific). The following table shows the settings that change when you swap slots.\nSettings that are swapped\nSettings that aren't swapped\nGeneral settings, such as framework version, 32/64-bit, web sockets\nPublishing endpoints\nApp settings (can be configured to stick to a slot)\nCustom domain names\nConnection strings (can be configured to stick to a slot)\nNonpublic certificates and TLS/SSL settings\nHandler mappings\nScale settings\nPublic certificates\nWebJobs schedulers\nWebJobs content\nIP restrictions\nHybrid connections\nAlways On\nAzure Content Delivery Network\nDiagnostic log settings\nService endpoints\nCross-origin resource sharing (CORS)\nPath mappings\nVirtual network integration\nManaged identities\nSettings that end with the suffix\n_EXTENSION_VERSION\nNote\nTo make settings swappable, add the app setting\nWEBSITE_OVERRIDE_PRESERVE_DEFAULT_STICKY_SLOT_SETTINGS\nin every slot of the app and set its value to\n0\nor\nfalse\n. These settings are either all swappable or not at all. You can't make just some settings swappable and not the others. Managed identities are never swapped and aren't affected by this override app setting.\nTo configure an app setting or connection string to stick to a specific slot (not swapped), go to the Configuration page for that slot. Add or edit a setting, and then select\nDeployment slot setting\n. Selecting this check box tells App Service that the setting isn't swappable.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Examine slot swapping"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [
                "applicationInitialization",
                "_EXTENSION_VERSION",
                "WEBSITE_OVERRIDE_PRESERVE_DEFAULT_STICKY_SLOT_SETTINGS"
              ],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 4,
              "title": "Swap deployment slots",
              "url": "https://learn.microsoft.com/en-us/training/modules/understand-app-service-deployment-slots/4-swap-deployment-slots",
              "href": "4-swap-deployment-slots",
              "content": "Read in English\nAdd\nAdd to plan\nSwap deployment slots\nCompleted\n6 minutes\nYou can swap deployment slots on your app's Deployment slots page and the Overview page. Before you swap an app from a deployment slot into production, make sure that production is your target slot and that all settings in the source slot are configured exactly as you want to have them in production.\nManually swapping deployment slots\nTo swap deployment slots:\nGo to your app's\nDeployment slots\npage and select\nSwap\n. The\nSwap\ndialog box shows settings in the selected source and target slots that are changed.\nSelect the desired\nSource\nand\nTarget\nslots. Usually, the target is the production slot. Also, select the\nSource Changes\nand\nTarget Changes\ntabs and verify that the configuration changes are expected. When you're finished, you can swap the slots immediately by selecting\nSwap\n.\nTo see how your target slot would run with the new settings before the swap actually happens, don't select Swap, but follow the instructions in\nSwap with preview\n.\nWhen you're finished, close the dialog box by selecting\nClose\n.\nSwap with preview (multi-phase swap)\nBefore you swap into production as the target slot, validate that the app runs with the swapped settings. The source slot is also warmed up before the swap completion, which is desirable for mission-critical applications.\nWhen you perform a swap with preview, App Service performs the same swap operation but pauses after the first step. You can then verify the result on the staging slot before completing the swap.\nIf you cancel the swap, App Service reapplies configuration elements to the source slot.\nTo swap with preview:\nFollow the steps in Swap deployment slots section, but select the\nPerform swap with preview\ncheckbox. The dialog box shows you how the configuration in the source slot changes in phase 1, and how the source and target slot change in phase 2.\nWhen you're ready to start the swap, select\nStart Swap\n.\nWhen phase 1 finishes, you're notified in the dialog box. Preview the swap in the source slot by going to\nhttps://<app_name>-<source-slot-name>.azurewebsites.net\n.\nWhen you're ready to complete the pending swap, select\nComplete Swap\nin\nSwap action\nand select\nComplete Swap\n.\nTo cancel a pending swap, select\nCancel Swap\ninstead.\nWhen you're finished, close the dialog box by selecting\nClose\n.\nConfigure auto swap\nAuto swap streamlines Azure DevOps Services scenarios where you want to deploy your app continuously with zero cold starts and zero downtime for customers of the app. When auto swap is enabled from a slot into production, every time you push your code changes to that slot, App Service automatically swaps the app into production after the source slot is warmed up.\nNote\nAuto swap isn't currently supported in web apps on Linux and Web App for Containers.\nTo configure auto swap:\nGo to your app's resource page and select the deployment slot you want to configure to auto swap. The setting is on the\nConfiguration > General settings\npage.\nSet\nAuto swap enabled\nto\nOn\n. Then select the desired target slot for\nAuto swap deployment slot\n, and select\nSave\non the command bar.\nExecute a code push to the source slot. Auto swap happens after a short time, and the update is reflected at your target slot's URL.\nSpecify custom warm-up\nSome apps might require custom warm-up actions before the swap. The\napplicationInitialization\nconfiguration element in web.config lets you specify custom initialization actions. The swap operation waits for this custom warm-up to finish before swapping with the target slot. Here's a sample web.config fragment.\n<system.webServer>\n    <applicationInitialization>\n        <add initializationPage=\"/\" hostName=\"[app hostname]\" />\n        <add initializationPage=\"/Home/About\" hostName=\"[app hostname]\" />\n    </applicationInitialization>\n</system.webServer>\nFor more information on customizing the\napplicationInitialization\nelement, see\nMost common deployment slot swap failures and how to fix them\n.\nYou can also customize the warm-up behavior with one or both of the following app settings:\nWEBSITE_SWAP_WARMUP_PING_PATH\n: The path to ping to warm up your site. Add this app setting by specifying a custom path that begins with a slash as the value. An example is\n/statuscheck\n. The default value is\n/\n.\nWEBSITE_SWAP_WARMUP_PING_STATUSES\n: Valid HTTP response codes for the warm-up operation. Add this app setting with a comma-separated list of HTTP codes. An example is\n200,202\n. If the returned status code isn't in the list, the warmup and swap operations are stopped. By default, all response codes are valid.\nWEBSITE_WARMUP_PATH\n: A relative path on the site that should be pinged whenever the site restarts (not only during slot swaps). Example values include\n/statuscheck\nor the root path,\n/\n.\nRoll back and monitor a swap\nIf any errors occur in the target slot (for example, the production slot) after a slot swap, restore the slots to their pre-swap states by swapping the same two slots immediately.\nIf the swap operation takes a long time to complete, you can get information on the swap operation in the activity log.\nOn your app's resource page in the portal, in the left pane, select\nActivity log\n.\nA swap operation appears in the log query as\nSwap Web App Slots\n. You can expand it and select one of the suboperations or errors to see the details.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Swap deployment slots"
                },
                {
                  "level": 2,
                  "text": "Manually swapping deployment slots"
                },
                {
                  "level": 3,
                  "text": "Swap with preview (multi-phase swap)"
                },
                {
                  "level": 2,
                  "text": "Configure auto swap"
                },
                {
                  "level": 2,
                  "text": "Specify custom warm-up"
                },
                {
                  "level": 2,
                  "text": "Roll back and monitor a swap"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [
                "https://<app_name>-<source-slot-name>.azurewebsites.net",
                "applicationInitialization",
                "<system.webServer>\n    <applicationInitialization>\n        <add initializationPage=\"/\" hostName=\"[app hostname]\" />\n        <add initializationPage=\"/Home/About\" hostName=\"[app hostname]\" />\n    </applicationInitialization>\n</system.webServer>",
                "<system.webServer>\n    <applicationInitialization>\n        <add initializationPage=\"/\" hostName=\"[app hostname]\" />\n        <add initializationPage=\"/Home/About\" hostName=\"[app hostname]\" />\n    </applicationInitialization>\n</system.webServer>",
                "applicationInitialization",
                "WEBSITE_SWAP_WARMUP_PING_PATH",
                "/statuscheck",
                "WEBSITE_SWAP_WARMUP_PING_STATUSES",
                "WEBSITE_WARMUP_PATH",
                "/statuscheck",
                "Swap Web App Slots"
              ],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                },
                {
                  "url": "https://ruslany.net/2017/11/most-common-deployment-slot-swap-failures-and-how-to-fix-them/",
                  "text": "Most common deployment slot swap failures and how to fix them"
                }
              ]
            },
            {
              "number": 5,
              "title": "Route traffic in App Service",
              "url": "https://learn.microsoft.com/en-us/training/modules/understand-app-service-deployment-slots/5-route-traffic-app-service",
              "href": "5-route-traffic-app-service",
              "content": "Read in English\nAdd\nAdd to plan\nRoute traffic in App Service\nCompleted\n4 minutes\nBy default, all client requests to the app's production URL (\nhttp://<app_name>.azurewebsites.net\n) are routed to the production slot. You can route a portion of the traffic to another slot. This feature is useful if you need user feedback for a new update, but you're not ready to release it to production.\nRoute production traffic automatically\nTo route production traffic automatically:\nGo to your app's resource page and select\nDeployment slots\n.\nIn the\nTraffic %\ncolumn of the slot you want to route to, specify a percentage (between 0 and 100) to represent the amount of total traffic you want to route. Select\nSave\n.\nAfter the setting is saved, the specified percentage of clients is randomly routed to the nonproduction slot.\nAfter a client is automatically routed to a specific slot, it's \"pinned\" to that slot for the life of that client session. On the client browser, you can see which slot your session is pinned to by looking at the\nx-ms-routing-name\ncookie in your HTTP headers. A request routed to the \"staging\" slot has the cookie\nx-ms-routing-name=staging\n. A request routed to the production slot has the cookie\nx-ms-routing-name=self\n.\nRoute production traffic manually\nIn addition to automatic traffic routing, App Service can route requests to a specific slot. This is useful when you want your users to be able to opt in to or opt out of your beta app. To route production traffic manually, you use the\nx-ms-routing-name\nquery parameter.\nTo let users opt out of your beta app, for example, you can put this link on your webpage:\n<a href=\"<webappname>.azurewebsites.net/?x-ms-routing-name=self\">Go back to production app</a>\nThe string\nx-ms-routing-name=self\nspecifies the production slot. The client browser is redirected production slot after accessing the link. Every subsequent request has the\nx-ms-routing-name=self\ncookie that pins the session to the production slot.\nEnable users opt in to your beta app, set the same query parameter to the name of the nonproduction slot. Here's an example:\n<webappname>.azurewebsites.net/?x-ms-routing-name=staging\nNew slots are given a routing rule of\n0%\nby default. Default values are displayed in grey. When you explicitly set the routing rule value to\n0%\nthe value is displayed in black, your users can access the staging slot manually by using the\nx-ms-routing-name\nquery parameter. They aren't routed to the slot automatically because the routing percentage is set to 0. This is an advanced scenario where you can \"hide\" your staging slot from the public while allowing internal teams to test changes on the slot.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Route traffic in App Service"
                },
                {
                  "level": 2,
                  "text": "Route production traffic automatically"
                },
                {
                  "level": 2,
                  "text": "Route production traffic manually"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [
                "http://<app_name>.azurewebsites.net",
                "x-ms-routing-name",
                "x-ms-routing-name=staging",
                "x-ms-routing-name=self",
                "x-ms-routing-name",
                "<a href=\"<webappname>.azurewebsites.net/?x-ms-routing-name=self\">Go back to production app</a>",
                "<a href=\"<webappname>.azurewebsites.net/?x-ms-routing-name=self\">Go back to production app</a>",
                "x-ms-routing-name=self",
                "x-ms-routing-name=self",
                "<webappname>.azurewebsites.net/?x-ms-routing-name=staging",
                "<webappname>.azurewebsites.net/?x-ms-routing-name=staging",
                "x-ms-routing-name"
              ],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 6,
              "title": "Module assessment",
              "url": "https://learn.microsoft.com/en-us/training/modules/understand-app-service-deployment-slots/6-knowledge-check",
              "href": "6-knowledge-check",
              "content": "Read in English\nAdd\nAdd to plan\nModule assessment\nCompleted\n3 minutes\nCheck your knowledge\n1.\nBy default, all client requests to the app's production URL (\nhttp://<app_name>.azurewebsites.net\n) are routed to the production slot. One can automatically route a portion of the traffic to another slot. What is the default routing rule applied to new deployment slots?\n0%\n10%\n20%\n2.\nSome configuration elements follow the content across a swap (not slot specific), whereas other configuration elements stay in the same slot after a swap (slot specific). Which of the following settings are swapped?\nPublishing endpoints\nWebJobs content\nWebJobs schedulers\nYou must answer all questions before checking your work.\nYou must answer all questions before checking your work.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Module assessment"
                },
                {
                  "level": 2,
                  "text": "Check your knowledge"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [
                "http://<app_name>.azurewebsites.net"
              ],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 7,
              "title": "Summary",
              "url": "https://learn.microsoft.com/en-us/training/modules/understand-app-service-deployment-slots/7-summary",
              "href": "7-summary",
              "content": "Read in English\nAdd\nAdd to plan\nSummary\nCompleted\n3 minutes\nIn this module, you learned how to:\nDescribe the benefits of using deployment slots\nUnderstand how slot swapping operates in App Service\nPerform manual swaps and enable auto swap\nRoute traffic manually and automatically\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Summary"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 900,
              "title": "Exercise - Swap deployment slots in Azure App Service",
              "url": "https://learn.microsoft.com/en-us/training/modules/understand-app-service-deployment-slots/4a-exercise-swap-deployment-slots",
              "href": "4a-exercise-swap-deployment-slots",
              "content": "Read in English\nAdd\nAdd to plan\nExercise - Swap deployment slots in Azure App Service\nCompleted\n30 minutes\nIn this exercise, you deploy a basic HTML+CSS site to Azure App Service with the Azure CLI\naz  webapp up\ncommand. Next, you update the code and deploy the change to a staging slot. Finally, you swap the slots.\nTasks performed in this exercise:\nDownload and deploy the sample app to Azure App Service.\nCreate a staging deployment slot.\nMake a change to the sample app and deploy it to the staging slot.\nSwap the staging and default production slots to move the changes to the production slot.\nThis exercise takes approximately\n30\nminutes to complete.\nBefore you start\nTo complete the exercise, you need:\nAn Azure subscription. If you don't already have one, you can sign up for one\nhttps://azure.microsoft.com/\n.\nGetting started\nSelect the\nLaunch Exercise\nbutton, it opens the exercise instructions in a new browser window. When you're finished with the exercise, return here to:\nComplete the module\nEarn a badge for completing this module\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Exercise - Swap deployment slots in Azure App Service"
                },
                {
                  "level": 2,
                  "text": "Before you start"
                },
                {
                  "level": 2,
                  "text": "Getting started"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [
                {
                  "src": "../../wwl-azure/understand-app-service-deployment-slots/media/launch-exercise.png",
                  "absolute_url": "https://learn.microsoft.com/en-us/wwl-azure/understand-app-service-deployment-slots/media/launch-exercise.png",
                  "alt_text": "Button to launch exercise.",
                  "title": "",
                  "filename": "launch-exercise.png",
                  "image_type": "icon",
                  "context": {
                    "preceding_heading": "Getting started",
                    "following_text": "Was this page helpful?",
                    "figure_caption": "",
                    "parent_section": ""
                  }
                }
              ],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                },
                {
                  "url": "https://azure.microsoft.com/",
                  "text": "https://azure.microsoft.com/"
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "title": "Implement Azure Functions",
      "url": "https://learn.microsoft.com/en-us/training/paths/implement-azure-functions/",
      "learn_uid": "learn.wwl.implement-azure-functions",
      "modules": [
        {
          "title": "Explore Azure Functions",
          "url": "https://learn.microsoft.com/en-us/training/modules/explore-azure-functions/",
          "description": "",
          "learning_objectives": [],
          "prerequisites": [],
          "units": [
            {
              "number": 1,
              "title": "Introduction",
              "url": "https://learn.microsoft.com/en-us/training/modules/explore-azure-functions/1-introduction",
              "href": "1-introduction",
              "content": "Read in English\nAdd\nAdd to plan\nIntroduction\nCompleted\n3 minutes\nAzure Functions lets you develop serverless applications on Microsoft Azure. You can write just the code you need for the problem at hand, without worrying about a whole application or the infrastructure to run it.\nAfter completing this module, you'll be able to:\nExplain functional differences between Azure Functions, Azure Logic Apps, and WebJobs\nDescribe Azure Functions hosting plan options\nDescribe how Azure Functions scale to meet business needs\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Introduction"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 2,
              "title": "Discover Azure Functions",
              "url": "https://learn.microsoft.com/en-us/training/modules/explore-azure-functions/2-azure-functions-overview",
              "href": "2-azure-functions-overview",
              "content": "Read in English\nAdd\nAdd to plan\nDiscover Azure Functions\nCompleted\n3 minutes\nAzure Functions is a serverless solution that allows you to write less code, maintain less infrastructure, and save on costs. Instead of worrying about deploying and maintaining servers, the cloud infrastructure provides all the up-to-date resources needed to keep your applications running.\nWe often build systems to react to a series of critical events. Whether you're building a web API, responding to database changes, processing IoT data streams, or even managing message queues - every application needs a way to run some code as these events occur.\nAzure Functions supports\ntriggers\n, which are ways to start execution of your code, and\nbindings\n, which are ways to simplify coding for input and output data. There are other integration and automation services in Azure and they all can solve integration problems and automate business processes. They can all define input, actions, conditions, and output.\nCompare Azure Functions and Azure Logic Apps\nBoth Functions and Logic Apps are Azure Services that enable serverless workloads. Azure Functions is a serverless compute service, whereas Azure Logic Apps is a serverless workflow integration platform. Both can create complex\norchestrations\n. An orchestration is a collection of functions or steps, called actions in Logic Apps, that are executed to accomplish a complex task.\nFor Azure Functions, you develop orchestrations by writing code and using the\nDurable Functions extension\n. For Logic Apps, you create orchestrations by using a GUI or editing configuration files.\nThe following table lists some of the key differences between Functions and Logic Apps:\nTopic\nAzure Functions\nLogic Apps\nDevelopment\nCode-first (imperative)\nDesigner-first (declarative)\nConnectivity\nAbout a dozen built-in binding types, write code for custom bindings\nLarge collection of connectors, Enterprise Integration Pack for B2B scenarios, build custom connectors\nActions\nEach activity is an Azure function; write code for activity functions\nLarge collection of ready-made actions\nMonitoring\nAzure Application Insights\nAzure portal, Azure Monitor logs\nManagement\nREST API, Visual Studio\nAzure portal, REST API, PowerShell, Visual Studio\nExecution context\nRuns in Azure, or locally\nRuns in Azure, locally, or on premises\nCompare Functions and WebJobs\nLike Azure Functions, Azure App Service WebJobs with the WebJobs SDK is a code-first integration service that is designed for developers. Both are built on Azure App Service and support features such as source control integration, authentication, and monitoring with Application Insights integration.\nAzure Functions is built on the WebJobs SDK, so it shares many of the same event triggers and connections to other Azure services. Here are some factors to consider when you're choosing between Azure Functions and WebJobs with the WebJobs SDK:\nFactor\nFunctions\nWebJobs with WebJobs SDK\nServerless app model with automatic scaling\nYes\nNo\nDevelop and test in browser\nYes\nNo\nPay-per-use pricing\nYes\nNo\nIntegration with Logic Apps\nYes\nNo\nTrigger events\nTimer\nAzure Storage queues and blobs\nAzure Service Bus queues and topics\nAzure Cosmos DB\nAzure Event Hubs\nHTTP/WebHook (GitHub\nSlack)\nAzure Event Grid\nTimer\nAzure Storage queues and blobs\nAzure Service Bus queues and topics\nAzure Cosmos DB\nAzure Event Hubs\nFile system\nAzure Functions offers more developer productivity than Azure App Service WebJobs does. It also offers more options for programming languages, development environments, Azure service integration, and pricing. For most scenarios, it's the best choice.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Discover Azure Functions"
                },
                {
                  "level": 2,
                  "text": "Compare Azure Functions and Azure Logic Apps"
                },
                {
                  "level": 2,
                  "text": "Compare Functions and WebJobs"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                },
                {
                  "url": "/en-us/azure/azure-functions/durable/durable-functions-overview",
                  "text": "Durable Functions extension"
                }
              ]
            },
            {
              "number": 3,
              "title": "Compare Azure Functions hosting options",
              "url": "https://learn.microsoft.com/en-us/training/modules/explore-azure-functions/3-compare-azure-functions-hosting-options",
              "href": "3-compare-azure-functions-hosting-options",
              "content": "Read in English\nAdd\nAdd to plan\nCompare Azure Functions hosting options\nCompleted\n5 minutes\nWhen you create a function app in Azure, you must choose a hosting plan for your app. Azure provides you with these hosting options for your function code:\nHosting option\nService\nAvailability\nContainer support\nConsumption plan\nAzure Functions\nGenerally available (GA)\nNone\nFlex Consumption plan\nAzure Functions\nGA\nNone\nPremium plan\nAzure Functions\nGA\nLinux\nDedicated plan\nAzure Functions\nGA\nLinux\nContainer Apps\nAzure Container Apps\nGA\nLinux\nAzure App Service infrastructure facilitates Azure Functions hosting on both Linux and Windows virtual machines. The hosting option you choose dictates the following behaviors:\nHow your function app is scaled.\nThe resources available to each function app instance.\nSupport for advanced functionality, such as Azure Virtual Network connectivity.\nSupport for Linux containers.\nThe plan you choose also impacts the costs for running your function code.\nOverview of plans\nFollowing is a summary of the benefits of the various hosting options:\nConsumption plan\nThe Consumption plan is the default hosting plan. Pay for compute resources only when your functions are running (pay-as-you-go) with automatic scale. On the Consumption plan, instances of the Functions host are dynamically added and removed based on the number of incoming events.\nFlex Consumption plan\nGet high scalability with compute choices, virtual networking, and pay-as-you-go billing. On the Flex Consumption plan, instances of the Functions host are dynamically added and removed based on the configured per instance concurrency and the number of incoming events.\nYou can reduce cold starts by specifying the number of pre-provisioned (always ready) instances. Scales automatically based on demand.\nPremium plan\nAutomatically scales based on demand using prewarmed workers, which run applications with no delay after being idle, runs on more powerful instances, and connects to virtual networks.\nConsider the Azure Functions Premium plan in the following situations:\nYour function apps run continuously, or nearly continuously.\nYou want more control of your instances and want to deploy multiple function apps on the same plan with event-driven scaling.\nYou have a high number of small executions and a high execution bill, but low GB seconds in the Consumption plan.\nYou need more CPU or memory options than are provided by consumption plans.\nYour code needs to run longer than the maximum execution time allowed on the Consumption plan.\nYou require virtual network connectivity.\nYou want to provide a custom Linux image in which to run your functions.\nDedicated plan\nRun your functions within an App Service plan at regular App Service plan rates. Best for long-running scenarios where Durable Functions can't be used.\nConsider an App Service plan in the following situations:\nYou must have fully predictable billing, or you need to manually scale instances.\nYou want to run multiple web apps and function apps on the same plan\nYou need access to larger compute size choices.\nFull compute isolation and secure network access provided by an App Service Environment (ASE).\nHigh memory usage and high scale (ASE).\nContainer Apps\nCreate and deploy containerized function apps in a fully managed environment hosted by Azure Container Apps.\nUse the Azure Functions programming model to build event-driven, serverless, cloud native function apps. Run your functions alongside other microservices, APIs, websites, and workflows as container-hosted programs.\nConsider hosting your functions on Container Apps in the following situations:\nYou want to package custom libraries with your function code to support line-of-business apps.\nYou need to migrate code execution from on-premises or legacy apps to cloud native microservices running in containers.\nYou want to avoid the overhead and complexity of managing Kubernetes clusters and dedicated compute.\nYou need the high-end processing power provided by dedicated CPU compute resources for your functions.\nFunction app time-out duration\nThe\nfunctionTimeout\nproperty in the\nhost.json\nproject file specifies the time-out duration for functions in a function app. This property applies specifically to function executions. After the trigger starts function execution, the function needs to return/respond within the time-out duration.\nThe following table shows the default and maximum values (in minutes) for specific plans:\nPlan\nDefault\nMaximum\n1\nFlex Consumption plan\n30\nUnbounded\n2\nPremium plan\n30\n4\nUnbounded\n2\nDedicated plan\n30\n4\nUnbounded\n3\nContainer Apps\n30\nUnbounded\n5\nConsumption plan\n5\n10\nRegardless of the function app time-out setting, 230 seconds is the maximum amount of time that an HTTP triggered function can take to respond to a request. This is because of the default idle time-out of Azure Load Balancer. For longer processing times, consider using the\nDurable Functions async pattern\nor defer the actual work and return an immediate response.\nThere's no maximum execution time-out duration enforced. However, the grace period given to a function execution is 60 minutes during scale in for the Flex Consumption and Premium plans, and a grace period of 10 minutes is given during platform updates.\nRequires the App Service plan be set to\nAlways On\n. A grace period of 10 minutes is given during platform updates.\nThe default time-out for version 1.x of the Functions host runtime is\nunbounded\n.\nWhen the minimum number of replicas is set to zero, the default time-out depends on the specific triggers used in the app.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Compare Azure Functions hosting options"
                },
                {
                  "level": 2,
                  "text": "Overview of plans"
                },
                {
                  "level": 3,
                  "text": "Consumption plan"
                },
                {
                  "level": 3,
                  "text": "Flex Consumption plan"
                },
                {
                  "level": 3,
                  "text": "Premium plan"
                },
                {
                  "level": 3,
                  "text": "Dedicated plan"
                },
                {
                  "level": 3,
                  "text": "Container Apps"
                },
                {
                  "level": 2,
                  "text": "Function app time-out duration"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [
                "functionTimeout"
              ],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                },
                {
                  "url": "/en-us/azure/azure-functions/consumption-plan",
                  "text": "Consumption plan"
                },
                {
                  "url": "/en-us/azure/azure-functions/flex-consumption-plan",
                  "text": "Flex Consumption plan"
                },
                {
                  "url": "/en-us/azure/azure-functions/functions-premium-plan",
                  "text": "Premium plan"
                },
                {
                  "url": "/en-us/azure/azure-functions/dedicated-plan",
                  "text": "Dedicated plan"
                },
                {
                  "url": "/en-us/azure/azure-functions/functions-container-apps-hosting",
                  "text": "Container Apps"
                },
                {
                  "url": "/en-us/azure/azure-functions/durable/durable-functions-overview#async-http",
                  "text": "Durable Functions async pattern"
                },
                {
                  "url": "/en-us/azure/azure-functions/dedicated-plan#always-on",
                  "text": "Always On"
                }
              ]
            },
            {
              "number": 4,
              "title": "Scale Azure Functions",
              "url": "https://learn.microsoft.com/en-us/training/modules/explore-azure-functions/4-scale-azure-functions",
              "href": "4-scale-azure-functions",
              "content": "Read in English\nAdd\nAdd to plan\nScale Azure Functions\nCompleted\n3 minutes\nThe following table compares the scaling behaviors of the various hosting plans. Maximum instances are given on a per-function app (Consumption) or per-plan (Premium/Dedicated) basis, unless otherwise indicated.\nPlan\nScale out\nMax # instances\nConsumption plan\nEvent driven. Scales out automatically, even during periods of high load. Functions infrastructure scales CPU and memory resources by adding more instances based on the number of incoming trigger events.\nWindows:\n200\nLinux:\n100\n1\nFlex Consumption plan\nPer-function scaling. Event-driven scaling decisions are calculated on a per-function basis, which provides a more deterministic way of scaling the functions in your app.\nLimited only by total memory usage of all instances across a given region.\nPremium plan\nEvent driven. Scale out automatically based on the number of events that its functions are triggered on.\nWindows:\n100\nLinux:\n20-100\n2\nDedicated plan\n3\nManual/autoscale\n10-30\n100 (ASE)\nContainer Apps\nEvent driven. Scale out automatically by adding more instances of the Functions host, based on the number of events that its functions are triggered on.\n10-300\n4\nDuring scale-out, there's currently a limit of 500 instances per subscription per hour for Linux 1. apps on a Consumption plan.\nIn some regions, Linux apps on a Premium plan can scale to 100 instances.\nFor specific limits for the various App Service plan options, see the\nApp Service plan limits\n.\nOn Container Apps, you can set the maximum number of replicas, which is honored as long as there's enough cores quota available\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Scale Azure Functions"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                },
                {
                  "url": "/en-us/azure/azure-resource-manager/management/azure-subscription-service-limits#app-service-limits",
                  "text": "App Service plan limits"
                }
              ]
            },
            {
              "number": 5,
              "title": "Module assessment",
              "url": "https://learn.microsoft.com/en-us/training/modules/explore-azure-functions/5-knowledge-check",
              "href": "5-knowledge-check",
              "content": "Read in English\nAdd\nAdd to plan\nModule assessment\nCompleted\n3 minutes\nCheck your knowledge\n1.\nAn organization wants to implement a serverless workflow to solve a business problem. One of the requirements is the solution needs to use a designer-first (declarative) development model. Which of the choices below meets the requirements?\nAzure Functions\nAzure Logic Apps\nWebJobs\n2.\nWhat is a key benefit of the Flex Consumption plan in Azure Functions hosting options?\nIt provides fully predictable billing and manual scale instances.\nIt offers high scalability with compute choices, virtual networking, and pay-as-you-go billing.\nIt allows for the packaging of custom libraries with function code.\n3.\nWhat is the maximum number of instances for a function app on a Consumption plan in Windows?\n300\n100\n200\nYou must answer all questions before checking your work.\nYou must answer all questions before checking your work.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Module assessment"
                },
                {
                  "level": 2,
                  "text": "Check your knowledge"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 6,
              "title": "Summary",
              "url": "https://learn.microsoft.com/en-us/training/modules/explore-azure-functions/6-summary",
              "href": "6-summary",
              "content": "Read in English\nAdd\nAdd to plan\nSummary\nCompleted\n3 minutes\nIn this module, you learned how to:\nExplain functional differences between Azure Functions, Azure Logic Apps, and WebJobs\nDescribe Azure Functions hosting plan options\nDescribe how Azure Functions scale to meet business needs\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Summary"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            }
          ]
        },
        {
          "title": "Develop Azure Functions",
          "url": "https://learn.microsoft.com/en-us/training/modules/develop-azure-functions/",
          "description": "",
          "learning_objectives": [],
          "prerequisites": [],
          "units": [
            {
              "number": 1,
              "title": "Introduction",
              "url": "https://learn.microsoft.com/en-us/training/modules/develop-azure-functions/1-introduction",
              "href": "1-introduction",
              "content": "Read in English\nAdd\nAdd to plan\nIntroduction\nCompleted\n3 minutes\nFunctions share a few core technical concepts and components, regardless of the language or binding you use.\nAfter completing this module, you'll be able to:\nExplain the key components of a function\nCreate triggers and bindings to control when a function runs and where the output is directed\nConnect a function to services in Azure\nCreate a function by using Visual Studio Code and the Azure Functions Core Tools\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Introduction"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 2,
              "title": "Explore Azure Functions development",
              "url": "https://learn.microsoft.com/en-us/training/modules/develop-azure-functions/2-azure-function-development-overview",
              "href": "2-azure-function-development-overview",
              "content": "Read in English\nAdd\nAdd to plan\nExplore Azure Functions development\nCompleted\n3 minutes\nA function app provides an execution context in Azure in which your functions run. As such, it's the unit of deployment and management for your functions. A function app is composed of one or more individual functions that are managed, deployed, and scaled together. All of the functions in a function app share the same pricing plan, deployment method, and runtime version. Think of a function app as a way to organize and collectively manage your functions.\nNote\nIn Functions 2.x, all functions in a function app must be authored in the same language. In previous versions of the Azure Functions runtime, this wasn't required.\nDevelop and test Azure Functions locally\nFunctions make it easy to use your favorite code editor and development tools to create and test functions on your local computer. Your local functions can connect to live Azure services, and you can debug them on your local computer using the full Functions runtime.\nThe way in which you develop functions on your local computer depends on your language and tooling preferences. For more information, see\nCode and test Azure Functions locally\n.\nNote\nBecause of limitations on editing function code in the Azure portal, you should develop your functions locally and publish your code project to a function app in Azure. For more information, see\nDevelopment limitations in the Azure portal\nLocal project files\nA Functions project directory contains the following files in the project root folder, regardless of language:\nhost.json\nlocal.settings.json\nOther files in the project depend on your language and specific functions.\nThe\nhost.json\nmetadata file contains configuration options that affect all functions in a function app instance. Other function app configuration options are managed depending on where the function app runs:\nDeployed to Azure:\nConfigured in your application settings\nOn your local computer:\nConfigured in the\nlocal.settings.json\nfile.\nConfigurations in\nhost.json\nrelated to bindings are applied equally to each function in the function app. You can also override or apply settings per environment using application settings. To learn more, see the\nhost.json reference\n.\nThe\nlocal.settings.json\nfile stores app settings, and settings used by local development tools. Settings in the\nlocal.settings.json\nfile are used only when you're running your project locally. When you publish your project to Azure, be sure to also add any required settings to the app settings for the function app.\nImportant\nBecause the\nlocal.settings.json\nmight contain secrets, such as connection strings, you should never store it in a remote repository.\nSynchronize settings\nWhen you develop your functions locally, any local settings required by your app must also be present in the app settings of the deployed function app. You can also download current settings from the function app to your local project.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Explore Azure Functions development"
                },
                {
                  "level": 2,
                  "text": "Develop and test Azure Functions locally"
                },
                {
                  "level": 3,
                  "text": "Local project files"
                },
                {
                  "level": 3,
                  "text": "Synchronize settings"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [
                "local.settings.json",
                "local.settings.json",
                "local.settings.json",
                "local.settings.json",
                "local.settings.json"
              ],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                },
                {
                  "url": "/en-us/azure/azure-functions/functions-develop-local",
                  "text": "Code and test Azure Functions locally"
                },
                {
                  "url": "/en-us/azure/azure-functions/functions-how-to-use-azure-function-app-settings#development-limitations-in-the-azure-portal",
                  "text": "Development limitations in the Azure portal"
                },
                {
                  "url": "/en-us/azure/azure-functions/functions-host-json",
                  "text": "host.json reference"
                }
              ]
            },
            {
              "number": 3,
              "title": "Create triggers and bindings",
              "url": "https://learn.microsoft.com/en-us/training/modules/develop-azure-functions/3-create-triggers-bindings",
              "href": "3-create-triggers-bindings",
              "content": "Read in English\nAdd\nAdd to plan\nCreate triggers and bindings\nCompleted\n8 minutes\nA trigger defines how a function is invoked and a function must have exactly one trigger. Triggers have associated data, which is often provided as the payload of the function.\nBinding to a function is a way of declaratively connecting another resource to the function; bindings might be connected as\ninput bindings\n,\noutput bindings\n, or both. Data from bindings is provided to the function as parameters.\nYou can mix and match different bindings to suit your needs. Bindings are optional and a function might have one or multiple input and/or output bindings.\nTriggers and bindings let you avoid hardcoding access to other services. Your function receives data (for example, the content of a queue message) in function parameters. You send data (for example, to create a queue message) by using the return value of the function.\nWhen you develop your functions locally, you need to take trigger and binding behaviors into consideration. For HTTP triggers, you can call the HTTP endpoint on the local computer, using\nhttp://localhost/\n. For non-HTTP triggered functions, there are several options to run locally:\nThe easiest way to test bindings during local development is to use connection strings that target live Azure services. You can target live services by adding the appropriate connection string settings in the\nValues\narray in the local.settings.json file. When you do this, local executions during testing use live service data. Because of this, consider setting-up separate services to use during development and testing, and then switch to different services during production.\nFor storage-based triggers, you can use the local\nAzurite emulator\nwhen testing functions with Azure Storage bindings (Queue Storage, Blob Storage, and Table Storage), without having to connect to remote storage services.\nYou can manually run non-HTTP trigger functions by using special administrator endpoints. For more information, see\nManually run a non HTTP-triggered function\n.\nTrigger and binding definitions\nTriggers and bindings are defined differently depending on the development language.\nLanguage\nConfigure triggers and bindings by...\nC# class library\ndecorating methods and parameters with C# attributes\nJava\ndecorating methods and parameters with Java annotations\nJavaScript/PowerShell/Python/TypeScript\nupdating\nfunction.json\nschema\nFor languages that rely on\nfunction.json\n, the portal provides a UI for adding bindings in the\nIntegration\ntab. You can also edit the file directly in the portal in the\nCode + test\ntab of your function.\nIn .NET and Java, the parameter type defines the data type for input data. For instance, use\nstring\nto bind to the text of a queue trigger, a byte array to read as binary, and a custom type to deserialize to an object. Since .NET class library functions and Java functions don't rely on\nfunction.json\nfor binding definitions, they can't be created and edited in the portal. C# portal editing is based on C# script, which uses\nfunction.json\ninstead of attributes.\nFor languages that are dynamically typed such as JavaScript, use the\ndataType\nproperty in the\nfunction.json\nfile. For example, to read the content of an HTTP request in binary format, set\ndataType\nto\nbinary\n:\n{\n    \"dataType\": \"binary\",\n    \"type\": \"httpTrigger\",\n    \"name\": \"req\",\n    \"direction\": \"in\"\n}\nOther options for\ndataType\nare\nstream\nand\nstring\n.\nBinding direction\nAll triggers and bindings have a direction property in the\nfunction.json\nfile:\nFor triggers, the direction is always\nin\nInput and output bindings use\nin\nand\nout\nSome bindings support a special direction\ninout\n. If you use\ninout\n, only the\nAdvanced editor\nis available via the\nIntegrate\ntab in the portal.\nWhen you use attributes in a class library to configure triggers and bindings, the direction is provided in an attribute constructor or inferred from the parameter type.\nAzure Functions trigger and binding example\nSuppose you want to write a new row to Azure Table storage whenever a new message appears in Azure Queue storage. This scenario can be implemented using an Azure Queue storage trigger and an Azure Table storage output binding.\nHere's a\nfunction.json\nfile for this scenario.\n{\n  \"disabled\": false,\n    \"bindings\": [\n        {\n            \"type\": \"queueTrigger\",\n            \"direction\": \"in\",\n            \"name\": \"myQueueItem\",\n            \"queueName\": \"myqueue-items\",\n            \"connection\":\"MyStorageConnectionAppSetting\"\n        },\n        {\n          \"tableName\": \"Person\",\n          \"connection\": \"MyStorageConnectionAppSetting\",\n          \"name\": \"tableBinding\",\n          \"type\": \"table\",\n          \"direction\": \"out\"\n        }\n  ]\n}\nThe first element in the\nbindings\narray is the Queue storage trigger. The\ntype\nand\ndirection\nproperties identify the trigger. The\nname\nproperty identifies the function parameter that receives the queue message content. The name of the queue to monitor is in\nqueueName\n, and the connection string is in the app setting identified by\nconnection\n.\nThe second element in the\nbindings\narray is the Azure Table Storage output binding. The\ntype\nand\ndirection\nproperties identify the binding. The\nname\nproperty specifies how the function provides the new table row, in this case by using the function return value. The name of the table is in\ntableName\n, and the connection string is in the app setting identified by\nconnection\n.\nC# function example\nFollowing is the same example represented in a C# function. The same trigger and binding information, queue and table names, storage accounts, and function parameters for input and output are provided by attributes instead of a\nfunction.json\nfile.\npublic static class QueueTriggerTableOutput\n{\n    [FunctionName(\"QueueTriggerTableOutput\")]\n    [return: Table(\"outTable\", Connection = \"MY_TABLE_STORAGE_ACCT_APP_SETTING\")]\n    public static Person Run(\n        [QueueTrigger(\"myqueue-items\", Connection = \"MY_STORAGE_ACCT_APP_SETTING\")]JObject order,\n        ILogger log)\n    {\n        return new Person() {\n                PartitionKey = \"Orders\",\n                RowKey = Guid.NewGuid().ToString(),\n                Name = order[\"Name\"].ToString(),\n                MobileNumber = order[\"MobileNumber\"].ToString() };\n    }\n}\n\npublic class Person\n{\n    public string PartitionKey { get; set; }\n    public string RowKey { get; set; }\n    public string Name { get; set; }\n    public string MobileNumber { get; set; }\n}\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Create triggers and bindings"
                },
                {
                  "level": 2,
                  "text": "Trigger and binding definitions"
                },
                {
                  "level": 2,
                  "text": "Binding direction"
                },
                {
                  "level": 2,
                  "text": "Azure Functions trigger and binding example"
                },
                {
                  "level": 3,
                  "text": "C# function example"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [
                "http://localhost/",
                "{\n    \"dataType\": \"binary\",\n    \"type\": \"httpTrigger\",\n    \"name\": \"req\",\n    \"direction\": \"in\"\n}",
                "{\n    \"dataType\": \"binary\",\n    \"type\": \"httpTrigger\",\n    \"name\": \"req\",\n    \"direction\": \"in\"\n}",
                "{\n  \"disabled\": false,\n    \"bindings\": [\n        {\n            \"type\": \"queueTrigger\",\n            \"direction\": \"in\",\n            \"name\": \"myQueueItem\",\n            \"queueName\": \"myqueue-items\",\n            \"connection\":\"MyStorageConnectionAppSetting\"\n        },\n        {\n          \"tableName\": \"Person\",\n          \"connection\": \"MyStorageConnectionAppSetting\",\n          \"name\": \"tableBinding\",\n          \"type\": \"table\",\n          \"direction\": \"out\"\n        }\n  ]\n}",
                "{\n  \"disabled\": false,\n    \"bindings\": [\n        {\n            \"type\": \"queueTrigger\",\n            \"direction\": \"in\",\n            \"name\": \"myQueueItem\",\n            \"queueName\": \"myqueue-items\",\n            \"connection\":\"MyStorageConnectionAppSetting\"\n        },\n        {\n          \"tableName\": \"Person\",\n          \"connection\": \"MyStorageConnectionAppSetting\",\n          \"name\": \"tableBinding\",\n          \"type\": \"table\",\n          \"direction\": \"out\"\n        }\n  ]\n}",
                "public static class QueueTriggerTableOutput\n{\n    [FunctionName(\"QueueTriggerTableOutput\")]\n    [return: Table(\"outTable\", Connection = \"MY_TABLE_STORAGE_ACCT_APP_SETTING\")]\n    public static Person Run(\n        [QueueTrigger(\"myqueue-items\", Connection = \"MY_STORAGE_ACCT_APP_SETTING\")]JObject order,\n        ILogger log)\n    {\n        return new Person() {\n                PartitionKey = \"Orders\",\n                RowKey = Guid.NewGuid().ToString(),\n                Name = order[\"Name\"].ToString(),\n                MobileNumber = order[\"MobileNumber\"].ToString() };\n    }\n}\n\npublic class Person\n{\n    public string PartitionKey { get; set; }\n    public string RowKey { get; set; }\n    public string Name { get; set; }\n    public string MobileNumber { get; set; }\n}",
                "public static class QueueTriggerTableOutput\n{\n    [FunctionName(\"QueueTriggerTableOutput\")]\n    [return: Table(\"outTable\", Connection = \"MY_TABLE_STORAGE_ACCT_APP_SETTING\")]\n    public static Person Run(\n        [QueueTrigger(\"myqueue-items\", Connection = \"MY_STORAGE_ACCT_APP_SETTING\")]JObject order,\n        ILogger log)\n    {\n        return new Person() {\n                PartitionKey = \"Orders\",\n                RowKey = Guid.NewGuid().ToString(),\n                Name = order[\"Name\"].ToString(),\n                MobileNumber = order[\"MobileNumber\"].ToString() };\n    }\n}\n\npublic class Person\n{\n    public string PartitionKey { get; set; }\n    public string RowKey { get; set; }\n    public string Name { get; set; }\n    public string MobileNumber { get; set; }\n}"
              ],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                },
                {
                  "url": "/en-us/azure/storage/common/storage-use-azurite",
                  "text": "Azurite emulator"
                },
                {
                  "url": "/en-us/azure/azure-functions/functions-manually-run-non-http",
                  "text": "Manually run a non HTTP-triggered function"
                }
              ]
            },
            {
              "number": 4,
              "title": "Connect functions to Azure services",
              "url": "https://learn.microsoft.com/en-us/training/modules/develop-azure-functions/4-connect-azure-services",
              "href": "4-connect-azure-services",
              "content": "Read in English\nAdd\nAdd to plan\nConnect functions to Azure services\nCompleted\n3 minutes\nAs a security best practice, Azure Functions takes advantage of the application settings functionality of Azure App Service to help you more securely store strings, keys, and other tokens required to connect to other services. Application settings in Azure are stored encrypted and accessed at runtime by your app as environment variable\nname\nvalue\npairs. For triggers and bindings that require a connection property, you set the application setting name instead of the actual connection string. You can't configure a binding directly with a connection string or key.\nThe default configuration provider uses environment variables. These variables are defined in application settings when running in the Azure and in the local settings file when developing locally.\nConfigure an identity-based connection\nSome connections in Azure Functions are configured to use an identity instead of a secret. Support depends on the extension using the connection. In some cases, a connection string might still be required in Functions even though the service to which you're connecting supports identity-based connections.\nNote\nAn app running in a Consumption or Elastic Premium plan, uses the\nWEBSITE_AZUREFILESCONNECTIONSTRING\nand\nWEBSITE_CONTENTSHARE\nsettings when connecting to Azure Files on the storage account used by your function app. Azure Files doesn't support using managed identity when accessing the file share.\nWhen hosted in the Azure Functions service, identity-based connections use a managed identity. The system-assigned identity is used by default, although a user-assigned identity can be specified with the\ncredential\nand\nclientID\nproperties. Configuring a user-assigned identity with a resource ID is\nnot\nsupported. When run in other contexts, such as local development, your developer identity is used instead, although this can be customized.\nGrant permission to the identity\nIdentities must have permissions to perform the intended actions. This is typically done by assigning a role in Azure role-based access control, or specifying the identity in an access policy depending on the service to which you're connecting.\nImportant\nThe target service might expose some permissions that aren't necessary for all contexts. Where possible, adhere to the\nprinciple of least privilege\n, granting the identity only required privileges.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Connect functions to Azure services"
                },
                {
                  "level": 2,
                  "text": "Configure an identity-based connection"
                },
                {
                  "level": 2,
                  "text": "Grant permission to the identity"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [
                "WEBSITE_AZUREFILESCONNECTIONSTRING",
                "WEBSITE_CONTENTSHARE"
              ],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 5,
              "title": "Exercise - Create an Azure Function by using Visual Studio Code",
              "url": "https://learn.microsoft.com/en-us/training/modules/develop-azure-functions/5-create-function-visual-studio-code",
              "href": "5-create-function-visual-studio-code",
              "content": "Read in English\nAdd\nAdd to plan\nExercise - Create an Azure Function by using Visual Studio Code\nCompleted\n15 minutes\nIn this exercise, you learn how to create a C# function that responds to HTTP requests. After creating and testing the code locally in Visual Studio Code, you deploy and test the function in Azure.\nTasks performed in this exercise:\nCreate your local project\nRun the function locally\nDeploy and execute the function in Azure\nClean up resources\nThis exercise takes approximately\n15\nminutes to complete.\nBefore you start\nTo complete the exercise, you need:\nAn Azure subscription. If you don't already have one, you can\nsign up for one\n.\nVisual Studio Code\non one of the\nsupported platforms\n.\n.NET 8\nis the target framework.\nC# Dev Kit\nfor Visual Studio Code.\nAzure Functions extension\nfor Visual Studio Code.\nAzure Functions Core Tools version 4.x. Run the following commands in a terminal to install Azure Functions Core Tools on your system. Visit\nAzure Function Core Tools on GitHub\nfor installation instructions on other platforms.\nwinget uninstall Microsoft.Azure.FunctionsCoreTools\nwinget install Microsoft.Azure.FunctionsCoreTools\nGetting started\nSelect the\nLaunch Exercise\nbutton, it opens the exercise instructions in a new browser window. When you're finished with the exercise, return here for:\nA quick knowledge check\nA summary of what you've learned\nTo earn a badge for completing this module\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Exercise - Create an Azure Function by using Visual Studio Code"
                },
                {
                  "level": 2,
                  "text": "Before you start"
                },
                {
                  "level": 2,
                  "text": "Getting started"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [
                "winget uninstall Microsoft.Azure.FunctionsCoreTools\nwinget install Microsoft.Azure.FunctionsCoreTools",
                "winget uninstall Microsoft.Azure.FunctionsCoreTools\nwinget install Microsoft.Azure.FunctionsCoreTools"
              ],
              "images": [
                {
                  "src": "../../wwl-azure/develop-azure-functions/media/launch-exercise.png",
                  "absolute_url": "https://learn.microsoft.com/en-us/wwl-azure/develop-azure-functions/media/launch-exercise.png",
                  "alt_text": "Button to launch exercise.",
                  "title": "",
                  "filename": "launch-exercise.png",
                  "image_type": "icon",
                  "context": {
                    "preceding_heading": "Getting started",
                    "following_text": "Was this page helpful?",
                    "figure_caption": "",
                    "parent_section": ""
                  }
                }
              ],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                },
                {
                  "url": "https://azure.microsoft.com/",
                  "text": "sign up for one"
                },
                {
                  "url": "https://code.visualstudio.com/",
                  "text": "Visual Studio Code"
                },
                {
                  "url": "https://code.visualstudio.com/docs/supporting/requirements#_platforms",
                  "text": "supported platforms"
                },
                {
                  "url": "https://dotnet.microsoft.com/en-us/download/dotnet/8.0",
                  "text": ".NET 8"
                },
                {
                  "url": "https://marketplace.visualstudio.com/items?itemName=ms-dotnettools.csdevkit",
                  "text": "C# Dev Kit"
                },
                {
                  "url": "https://marketplace.visualstudio.com/items?itemName=ms-azuretools.vscode-azurefunctions",
                  "text": "Azure Functions extension"
                },
                {
                  "url": "https://github.com/Azure/azure-functions-core-tools?tab=readme-ov-file#installing",
                  "text": "Azure Function Core Tools on GitHub"
                }
              ]
            },
            {
              "number": 6,
              "title": "Module assessment",
              "url": "https://learn.microsoft.com/en-us/training/modules/develop-azure-functions/6-knowledge-check",
              "href": "6-knowledge-check",
              "content": "Read in English\nAdd\nAdd to plan\nModule assessment\nCompleted\n3 minutes\nCheck your knowledge\n1.\nWhich of the following choices is required for a function to run?\nBinding\nTrigger\nBoth triggers and bindings\n2.\nWhich of the following choices supports both the\nin\nand\nout\ndirection settings?\nBindings\nTrigger\nConnection value\nYou must answer all questions before checking your work.\nYou must answer all questions before checking your work.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Module assessment"
                },
                {
                  "level": 2,
                  "text": "Check your knowledge"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 7,
              "title": "Summary",
              "url": "https://learn.microsoft.com/en-us/training/modules/develop-azure-functions/7-summary",
              "href": "7-summary",
              "content": "Read in English\nAdd\nAdd to plan\nSummary\nCompleted\n3 minutes\nIn this module, you learned how to:\nExplain the key components of a function\nCreate triggers and bindings to control when a function runs and where the output is directed\nConnect a function to services in Azure\nCreate a function by using Visual Studio Code and the Azure Functions Core Tools\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Summary"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "title": "Develop solutions that use Blob storage",
      "url": "https://learn.microsoft.com/en-us/training/paths/develop-solutions-that-use-blob-storage/",
      "learn_uid": "learn.wwl.develop-solutions-that-use-blob-storage",
      "modules": [
        {
          "title": "Explore Azure Blob storage",
          "url": "https://learn.microsoft.com/en-us/training/modules/explore-azure-blob-storage/",
          "description": "",
          "learning_objectives": [],
          "prerequisites": [],
          "units": [
            {
              "number": 1,
              "title": "Introduction",
              "url": "https://learn.microsoft.com/en-us/training/modules/explore-azure-blob-storage/1-introduction",
              "href": "1-introduction",
              "content": "Read in English\nAdd\nAdd to plan\nIntroduction\nCompleted\n3 minutes\nThis module will delve into the world of Azure Blob storage, Microsoft's cloud-based solution for storing large amounts of unstructured data. This service is versatile and can be used for a variety of purposes, from serving images or documents to a browser, to storing files for distributed access, and even streaming video and audio.\nThe topics covered in this module include:\nUnderstanding Azure Blob Storage: Features, Types of Storage Accounts, and Access Tiers\nUnderstanding Azure Blob Storage: Storage Accounts, Containers, and Blobs\nUnderstanding Azure Storage Security and Encryption Features\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Introduction"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 2,
              "title": "Explore Azure Blob storage",
              "url": "https://learn.microsoft.com/en-us/training/modules/explore-azure-blob-storage/2-blob-storage-overview",
              "href": "2-blob-storage-overview",
              "content": "Read in English\nAdd\nAdd to plan\nExplore Azure Blob storage\nCompleted\n3 minutes\nAzure Blob storage is Microsoft's object storage solution for the cloud. Blob storage is optimized for storing massive amounts of unstructured data. Unstructured data is data that doesn't adhere to a particular data model or definition, such as text or binary data.\nBlob storage is designed for:\nServing images or documents directly to a browser.\nStoring files for distributed access.\nStreaming video and audio.\nWriting to log files.\nStoring data for backup and restore, disaster recovery, and archiving.\nStoring data for analysis by an on-premises or Azure-hosted service.\nUsers or client applications can access objects in Blob storage via HTTP/HTTPS, from anywhere in the world. Objects in Blob storage are accessible via the Azure Storage REST API, Azure PowerShell, Azure CLI, or an Azure Storage client library.\nAn Azure Storage account is the top-level container for all of your Azure Blob storage. The storage account provides a unique namespace for your Azure Storage data that is accessible from anywhere in the world over HTTP or HTTPS.\nTypes of storage accounts\nAzure Storage offers two performance levels of storage accounts, standard and premium. Each performance level supports different features and has its own pricing model.\nStandard:\nThis is the standard general-purpose v2 account and is recommended for most scenarios using Azure Storage.\nPremium:\nPremium accounts offer higher performance by using solid-state drives. If you create a premium account you can choose between three account types, block blobs, page blobs, or file shares.\nThe following table describes the types of storage accounts recommended by Microsoft for most scenarios using Blob storage.\nType of storage account\nSupported storage services\nRedundancy options\nUsage\nStandard general-purpose v2\nBlob Storage (including Data Lake Storage), Queue Storage, Table Storage, and Azure Files\nLocally redundant storage (LRS) / geo-redundant storage (GRS) / read-access geo-redundant storage (RA-GRS)\nZone-redundant storage (ZRS) / geo-zone-redundant storage (GZRS) / read-access geo-zone-redundant storage (RA-GZRS)\nStandard storage account type for blobs, file shares, queues, and tables. Recommended for most scenarios using Azure Storage. If you want support for network file system (NFS) in Azure Files, use the premium file shares account type.\nPremium block blobs\nBlob Storage (including Data Lake Storage)\nLRS and ZRS\nPremium storage account type for block blobs and append blobs. Recommended for scenarios with high transaction rates or that use smaller objects or require consistently low storage latency.\nPremium file shares\nAzure Files\nLRS and ZRS\nPremium storage account type for file shares only. Recommended for enterprise or high-performance scale applications.\nPremium page blobs\nPage blobs only\nLRS and ZRS\nPremium storage account type for page blobs only.\nAccess tiers for block blob data\nAzure Storage provides different options for accessing block blob data based on usage patterns. Each access tier in Azure Storage is optimized for a particular pattern of data usage. By selecting the right access tier for your needs, you can store your block blob data in the most cost-effective manner.\nThe available access tiers are:\nThe\nHot\naccess tier, which is optimized for frequent access of objects in the storage account. The Hot tier has the highest storage costs, but the lowest access costs. New storage accounts are created in the hot tier by default.\nThe\nCool\naccess tier, which is optimized for storing large amounts of data that is infrequently accessed and stored for a minimum of 30 days. The Cool tier has lower storage costs and higher access costs compared to the Hot tier.\nThe\nCold\naccess tier, which is optimized for storing data that is infrequently accessed and stored for a minimum of 90 days. The cold tier has lower storage costs and higher access costs compared to the cool tier.\nThe\nArchive\ntier, which is available only for individual block blobs. The archive tier is optimized for data that can tolerate several hours of retrieval latency and remains in the Archive tier for a minimum 180 days. The archive tier is the most cost-effective option for storing data, but accessing that data is more expensive than accessing data in the hot or cool tiers.\nIf there's a change in the usage pattern of your data, you can switch between these access tiers at any time.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Explore Azure Blob storage"
                },
                {
                  "level": 2,
                  "text": "Types of storage accounts"
                },
                {
                  "level": 2,
                  "text": "Access tiers for block blob data"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 3,
              "title": "Discover Azure Blob storage resource types",
              "url": "https://learn.microsoft.com/en-us/training/modules/explore-azure-blob-storage/3-blob-storage-resources",
              "href": "3-blob-storage-resources",
              "content": "Read in English\nAdd\nAdd to plan\nDiscover Azure Blob storage resource types\nCompleted\n3 minutes\nBlob storage offers three types of resources:\nThe\nstorage account\n.\nA\ncontainer\nin the storage account\nA\nblob\nin a container\nStorage accounts\nA storage account provides a unique namespace in Azure for your data. Every object that you store in Azure Storage has an address that includes your unique account name. The combination of the account name and the Azure Storage blob endpoint forms the base address for the objects in your storage account.\nFor example, if your storage account is named\nmystorageaccount\n, then the default endpoint for Blob storage is:\nhttp://mystorageaccount.blob.core.windows.net\nContainers\nA container organizes a set of blobs, similar to a directory in a file system. A storage account can include an unlimited number of containers, and a container can store an unlimited number of blobs.\nA container name must be a valid DNS name, as it forms part of the unique URI (Uniform resource identifier) used to address the container or its blobs. Follow these rules when naming a container:\nContainer names can be between 3 and 63 characters long.\nContainer names must start with a letter or number, and can contain only lowercase letters, numbers, and the dash (-) character.\nTwo or more consecutive dash characters aren't permitted in container names.\nThe URI for a container is similar to:\nhttps://myaccount.blob.core.windows.net/mycontainer\nBlobs\nAzure Storage supports three types of blobs:\nBlock blobs\nstore text and binary data. Block blobs are made up of blocks of data that can be managed individually. Block blobs can store up to about 190.7 TiB.\nAppend blobs\nare made up of blocks like block blobs, but are optimized for append operations. Append blobs are ideal for scenarios such as logging data from virtual machines.\nPage blobs\nstore random access files up to 8 TB in size. Page blobs store virtual hard drive (VHD) files and serve as disks for Azure virtual machines.\nThe URI for a blob is similar to:\nhttps://myaccount.blob.core.windows.net/mycontainer/myblob\nor\nhttps://myaccount.blob.core.windows.net/mycontainer/myvirtualdirectory/myblob\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Discover Azure Blob storage resource types"
                },
                {
                  "level": 2,
                  "text": "Storage accounts"
                },
                {
                  "level": 2,
                  "text": "Containers"
                },
                {
                  "level": 2,
                  "text": "Blobs"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [
                "http://mystorageaccount.blob.core.windows.net",
                "http://mystorageaccount.blob.core.windows.net",
                "https://myaccount.blob.core.windows.net/mycontainer",
                "https://myaccount.blob.core.windows.net/mycontainer",
                "https://myaccount.blob.core.windows.net/mycontainer/myblob",
                "https://myaccount.blob.core.windows.net/mycontainer/myblob",
                "https://myaccount.blob.core.windows.net/mycontainer/myvirtualdirectory/myblob",
                "https://myaccount.blob.core.windows.net/mycontainer/myvirtualdirectory/myblob"
              ],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 4,
              "title": "Explore Azure Storage security features",
              "url": "https://learn.microsoft.com/en-us/training/modules/explore-azure-blob-storage/4-blob-storage-security",
              "href": "4-blob-storage-security",
              "content": "Read in English\nAdd\nAdd to plan\nExplore Azure Storage security features\nCompleted\n5 minutes\nAzure Storage uses service-side encryption (SSE) to automatically encrypt your data when it's persisted to the cloud. Azure Storage encryption protects your data and to help you to meet your organizational security and compliance commitments.\nMicrosoft recommends using service-side encryption to protect your data for most scenarios. However, the Azure Storage client libraries for Blob Storage and Queue Storage also provide client-side encryption for customers who need to encrypt data on the client.\nAzure Storage encryption for data at rest\nAzure Storage automatically encrypts your data when persisting it to the cloud. Encryption protects your data and helps you meet your organizational security and compliance commitments. Data in Azure Storage is encrypted and decrypted transparently using 256-bit Advanced Encryption Standard (AES) encryption, one of the strongest block ciphers available, and is Federal Information Processing Standards (FIPS) 140-2 compliant. Azure Storage encryption is similar to BitLocker encryption on Windows.\nAzure Storage encryption is enabled for all storage accounts and can't be disabled. Because your data is secured by default, you don't need to modify your code or applications to take advantage of Azure Storage encryption.\nData in a storage account is encrypted regardless of performance tier, access tier, or deployment model. All new and existing block blobs, append blobs, and page blobs are encrypted, including blobs in the archive tier. All Azure Storage redundancy options support encryption, and all data in both the primary and secondary regions is encrypted when geo-replication is enabled. All Azure Storage resources are encrypted, including blobs, disks, files, queues, and tables. All object metadata is also encrypted.\nThere's no extra cost for Azure Storage encryption.\nEncryption key management\nData in a new storage account is encrypted with Microsoft-managed keys by default. You can continue to rely on Microsoft-managed keys for the encryption of your data, or you can manage encryption with your own keys. If you choose to manage encryption with your own keys, you have two options. You can use either type of key management, or both:\nYou can specify a\ncustomer-managed key\nto use for encrypting and decrypting data in Blob Storage and in Azure Files.Customer-managed keys must be stored in Azure Key Vault or Azure Key Vault Managed Hardware Security Model (HSM).\nYou can specify a\ncustomer-provided key\non Blob Storage operations. A client can include an encryption key on a read/write request for granular control over how blob data is encrypted and decrypted.\nThe following table compares key management options for Azure Storage encryption.\nKey management parameter\nMicrosoft-managed keys\nCustomer-managed keys\nCustomer-provided keys\nEncryption/decryption operations\nAzure\nAzure\nAzure\nAzure Storage services supported\nAll\nBlob Storage, Azure Files\nBlob Storage\nKey storage\nMicrosoft key store\nAzure Key Vault or Key Vault HSM\nCustomer's own key store\nKey rotation responsibility\nMicrosoft\nCustomer\nCustomer\nKey control\nMicrosoft\nCustomer\nCustomer\nKey scope\nAccount (default), container, or blob\nAccount (default), container, or blob\nN/A\nClient-side encryption\nThe Azure Blob Storage client libraries for .NET, Java, and Python support encrypting data within client applications before uploading to Azure Storage, and decrypting data while downloading to the client. The Queue Storage client libraries for .NET and Python also support client-side encryption.\nThe Blob Storage and Queue Storage client libraries uses AES in order to encrypt user data. There are two versions of client-side encryption available in the client libraries:\nVersion 2 uses Galois/Counter Mode (GCM) mode with AES. The Blob Storage and Queue Storage SDKs support client-side encryption with v2.\nVersion 1 uses Cipher Block Chaining (CBC) mode with AES. The Blob Storage, Queue Storage, and Table Storage SDKs support client-side encryption with v1.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Explore Azure Storage security features"
                },
                {
                  "level": 2,
                  "text": "Azure Storage encryption for data at rest"
                },
                {
                  "level": 3,
                  "text": "Encryption key management"
                },
                {
                  "level": 2,
                  "text": "Client-side encryption"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 5,
              "title": "Module assessment",
              "url": "https://learn.microsoft.com/en-us/training/modules/explore-azure-blob-storage/5-knowledge-check",
              "href": "5-knowledge-check",
              "content": "Read in English\nAdd\nAdd to plan\nModule assessment\nCompleted\n3 minutes\nCheck your knowledge\n1.\nWhich of the following types of blobs are used to store virtual hard drive files?\nBlock blobs\nAppend blobs\nPage blobs\n2.\nWhich of the following types of storage accounts is recommended for most scenarios using Azure Storage?\nGeneral-purpose v2\nGeneral-purpose v1\nFileStorage\n3.\nWhat is the maximum size of data that a block blob in Azure Blob storage can store?\n8-TB\nUnlimited\n190.7-TiB\n4.\nWhat are the two versions of client-side encryption available in the Azure Blob Storage and Queue Storage client libraries?\nVersion 1 uses Cipher Block Chaining (CBC) mode with AES and Version 2 uses Galois/Counter Mode (GCM) mode with AES\nVersion 1 uses Advanced Encryption Standard (AES) and Version 2 uses Federal Information Processing Standards (FIPS)\nVersion 1 uses Galois/Counter Mode (GCM) mode with AES and Version 2 uses Cipher Block Chaining (CBC) mode with AES\nYou must answer all questions before checking your work.\nYou must answer all questions before checking your work.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Module assessment"
                },
                {
                  "level": 2,
                  "text": "Check your knowledge"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 6,
              "title": "Summary",
              "url": "https://learn.microsoft.com/en-us/training/modules/explore-azure-blob-storage/6-summary",
              "href": "6-summary",
              "content": "Read in English\nAdd\nAdd to plan\nSummary\nCompleted\n3 minutes\nThe topics you learned about in this module included:\nUnderstanding Azure Blob Storage: Features, Types of Storage Accounts, and Access Tiers\nUnderstanding Azure Blob Storage: Storage Accounts, Containers, and Blobs\nUnderstanding Azure Storage Security and Encryption Features\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Summary"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            }
          ]
        },
        {
          "title": "Manage the Azure Blob storage lifecycle",
          "url": "https://learn.microsoft.com/en-us/training/modules/manage-azure-blob-storage-lifecycle/",
          "description": "",
          "learning_objectives": [],
          "prerequisites": [],
          "units": [
            {
              "number": 1,
              "title": "Introduction",
              "url": "https://learn.microsoft.com/en-us/training/modules/manage-azure-blob-storage-lifecycle/1-introduction",
              "href": "1-introduction",
              "content": "Read in English\nAdd\nAdd to plan\nIntroduction\nCompleted\n3 minutes\nData sets have unique lifecycles. Early in the lifecycle, people access some data often. But the need for access drops drastically as the data ages. Some data stays idle in the cloud and is rarely accessed once stored.\nAfter completing this module, you'll be able to:\nDescribe how each of the access tiers is optimized.\nCreate and implement a lifecycle policy.\nRehydrate blob data stored in an archive tier.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Introduction"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 2,
              "title": "Explore the Azure Blob storage lifecycle",
              "url": "https://learn.microsoft.com/en-us/training/modules/manage-azure-blob-storage-lifecycle/2-blob-storage-lifecycle",
              "href": "2-blob-storage-lifecycle",
              "content": "Read in English\nAdd\nAdd to plan\nExplore the Azure Blob storage lifecycle\nCompleted\n3 minutes\nData sets have unique lifecycles. Early in the lifecycle, people access some data often. But the need for access drops drastically as the data ages. Some data stays idle in the cloud and is rarely accessed once stored. Some data expires days or months after creation, while other data sets are actively read and modified throughout their lifetimes.\nAccess tiers\nAzure storage offers different access tiers, allowing you to store blob object data in the most cost-effective manner. Available access tiers include:\nHot\n- An online tier optimized for storing data that is accessed frequently.\nCool\n- An online tier optimized for storing data that is infrequently accessed and stored for a minimum of 30 days.\nCold tier\n- An online tier optimized for storing data that is infrequently accessed and stored for a minimum of 90 days. The cold tier has lower storage costs and higher access costs compared to the cool tier.\nArchive\n- An offline tier optimized for storing data that is rarely accessed and stored for at least 180 days with flexible latency requirements, on the order of hours.\nData storage limits are set at the account level and not per access tier. You can choose to use all of your limit in one tier or across all three tiers.\nManage the data lifecycle\nAzure Blob Storage lifecycle management offers a rule-based policy that you can use to transition blob data to the appropriate access tiers or to expire data at the end of the data lifecycle.\nWith the lifecycle management policy, you can:\nTransition blobs from cool to hot immediately when accessed, to optimize for performance.\nTransition current versions of a blob, previous versions of a blob, or blob snapshots to a cooler storage tier if these objects aren't accessed or modified for a period of time, to optimize for cost.\nDelete current versions of a blob, previous versions of a blob, or blob snapshots at the end of their lifecycles.\nApply rules to an entire storage account, to select containers, or to a subset of blobs using name prefixes or blob index tags as filters.\nConsider a scenario where data is frequently accessed during the early stages of the lifecycle, but only occasionally after two weeks. Beyond the first month, the data set is rarely accessed. In this scenario, hot storage is best during the early stages. Cool storage is most appropriate for occasional access. Archive storage is the best tier option after the data ages over a month. By moving data to the appropriate storage tier based on its age with lifecycle management policy rules, you can design the least expensive solution for your needs.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Explore the Azure Blob storage lifecycle"
                },
                {
                  "level": 2,
                  "text": "Access tiers"
                },
                {
                  "level": 2,
                  "text": "Manage the data lifecycle"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 3,
              "title": "Discover Blob storage lifecycle policies",
              "url": "https://learn.microsoft.com/en-us/training/modules/manage-azure-blob-storage-lifecycle/3-blob-storage-lifecycle-policies",
              "href": "3-blob-storage-lifecycle-policies",
              "content": "Read in English\nAdd\nAdd to plan\nDiscover Blob storage lifecycle policies\nCompleted\n3 minutes\nA lifecycle management policy is a collection of rules in a JSON document. Each rule definition within a policy includes a filter set and an action set. The filter set limits rule actions to a certain set of objects within a container or objects names. The action set applies the tier or delete actions to the filtered set of objects:\n{\n  \"rules\": [\n    {\n      \"name\": \"rule1\",\n      \"enabled\": true,\n      \"type\": \"Lifecycle\",\n      \"definition\": {...}\n    },\n    {\n      \"name\": \"rule2\",\n      \"type\": \"Lifecycle\",\n      \"definition\": {...}\n    }\n  ]\n}\nA policy is a collection of rules:\nParameter name\nParameter type\nNotes\nrules\nAn array of rule objects\nAt least one rule is required in a policy. You can define up to 100 rules in a policy.\nEach rule within the policy has several parameters:\nParameter name\nParameter type\nNotes\nRequired\nname\nString\nA rule name can include up to 256 alphanumeric characters. Rule name is case-sensitive. It must be unique within a policy.\nTrue\nenabled\nBoolean\nAn optional boolean to allow a rule to be temporarily disabled. Default value is true.\nFalse\ntype\nAn enum value\nThe current valid type is Lifecycle.\nTrue\ndefinition\nAn object that defines the lifecycle rule\nEach definition is made up of a filter set and an action set.\nTrue\nRules\nEach rule definition includes a filter set and an action set. The filter set limits rule actions to a certain set of objects within a container or objects names. The action set applies the tier or delete actions to the filtered set of objects.\nThe following sample rule filters the account to run the actions on objects that exist inside\nsample-container\nand start with\nblob1\n.\nTier blob to cool tier 30 days after last modification\nTier blob to archive tier 90 days after last modification\nDelete blob 2,555 days (seven years) after last modification\nDelete blob snapshots 90 days after snapshot creation\n{\n  \"rules\": [\n    {\n      \"enabled\": true,\n      \"name\": \"sample-rule\",\n      \"type\": \"Lifecycle\",\n      \"definition\": {\n        \"actions\": {\n          \"version\": {\n            \"delete\": {\n              \"daysAfterCreationGreaterThan\": 90\n            }\n          },\n          \"baseBlob\": {\n            \"tierToCool\": {\n              \"daysAfterModificationGreaterThan\": 30\n            },\n            \"tierToArchive\": {\n              \"daysAfterModificationGreaterThan\": 90,\n              \"daysAfterLastTierChangeGreaterThan\": 7\n            },\n            \"delete\": {\n              \"daysAfterModificationGreaterThan\": 2555\n            }\n          }\n        },\n        \"filters\": {\n          \"blobTypes\": [\n            \"blockBlob\"\n          ],\n          \"prefixMatch\": [\n            \"sample-container/blob1\"\n          ]\n        }\n      }\n    }\n  ]\n}\nRule filters\nFilters limit rule actions to a subset of blobs within the storage account. If more than one filter is defined, a logical AND runs on all filters. Filters include:\nFilter name\nType\nIs Required\nblobTypes\nAn array of predefined enum values.\nYes\nprefixMatch\nAn array of strings for prefixes to be match. Each rule can define up to 10 prefixes. A prefix string must start with a container name.\nNo\nblobIndexMatch\nAn array of dictionary values consisting of blob index tag key and value conditions to be matched. Each rule can define up to 10 blob index tag condition.\nNo\nRule actions\nActions are applied to the filtered blobs when the run condition is met.\nLifecycle management supports tiering and deletion of blobs and deletion of blob snapshots. Define at least one action for each rule on blobs or blob snapshots.\nAction\nCurrent Version\nSnapshot\nPrevious Versions\ntierToCool\nSupported for\nblockBlob\nSupported\nSupported\ntierToCold\nSupported for\nblockBlob\nSupported\nSupported\nenableAutoTierToHotFromCool\nSupported for\nblockBlob\nNot supported\nNot supported\ntierToArchive\nSupported for\nblockBlob\nSupported\nSupported\ndelete\nSupported for\nblockBlob\nand\nappendBlob\nSupported\nSupported\nNote\nIf you define more than one action on the same blob, lifecycle management applies the least expensive action to the blob. For example, action\ndelete\nis cheaper than action\ntierToArchive\n. Action\ntierToArchive\nis cheaper than action\ntierToCool\n.\nThe run conditions are based on age. Base blobs use the last modified time to track age, and blob snapshots use the snapshot creation time to track age.\nAction run condition\nCondition value\nDescription\ndaysAfterModificationGreaterThan\nInteger value indicating the age in days\nThe condition for base blob actions\ndaysAfterCreationGreaterThan\nInteger value indicating the age in days\nThe condition for blob snapshot actions\ndaysAfterLastAccessTimeGreaterThan\nInteger value indicating the age in days\nThe condition for a current version of a blob when access tracking is enabled\ndaysAfterLastTierChangeGreaterThan\nInteger value indicating the age in days after last blob tier change time\nThe minimum duration in days that a rehydrated blob is kept in hot, cool, or cold tiers before being returned to the archive tier. This condition applies only to\ntierToArchive\nactions.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Discover Blob storage lifecycle policies"
                },
                {
                  "level": 2,
                  "text": "Rules"
                },
                {
                  "level": 2,
                  "text": "Rule filters"
                },
                {
                  "level": 2,
                  "text": "Rule actions"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [
                "{\n  \"rules\": [\n    {\n      \"name\": \"rule1\",\n      \"enabled\": true,\n      \"type\": \"Lifecycle\",\n      \"definition\": {...}\n    },\n    {\n      \"name\": \"rule2\",\n      \"type\": \"Lifecycle\",\n      \"definition\": {...}\n    }\n  ]\n}",
                "{\n  \"rules\": [\n    {\n      \"name\": \"rule1\",\n      \"enabled\": true,\n      \"type\": \"Lifecycle\",\n      \"definition\": {...}\n    },\n    {\n      \"name\": \"rule2\",\n      \"type\": \"Lifecycle\",\n      \"definition\": {...}\n    }\n  ]\n}",
                "sample-container",
                "{\n  \"rules\": [\n    {\n      \"enabled\": true,\n      \"name\": \"sample-rule\",\n      \"type\": \"Lifecycle\",\n      \"definition\": {\n        \"actions\": {\n          \"version\": {\n            \"delete\": {\n              \"daysAfterCreationGreaterThan\": 90\n            }\n          },\n          \"baseBlob\": {\n            \"tierToCool\": {\n              \"daysAfterModificationGreaterThan\": 30\n            },\n            \"tierToArchive\": {\n              \"daysAfterModificationGreaterThan\": 90,\n              \"daysAfterLastTierChangeGreaterThan\": 7\n            },\n            \"delete\": {\n              \"daysAfterModificationGreaterThan\": 2555\n            }\n          }\n        },\n        \"filters\": {\n          \"blobTypes\": [\n            \"blockBlob\"\n          ],\n          \"prefixMatch\": [\n            \"sample-container/blob1\"\n          ]\n        }\n      }\n    }\n  ]\n}",
                "{\n  \"rules\": [\n    {\n      \"enabled\": true,\n      \"name\": \"sample-rule\",\n      \"type\": \"Lifecycle\",\n      \"definition\": {\n        \"actions\": {\n          \"version\": {\n            \"delete\": {\n              \"daysAfterCreationGreaterThan\": 90\n            }\n          },\n          \"baseBlob\": {\n            \"tierToCool\": {\n              \"daysAfterModificationGreaterThan\": 30\n            },\n            \"tierToArchive\": {\n              \"daysAfterModificationGreaterThan\": 90,\n              \"daysAfterLastTierChangeGreaterThan\": 7\n            },\n            \"delete\": {\n              \"daysAfterModificationGreaterThan\": 2555\n            }\n          }\n        },\n        \"filters\": {\n          \"blobTypes\": [\n            \"blockBlob\"\n          ],\n          \"prefixMatch\": [\n            \"sample-container/blob1\"\n          ]\n        }\n      }\n    }\n  ]\n}",
                "tierToArchive",
                "tierToArchive",
                "tierToArchive"
              ],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 4,
              "title": "Implement Blob storage lifecycle policies",
              "url": "https://learn.microsoft.com/en-us/training/modules/manage-azure-blob-storage-lifecycle/4-add-policy-blob-storage",
              "href": "4-add-policy-blob-storage",
              "content": "Read in English\nAdd\nAdd to plan\nImplement Blob storage lifecycle policies\nCompleted\n3 minutes\nYou can add, edit, or remove a policy by using any of the following methods:\nAzure portal\nAzure PowerShell\nAzure CLI\nREST APIs\nThe following are the steps and some examples for the Portal and Azure CLI.\nAzure portal\nThere are two ways to add a policy through the Azure portal: Azure portal List view, and Azure portal Code view. Following is an example of how to add a policy in the Azure portal Code view.\nAzure portal Code view\nIn the Azure portal, navigate to your storage account.\nUnder\nData management\n, select\nLifecycle Management\nto view or change lifecycle management policies.\nSelect the\nCode View\ntab. On this tab, you can define a lifecycle management policy in JSON.\nThe following JSON is an example of a policy that  moves a block blob whose name begins with\nlog\nto the cool tier if it has been more than 30 days since the blob was modified.\n{\n  \"rules\": [\n    {\n      \"enabled\": true,\n      \"name\": \"move-to-cool\",\n      \"type\": \"Lifecycle\",\n      \"definition\": {\n        \"actions\": {\n          \"baseBlob\": {\n            \"tierToCool\": {\n              \"daysAfterModificationGreaterThan\": 30\n            }\n          }\n        },\n        \"filters\": {\n          \"blobTypes\": [\n            \"blockBlob\"\n          ],\n          \"prefixMatch\": [\n            \"sample-container/log\"\n          ]\n        }\n      }\n    }\n  ]\n}\nAzure CLI\nTo add a lifecycle management policy with Azure CLI, write the policy to a JSON file, then call the\naz storage account management-policy create\ncommand to create the policy.\naz storage account management-policy create \\\n    --account-name <storage-account> \\\n    --policy @policy.json \\\n    --resource-group <resource-group>\nA lifecycle management policy must be read or written in full. Partial updates aren't supported.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Implement Blob storage lifecycle policies"
                },
                {
                  "level": 2,
                  "text": "Azure portal"
                },
                {
                  "level": 3,
                  "text": "Azure portal Code view"
                },
                {
                  "level": 2,
                  "text": "Azure CLI"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [
                "{\n  \"rules\": [\n    {\n      \"enabled\": true,\n      \"name\": \"move-to-cool\",\n      \"type\": \"Lifecycle\",\n      \"definition\": {\n        \"actions\": {\n          \"baseBlob\": {\n            \"tierToCool\": {\n              \"daysAfterModificationGreaterThan\": 30\n            }\n          }\n        },\n        \"filters\": {\n          \"blobTypes\": [\n            \"blockBlob\"\n          ],\n          \"prefixMatch\": [\n            \"sample-container/log\"\n          ]\n        }\n      }\n    }\n  ]\n}",
                "{\n  \"rules\": [\n    {\n      \"enabled\": true,\n      \"name\": \"move-to-cool\",\n      \"type\": \"Lifecycle\",\n      \"definition\": {\n        \"actions\": {\n          \"baseBlob\": {\n            \"tierToCool\": {\n              \"daysAfterModificationGreaterThan\": 30\n            }\n          }\n        },\n        \"filters\": {\n          \"blobTypes\": [\n            \"blockBlob\"\n          ],\n          \"prefixMatch\": [\n            \"sample-container/log\"\n          ]\n        }\n      }\n    }\n  ]\n}",
                "az storage account management-policy create",
                "az storage account management-policy create \\\n    --account-name <storage-account> \\\n    --policy @policy.json \\\n    --resource-group <resource-group>",
                "az storage account management-policy create \\\n    --account-name <storage-account> \\\n    --policy @policy.json \\\n    --resource-group <resource-group>"
              ],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 5,
              "title": "Rehydrate blob data from the archive tier",
              "url": "https://learn.microsoft.com/en-us/training/modules/manage-azure-blob-storage-lifecycle/5-rehydrate-blob-data",
              "href": "5-rehydrate-blob-data",
              "content": "Read in English\nAdd\nAdd to plan\nRehydrate blob data from the archive tier\nCompleted\n3 minutes\nWhile a blob is in the archive access tier, it's considered to be offline and can't be read or modified. In order to read or modify data in an archived blob, you must first rehydrate the blob to an online tier, either the hot or cool tier. There are two options for rehydrating a blob that is stored in the archive tier:\nCopy an archived blob to an online tier\n: You can rehydrate an archived blob by copying it to a new blob in the hot or cool tier with the\nCopy Blob\nor\nCopy Blob from URL\noperation. Microsoft recommends this option for most scenarios.\nChange a blob's access tier to an online tier\n: You can rehydrate an archived blob to hot or cool by changing its tier using the\nSet Blob Tier\noperation.\nRehydrating a blob from the archive tier can take several hours to complete. Microsoft recommends rehydrating larger blobs for optimal performance. Rehydrating several small blobs concurrently might require extra time.\nRehydration priority\nWhen you rehydrate a blob, you can set the priority for the rehydration operation via the optional\nx-ms-rehydrate-priority\nheader on a\nSet Blob Tier\nor\nCopy Blob/Copy Blob From URL\noperation. Rehydration priority options include:\nStandard priority\n: The rehydration request is processed in the order it was received and might take up to 15 hours.\nHigh priority\n: The rehydration request is prioritized over standard priority requests and might complete in under one hour for objects under 10 GB in size.\nTo check the rehydration priority while the rehydration operation is underway, call\nGet Blob Properties\nto return the value of the\nx-ms-rehydrate-priority\nheader. The rehydration priority property returns either\nStandard\nor\nHigh\n.\nCopy an archived blob to an online tier\nThe first option for moving a blob from the archive tier to an online tier is to copy the archived blob to a new destination blob that is in either the hot or cool tier. You can use the\nCopy Blob\noperation to copy the blob. When you copy an archived blob to a new blob in an online tier, the source blob remains unmodified in the archive tier. You must copy the archived blob to a new blob with a different name or to a different container. You can't overwrite the source blob by copying to the same blob.\nRehydrating an archived blob by copying it to an online destination tier is supported within the same storage account only for service versions earlier than 2021-02-12. Beginning with service version 2021-02-12, you can rehydrate an archived blob by copying it to a different storage account, as long as the destination account is in the same region as the source account.\nChange a blob's access tier to an online tier\nThe second option for rehydrating a blob from the archive tier to an online tier is to change the blob's tier by calling\nSet Blob Tier\n. With this operation, you can change the tier of the archived blob to either hot or cool.\nOnce a\nSet Blob Tier\nrequest is initiated, it can't be canceled. During the rehydration operation, the blob's access tier setting continues to show as archived until the rehydration process is complete.\nTo learn how to rehydrate a blob by changing its tier to an online tier, see\nRehydrate a blob by changing its tier\n.\nCaution\nChanging a blob's tier doesn't affect its last modified time. If there is a lifecycle management policy in effect for the storage account, then rehydrating a blob with\nSet Blob Tier\ncan result in a scenario where the lifecycle policy moves the blob back to the archive tier after rehydration because the last modified time is beyond the threshold set for the policy.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Rehydrate blob data from the archive tier"
                },
                {
                  "level": 2,
                  "text": "Rehydration priority"
                },
                {
                  "level": 2,
                  "text": "Copy an archived blob to an online tier"
                },
                {
                  "level": 2,
                  "text": "Change a blob's access tier to an online tier"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [
                "x-ms-rehydrate-priority",
                "x-ms-rehydrate-priority"
              ],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                },
                {
                  "url": "/en-us/rest/api/storageservices/copy-blob",
                  "text": "Copy Blob"
                },
                {
                  "url": "/en-us/rest/api/storageservices/copy-blob-from-url",
                  "text": "Copy Blob from URL"
                },
                {
                  "url": "/en-us/rest/api/storageservices/set-blob-tier",
                  "text": "Set Blob Tier"
                },
                {
                  "url": "/en-us/rest/api/storageservices/set-blob-tier",
                  "text": "Set Blob Tier"
                },
                {
                  "url": "/en-us/rest/api/storageservices/get-blob-properties",
                  "text": "Get Blob Properties"
                },
                {
                  "url": "/en-us/rest/api/storageservices/copy-blob",
                  "text": "Copy Blob"
                },
                {
                  "url": "/en-us/azure/storage/blobs/archive-rehydrate-to-online-tier#rehydrate-a-blob-by-changing-its-tier",
                  "text": "Rehydrate a blob by changing its tier"
                }
              ]
            },
            {
              "number": 6,
              "title": "Module assessment",
              "url": "https://learn.microsoft.com/en-us/training/modules/manage-azure-blob-storage-lifecycle/6-knowledge-check",
              "href": "6-knowledge-check",
              "content": "Read in English\nAdd\nAdd to plan\nModule assessment\nCompleted\n3 minutes\nCheck your knowledge\n1.\nWhich access tier is considered to be offline and can't be read or modified?\nCool\nArchive\nHot\n2.\nWhich of the following storage account types supports lifecycle policies?\nGeneral Purpose v1\nGeneral Purpose v2\nFileStorage\nYou must answer all questions before checking your work.\nYou must answer all questions before checking your work.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Module assessment"
                },
                {
                  "level": 2,
                  "text": "Check your knowledge"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 7,
              "title": "Summary",
              "url": "https://learn.microsoft.com/en-us/training/modules/manage-azure-blob-storage-lifecycle/7-summary",
              "href": "7-summary",
              "content": "Read in English\nAdd\nAdd to plan\nSummary\nCompleted\n3 minutes\nIn this module you learned how to:\nDescribe how each of the access tiers is optimized.\nCreate and implement a lifecycle policy.\nRehydrate blob data stored in an archive tier.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Summary"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            }
          ]
        },
        {
          "title": "Work with Azure Blob storage",
          "url": "https://learn.microsoft.com/en-us/training/modules/work-azure-blob-storage/",
          "description": "",
          "learning_objectives": [],
          "prerequisites": [],
          "units": [
            {
              "number": 1,
              "title": "Introduction",
              "url": "https://learn.microsoft.com/en-us/training/modules/work-azure-blob-storage/1-introduction",
              "href": "1-introduction",
              "content": "Read in English\nAdd\nAdd to plan\nIntroduction\nCompleted\n3 minutes\nThe Azure Storage client libraries for .NET offer a convenient interface for making calls to Azure Storage.\nAfter completing this module, you'll be able to:\nCreate an application to create and manipulate data by using the Azure Storage client library for Blob storage.\nManage container properties and metadata by using .NET and REST.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Introduction"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 2,
              "title": "Explore Azure Blob storage client library",
              "url": "https://learn.microsoft.com/en-us/training/modules/work-azure-blob-storage/2-blob-storage-client-library-overview",
              "href": "2-blob-storage-client-library-overview",
              "content": "Read in English\nAdd\nAdd to plan\nExplore Azure Blob storage client library\nCompleted\n3 minutes\nThe Azure Storage client libraries for .NET offer a convenient interface for making calls to Azure Storage. The latest version of the Azure Storage client library is version 12.x. Microsoft recommends using version 12.x for new applications.\nThe following table lists the basic classes, along with a brief description:\nClass\nDescription\nBlobClient\nThe\nBlobClient\nallows you to manipulate Azure Storage blobs.\nBlobClientOptions\nProvides the client configuration options for connecting to Azure Blob Storage.\nBlobContainerClient\nThe\nBlobContainerClient\nallows you to manipulate Azure Storage containers and their blobs.\nBlobServiceClient\nThe\nBlobServiceClient\nallows you to manipulate Azure Storage service resources and blob containers. The storage account provides the top-level namespace for the Blob service.\nBlobUriBuilder\nThe\nBlobUriBuilder\nclass provides a convenient way to modify the contents of a Uri instance to point to different Azure Storage resources like an account, container, or blob.\nThe following packages contain the classes used to work with Blob Storage data resources:\nAzure.Storage.Blobs\n: Contains the primary classes (client objects) that you can use to operate on the service, containers, and blobs.\nAzure.Storage.Blobs.Specialized\n: Contains classes that you can use to perform operations specific to a blob type, such as block blobs.\nAzure.Storage.Blobs.Models\n: All other utility classes, structures, and enumeration types.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Explore Azure Blob storage client library"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [
                "BlobClientOptions",
                "BlobContainerClient",
                "BlobServiceClient",
                "BlobUriBuilder"
              ],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                },
                {
                  "url": "/en-us/dotnet/api/azure.storage.blobs.blobclient",
                  "text": "BlobClient"
                },
                {
                  "url": "/en-us/dotnet/api/azure.storage.blobs.blobcontainerclient",
                  "text": "BlobContainerClient"
                },
                {
                  "url": "/en-us/dotnet/api/azure.storage.blobs.blobserviceclient",
                  "text": "BlobServiceClient"
                },
                {
                  "url": "/en-us/dotnet/api/azure.storage.blobs.bloburibuilder",
                  "text": "BlobUriBuilder"
                },
                {
                  "url": "/en-us/dotnet/api/azure.storage.blobs",
                  "text": "Azure.Storage.Blobs"
                },
                {
                  "url": "/en-us/dotnet/api/azure.storage.blobs.specialized",
                  "text": "Azure.Storage.Blobs.Specialized"
                },
                {
                  "url": "/en-us/dotnet/api/azure.storage.blobs.models",
                  "text": "Azure.Storage.Blobs.Models"
                }
              ]
            },
            {
              "number": 3,
              "title": "Create a client object",
              "url": "https://learn.microsoft.com/en-us/training/modules/work-azure-blob-storage/3-create-client-object",
              "href": "3-create-client-object",
              "content": "Read in English\nAdd\nAdd to plan\nCreate a client object\nCompleted\n5 minutes\nWorking with any Azure resource using the SDK begins with creating a client object. In this section, you learn how to create client objects to interact with the three types of resources in the storage service: storage accounts, containers, and blobs.\nWhen your application creates a client object, you pass a URI referencing the endpoint to the client constructor. You can construct the endpoint string manually, as shown in the examples in this article, or you can query for the endpoint at runtime using the Azure Storage management library.\nThe code samples in this unit use\nDefaultAzureCredential\nto authenticate to Azure via a Microsoft Entra security principal. The authentication process includes obtaining an access token for authorization. This access token is passed as a credential when the client is instantiated, and the credential persists throughout the client lifetime. The Microsoft Entra security principal requesting the token must be assigned an appropriate Azure RBAC role that grants access to blob data.\nCreate a BlobServiceClient object\nAn authorized\nBlobServiceClient\nobject allows your app to interact with resources at the storage account level.\nBlobServiceClient\nprovides methods to retrieve and configure account properties, as well as list, create, and delete containers within the storage account. This client object is the starting point for interacting with resources in the storage account.\nThe following example shows how to create a\nBlobServiceClient\nobject:\nusing Azure.Identity;\nusing Azure.Storage.Blobs;\n\npublic BlobServiceClient GetBlobServiceClient(string accountName)\n{\n    BlobServiceClient client = new(\n        new Uri($\"https://{accountName}.blob.core.windows.net\"),\n        new DefaultAzureCredential());\n\n    return client;\n}\nCreate a BlobContainerClient object\nYou can use a\nBlobServiceClient\nobject to create a new\nBlobContainerClient\nobject. A\nBlobContainerClient\nobject allows you to interact with a specific container resource.\nBlobContainerClient\nprovides methods to create, delete, or configure a container, and includes methods to list, upload, and delete the blobs within it.\nThe following example shows how to create a container client from a\nBlobServiceClient\nobject to interact with a specific container resource:\npublic BlobContainerClient GetBlobContainerClient(\n    BlobServiceClient blobServiceClient,\n    string containerName)\n{\n    // Create the container client using the service client object\n    BlobContainerClient client = blobServiceClient.GetBlobContainerClient(containerName);\n    return client;\n}\nIf your work is narrowly scoped to a single container, you might choose to create a\nBlobContainerClient\nobject directly without using\nBlobServiceClient\n.\npublic BlobContainerClient GetBlobContainerClient(\n    string accountName,\n    string containerName,\n    BlobClientOptions clientOptions)\n{\n    // Append the container name to the end of the URI\n    BlobContainerClient client = new(\n        new Uri($\"https://{accountName}.blob.core.windows.net/{containerName}\"),\n        new DefaultAzureCredential(),\n        clientOptions);\n\n    return client;\n}\nCreate a BlobClient object\nTo interact with a specific blob resource, create a\nBlobClient\nobject from a service client or container client. A\nBlobClient\nobject allows you to interact with a specific blob resource.\nThe following example shows how to create a blob client to interact with a specific blob resource:\npublic BlobClient GetBlobClient(\n    BlobServiceClient blobServiceClient,\n    string containerName,\n    string blobName)\n{\n    BlobClient client =\n        blobServiceClient.GetBlobContainerClient(containerName).GetBlobClient(blobName);\n    return client;\n}\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Create a client object"
                },
                {
                  "level": 2,
                  "text": "Create a BlobServiceClient object"
                },
                {
                  "level": 2,
                  "text": "Create a BlobContainerClient object"
                },
                {
                  "level": 2,
                  "text": "Create a BlobClient object"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [
                "BlobServiceClient",
                "BlobServiceClient",
                "BlobServiceClient",
                "using Azure.Identity;\nusing Azure.Storage.Blobs;\n\npublic BlobServiceClient GetBlobServiceClient(string accountName)\n{\n    BlobServiceClient client = new(\n        new Uri($\"https://{accountName}.blob.core.windows.net\"),\n        new DefaultAzureCredential());\n\n    return client;\n}",
                "using Azure.Identity;\nusing Azure.Storage.Blobs;\n\npublic BlobServiceClient GetBlobServiceClient(string accountName)\n{\n    BlobServiceClient client = new(\n        new Uri($\"https://{accountName}.blob.core.windows.net\"),\n        new DefaultAzureCredential());\n\n    return client;\n}",
                "BlobServiceClient",
                "BlobContainerClient",
                "BlobContainerClient",
                "BlobContainerClient",
                "BlobServiceClient",
                "public BlobContainerClient GetBlobContainerClient(\n    BlobServiceClient blobServiceClient,\n    string containerName)\n{\n    // Create the container client using the service client object\n    BlobContainerClient client = blobServiceClient.GetBlobContainerClient(containerName);\n    return client;\n}",
                "public BlobContainerClient GetBlobContainerClient(\n    BlobServiceClient blobServiceClient,\n    string containerName)\n{\n    // Create the container client using the service client object\n    BlobContainerClient client = blobServiceClient.GetBlobContainerClient(containerName);\n    return client;\n}",
                "BlobContainerClient",
                "BlobServiceClient",
                "public BlobContainerClient GetBlobContainerClient(\n    string accountName,\n    string containerName,\n    BlobClientOptions clientOptions)\n{\n    // Append the container name to the end of the URI\n    BlobContainerClient client = new(\n        new Uri($\"https://{accountName}.blob.core.windows.net/{containerName}\"),\n        new DefaultAzureCredential(),\n        clientOptions);\n\n    return client;\n}",
                "public BlobContainerClient GetBlobContainerClient(\n    string accountName,\n    string containerName,\n    BlobClientOptions clientOptions)\n{\n    // Append the container name to the end of the URI\n    BlobContainerClient client = new(\n        new Uri($\"https://{accountName}.blob.core.windows.net/{containerName}\"),\n        new DefaultAzureCredential(),\n        clientOptions);\n\n    return client;\n}",
                "public BlobClient GetBlobClient(\n    BlobServiceClient blobServiceClient,\n    string containerName,\n    string blobName)\n{\n    BlobClient client =\n        blobServiceClient.GetBlobContainerClient(containerName).GetBlobClient(blobName);\n    return client;\n}",
                "public BlobClient GetBlobClient(\n    BlobServiceClient blobServiceClient,\n    string containerName,\n    string blobName)\n{\n    BlobClient client =\n        blobServiceClient.GetBlobContainerClient(containerName).GetBlobClient(blobName);\n    return client;\n}"
              ],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                },
                {
                  "url": "/en-us/dotnet/api/azure.identity.defaultazurecredential",
                  "text": "DefaultAzureCredential"
                }
              ]
            },
            {
              "number": 4,
              "title": "Exercise - Create Blob storage resources by using the .NET client library",
              "url": "https://learn.microsoft.com/en-us/training/modules/work-azure-blob-storage/4-develop-blob-storage-dotnet",
              "href": "4-develop-blob-storage-dotnet",
              "content": "Read in English\nAdd\nAdd to plan\nExercise - Create Blob storage resources by using the .NET client library\nCompleted\n30 minutes\nIn this exercise, you create an Azure Storage account and build a .NET console application using the Azure Storage Blob client library to create containers, upload files to blob storage, list blobs, and download files. You learn how to authenticate with Azure, perform blob storage operations programmatically, and verify results in the Azure portal.\nTasks performed in this exercise:\nPrepare the Azure resources\nCreate a console app to create and download data\nRun the app and verify results\nClean up resources\nThis exercise takes approximately\n30\nminutes to complete.\nBefore you start\nTo complete the exercise, you need:\nAn Azure subscription. If you don't already have one, you can sign up for one\nhttps://azure.microsoft.com/\n.\nGet started\nSelect the\nLaunch Exercise\nbutton to open the exercise instructions in a new browser window. When you're finished with the exercise, return here to:\nComplete the module\nEarn a badge for completing this module\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Exercise - Create Blob storage resources by using the .NET client library"
                },
                {
                  "level": 2,
                  "text": "Before you start"
                },
                {
                  "level": 2,
                  "text": "Get started"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [
                {
                  "src": "../../wwl-azure/work-azure-blob-storage/media/launch-exercise.png",
                  "absolute_url": "https://learn.microsoft.com/en-us/wwl-azure/work-azure-blob-storage/media/launch-exercise.png",
                  "alt_text": "Button to launch exercise.",
                  "title": "",
                  "filename": "launch-exercise.png",
                  "image_type": "icon",
                  "context": {
                    "preceding_heading": "Get started",
                    "following_text": "Was this page helpful?",
                    "figure_caption": "",
                    "parent_section": ""
                  }
                }
              ],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                },
                {
                  "url": "https://azure.microsoft.com/",
                  "text": "https://azure.microsoft.com/"
                }
              ]
            },
            {
              "number": 5,
              "title": "Manage container properties and metadata by using .NET",
              "url": "https://learn.microsoft.com/en-us/training/modules/work-azure-blob-storage/5-manage-container-properties-metadata-dotnet",
              "href": "5-manage-container-properties-metadata-dotnet",
              "content": "Read in English\nAdd\nAdd to plan\nManage container properties and metadata by using .NET\nCompleted\n3 minutes\nBlob containers support system properties and user-defined metadata, in addition to the data they contain.\nSystem properties\n: System properties exist on each Blob storage resource. Some of them can be read or set, while others are read-only. Under the covers, some system properties correspond to certain standard HTTP headers. The Azure Storage client library for .NET maintains these properties for you.\nUser-defined metadata\n: User-defined metadata consists of one or more name-value pairs that you specify for a Blob storage resource. You can use metadata to store other values with the resource. Metadata values are for your own purposes only, and don't affect how the resource behaves.\nMetadata name/value pairs are valid HTTP headers, and so should adhere to all restrictions governing HTTP headers. Metadata names must be valid HTTP header names and valid C# identifiers, can contain only ASCII characters, and should be treated as case-insensitive. Metadata values containing non-ASCII characters should be Base64-encoded or URL-encoded.\nRetrieve container properties\nTo retrieve container properties, call one of the following methods of the\nBlobContainerClient\nclass:\nGetProperties\nGetPropertiesAsync\nThe following code example fetches a container's system properties and writes some property values to a console window:\nprivate static async Task ReadContainerPropertiesAsync(BlobContainerClient container)\n{\n    try\n    {\n        // Fetch some container properties and write out their values.\n        var properties = await container.GetPropertiesAsync();\n        Console.WriteLine($\"Properties for container {container.Uri}\");\n        Console.WriteLine($\"Public access level: {properties.Value.PublicAccess}\");\n        Console.WriteLine($\"Last modified time in UTC: {properties.Value.LastModified}\");\n    }\n    catch (RequestFailedException e)\n    {\n        Console.WriteLine($\"HTTP error code {e.Status}: {e.ErrorCode}\");\n        Console.WriteLine(e.Message);\n        Console.ReadLine();\n    }\n}\nSet and retrieve metadata\nYou can specify metadata as one or more name-value pairs on a blob or container resource. To set metadata, add name-value pairs to an\nIDictionary\nobject, and then call one of the following methods of the\nBlobContainerClient\nclass to write the values:\nSetMetadata\nSetMetadataAsync\nThe name of your metadata must conform to the naming conventions for C# identifiers. Metadata names preserve the case with which they were created, but are case-insensitive when set or read. If two or more metadata headers with the same name are submitted for a resource, the Blob service returns status code\n400 (Bad Request)\n.\nThe following code example sets metadata on a container.\npublic static async Task AddContainerMetadataAsync(BlobContainerClient container)\n{\n    try\n    {\n        IDictionary<string, string> metadata =\n           new Dictionary<string, string>();\n\n        // Add some metadata to the container.\n        metadata.Add(\"docType\", \"textDocuments\");\n        metadata.Add(\"category\", \"guidance\");\n\n        // Set the container's metadata.\n        await container.SetMetadataAsync(metadata);\n    }\n    catch (RequestFailedException e)\n    {\n        Console.WriteLine($\"HTTP error code {e.Status}: {e.ErrorCode}\");\n        Console.WriteLine(e.Message);\n        Console.ReadLine();\n    }\n}\nThe\nGetProperties\nand\nGetPropertiesAsync\nmethods are used to retrieve metadata in addition to properties as shown earlier.\nThe following code example retrieves metadata from a container.\npublic static async Task ReadContainerMetadataAsync(BlobContainerClient container)\n{\n    try\n    {\n        var properties = await container.GetPropertiesAsync();\n\n        // Enumerate the container's metadata.\n        Console.WriteLine(\"Container metadata:\");\n        foreach (var metadataItem in properties.Value.Metadata)\n        {\n            Console.WriteLine($\"\\tKey: {metadataItem.Key}\");\n            Console.WriteLine($\"\\tValue: {metadataItem.Value}\");\n        }\n    }\n    catch (RequestFailedException e)\n    {\n        Console.WriteLine($\"HTTP error code {e.Status}: {e.ErrorCode}\");\n        Console.WriteLine(e.Message);\n        Console.ReadLine();\n    }\n}\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Manage container properties and metadata by using .NET"
                },
                {
                  "level": 2,
                  "text": "Retrieve container properties"
                },
                {
                  "level": 2,
                  "text": "Set and retrieve metadata"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [
                "BlobContainerClient",
                "GetProperties",
                "GetPropertiesAsync",
                "private static async Task ReadContainerPropertiesAsync(BlobContainerClient container)\n{\n    try\n    {\n        // Fetch some container properties and write out their values.\n        var properties = await container.GetPropertiesAsync();\n        Console.WriteLine($\"Properties for container {container.Uri}\");\n        Console.WriteLine($\"Public access level: {properties.Value.PublicAccess}\");\n        Console.WriteLine($\"Last modified time in UTC: {properties.Value.LastModified}\");\n    }\n    catch (RequestFailedException e)\n    {\n        Console.WriteLine($\"HTTP error code {e.Status}: {e.ErrorCode}\");\n        Console.WriteLine(e.Message);\n        Console.ReadLine();\n    }\n}",
                "private static async Task ReadContainerPropertiesAsync(BlobContainerClient container)\n{\n    try\n    {\n        // Fetch some container properties and write out their values.\n        var properties = await container.GetPropertiesAsync();\n        Console.WriteLine($\"Properties for container {container.Uri}\");\n        Console.WriteLine($\"Public access level: {properties.Value.PublicAccess}\");\n        Console.WriteLine($\"Last modified time in UTC: {properties.Value.LastModified}\");\n    }\n    catch (RequestFailedException e)\n    {\n        Console.WriteLine($\"HTTP error code {e.Status}: {e.ErrorCode}\");\n        Console.WriteLine(e.Message);\n        Console.ReadLine();\n    }\n}",
                "IDictionary",
                "BlobContainerClient",
                "SetMetadata",
                "SetMetadataAsync",
                "400 (Bad Request)",
                "public static async Task AddContainerMetadataAsync(BlobContainerClient container)\n{\n    try\n    {\n        IDictionary<string, string> metadata =\n           new Dictionary<string, string>();\n\n        // Add some metadata to the container.\n        metadata.Add(\"docType\", \"textDocuments\");\n        metadata.Add(\"category\", \"guidance\");\n\n        // Set the container's metadata.\n        await container.SetMetadataAsync(metadata);\n    }\n    catch (RequestFailedException e)\n    {\n        Console.WriteLine($\"HTTP error code {e.Status}: {e.ErrorCode}\");\n        Console.WriteLine(e.Message);\n        Console.ReadLine();\n    }\n}",
                "public static async Task AddContainerMetadataAsync(BlobContainerClient container)\n{\n    try\n    {\n        IDictionary<string, string> metadata =\n           new Dictionary<string, string>();\n\n        // Add some metadata to the container.\n        metadata.Add(\"docType\", \"textDocuments\");\n        metadata.Add(\"category\", \"guidance\");\n\n        // Set the container's metadata.\n        await container.SetMetadataAsync(metadata);\n    }\n    catch (RequestFailedException e)\n    {\n        Console.WriteLine($\"HTTP error code {e.Status}: {e.ErrorCode}\");\n        Console.WriteLine(e.Message);\n        Console.ReadLine();\n    }\n}",
                "GetProperties",
                "GetPropertiesAsync",
                "public static async Task ReadContainerMetadataAsync(BlobContainerClient container)\n{\n    try\n    {\n        var properties = await container.GetPropertiesAsync();\n\n        // Enumerate the container's metadata.\n        Console.WriteLine(\"Container metadata:\");\n        foreach (var metadataItem in properties.Value.Metadata)\n        {\n            Console.WriteLine($\"\\tKey: {metadataItem.Key}\");\n            Console.WriteLine($\"\\tValue: {metadataItem.Value}\");\n        }\n    }\n    catch (RequestFailedException e)\n    {\n        Console.WriteLine($\"HTTP error code {e.Status}: {e.ErrorCode}\");\n        Console.WriteLine(e.Message);\n        Console.ReadLine();\n    }\n}",
                "public static async Task ReadContainerMetadataAsync(BlobContainerClient container)\n{\n    try\n    {\n        var properties = await container.GetPropertiesAsync();\n\n        // Enumerate the container's metadata.\n        Console.WriteLine(\"Container metadata:\");\n        foreach (var metadataItem in properties.Value.Metadata)\n        {\n            Console.WriteLine($\"\\tKey: {metadataItem.Key}\");\n            Console.WriteLine($\"\\tValue: {metadataItem.Value}\");\n        }\n    }\n    catch (RequestFailedException e)\n    {\n        Console.WriteLine($\"HTTP error code {e.Status}: {e.ErrorCode}\");\n        Console.WriteLine(e.Message);\n        Console.ReadLine();\n    }\n}"
              ],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 6,
              "title": "Set and retrieve properties and metadata for blob resources by using REST",
              "url": "https://learn.microsoft.com/en-us/training/modules/work-azure-blob-storage/6-set-retrieve-properties-metadata-rest",
              "href": "6-set-retrieve-properties-metadata-rest",
              "content": "Read in English\nAdd\nAdd to plan\nSet and retrieve properties and metadata for blob resources by using REST\nCompleted\n3 minutes\nContainers and blobs support custom metadata, represented as HTTP headers. Metadata headers can be set on a request that creates a new container or blob resource, or on a request that explicitly creates a property on an existing resource.\nMetadata header format\nMetadata headers are name/value pairs. The format for the header is:\nx-ms-meta-name:string-value\nBeginning with version 2009-09-19, metadata names must adhere to the naming rules for C# identifiers.\nNames are case-insensitive. Metadata names preserve the case with which they were created, but are case-insensitive when set or read. If two or more metadata headers with the same name are submitted for a resource, the Blob service returns status code\n400 (Bad Request)\n.\nThe metadata consists of name/value pairs. The total size of all metadata pairs can be up to 8 KB in size.\nMetadata name/value pairs are valid HTTP headers, and so they adhere to all restrictions governing HTTP headers.\nOperations on metadata\nMetadata on a blob or container resource can be retrieved or set directly, without returning or altering the content of the resource.\nMetadata values can only be read or written in full; partial updates aren't supported. Setting metadata on a resource overwrites any existing metadata values for that resource.\nRetrieving properties and metadata\nThe GET/HEAD operation retrieves metadata headers for the specified container or blob. These operations return headers only; they don't return a response body. The URI syntax for retrieving metadata headers on a container is as follows:\nGET/HEAD https://myaccount.blob.core.windows.net/mycontainer?restype=container\nThe URI syntax for retrieving metadata headers on a blob is as follows:\nGET/HEAD https://myaccount.blob.core.windows.net/mycontainer/myblob?comp=metadata\nSetting Metadata Headers\nThe PUT operation sets metadata headers on the specified container or blob, overwriting any existing metadata on the resource. Calling PUT without any headers on the request clears all existing metadata on the resource.\nThe URI syntax for setting metadata headers on a container is as follows:\nPUT https://myaccount.blob.core.windows.net/mycontainer?comp=metadata&restype=container\nThe URI syntax for setting metadata headers on a blob is as follows:\nPUT https://myaccount.blob.core.windows.net/mycontainer/myblob?comp=metadata\nStandard HTTP properties for containers and blobs\nContainers and blobs also support certain standard HTTP properties. Properties and metadata are both represented as standard HTTP headers; the difference between them is in the naming of the headers. Metadata headers are named with the header prefix\nx-ms-meta-\nand a custom name. Property headers use standard HTTP header names, as specified in the Header Field Definitions section 14 of the HTTP/1.1 protocol specification.\nThe standard HTTP headers supported on containers include:\nETag\nLast-Modified\nThe standard HTTP headers supported on blobs include:\nETag\nLast-Modified\nContent-Length\nContent-Type\nContent-MD5\nContent-Encoding\nContent-Language\nCache-Control\nOrigin\nRange\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Set and retrieve properties and metadata for blob resources by using REST"
                },
                {
                  "level": 2,
                  "text": "Metadata header format"
                },
                {
                  "level": 2,
                  "text": "Operations on metadata"
                },
                {
                  "level": 3,
                  "text": "Retrieving properties and metadata"
                },
                {
                  "level": 3,
                  "text": "Setting Metadata Headers"
                },
                {
                  "level": 2,
                  "text": "Standard HTTP properties for containers and blobs"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [
                "x-ms-meta-name:string-value",
                "x-ms-meta-name:string-value",
                "400 (Bad Request)",
                "GET/HEAD https://myaccount.blob.core.windows.net/mycontainer?restype=container",
                "GET/HEAD https://myaccount.blob.core.windows.net/mycontainer?restype=container",
                "GET/HEAD https://myaccount.blob.core.windows.net/mycontainer/myblob?comp=metadata",
                "GET/HEAD https://myaccount.blob.core.windows.net/mycontainer/myblob?comp=metadata",
                "PUT https://myaccount.blob.core.windows.net/mycontainer?comp=metadata&restype=container",
                "PUT https://myaccount.blob.core.windows.net/mycontainer?comp=metadata&restype=container",
                "PUT https://myaccount.blob.core.windows.net/mycontainer/myblob?comp=metadata",
                "PUT https://myaccount.blob.core.windows.net/mycontainer/myblob?comp=metadata",
                "Last-Modified",
                "Last-Modified",
                "Content-Length",
                "Content-Type",
                "Content-MD5",
                "Content-Encoding",
                "Content-Language",
                "Cache-Control"
              ],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 7,
              "title": "Module assessment",
              "url": "https://learn.microsoft.com/en-us/training/modules/work-azure-blob-storage/7-knowledge-check",
              "href": "7-knowledge-check",
              "content": "Read in English\nAdd\nAdd to plan\nModule assessment\nCompleted\n3 minutes\nCheck your knowledge\n1.\nWhich of the following standard HTTP headers are supported for both containers and blobs when setting properties by using REST?\nLast-Modified\nContent-Length\nOrigin\n2.\nWhich of the following classes of the Azure Storage client library for .NET allows you to manipulate both Azure Storage containers and their blobs?\nBlobClient\nBlobContainerClient\nBlobUriBuilder\nYou must answer all questions before checking your work.\nYou must answer all questions before checking your work.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Module assessment"
                },
                {
                  "level": 2,
                  "text": "Check your knowledge"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 8,
              "title": "Summary",
              "url": "https://learn.microsoft.com/en-us/training/modules/work-azure-blob-storage/8-summary",
              "href": "8-summary",
              "content": "Read in English\nAdd\nAdd to plan\nSummary\nCompleted\n3 minutes\nIn this module you learned how to:\nCreate an application to create and manipulate data by using the Azure Storage client library for Blob storage.\nManage container properties and metadata by using .NET and REST.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Summary"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "title": "Develop solutions that use Azure Cosmos DB",
      "url": "https://learn.microsoft.com/en-us/training/paths/az-204-develop-solutions-that-use-azure-cosmos-db/",
      "learn_uid": "learn.wwl.az-204-develop-solutions-that-use-azure-cosmos-db",
      "modules": [
        {
          "title": "Explore Azure Cosmos DB",
          "url": "https://learn.microsoft.com/en-us/training/modules/explore-azure-cosmos-db/",
          "description": "",
          "learning_objectives": [],
          "prerequisites": [],
          "units": [
            {
              "number": 1,
              "title": "Introduction",
              "url": "https://learn.microsoft.com/en-us/training/modules/explore-azure-cosmos-db/1-introduction",
              "href": "1-introduction",
              "content": "Read in English\nAdd\nAdd to plan\nIntroduction\nCompleted\n3 minutes\nAzure Cosmos DB is a globally distributed database system that allows you to read and write data from the local replicas of your database and it transparently replicates the data to all the regions associated with your Cosmos account.\nAfter completing this module, you'll be able to:\nIdentify the key benefits provided by Azure Cosmos DB\nDescribe the elements in an Azure Cosmos DB account and how they're organized\nExplain the different consistency levels and choose the correct one for your project\nExplore the APIs supported in Azure Cosmos DB and choose the appropriate API for your solution\nDescribe how request units impact costs\nCreate Azure Cosmos DB resources by using the Azure portal.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Introduction"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 2,
              "title": "Identify key benefits of Azure Cosmos DB",
              "url": "https://learn.microsoft.com/en-us/training/modules/explore-azure-cosmos-db/2-cosmos-db-benefits",
              "href": "2-cosmos-db-benefits",
              "content": "Read in English\nAdd\nAdd to plan\nIdentify key benefits of Azure Cosmos DB\nCompleted\n3 minutes\nAzure\nCosmos\nDB is a fully managed NoSQL database designed to provide low latency, elastic scalability of throughput, well-defined semantics for data consistency, and high availability.\nYou can configure your databases to be globally distributed and available in any of the Azure regions. To lower the latency, place the data close to where your users are. Choosing the required regions depends on the global reach of your application and where your users are located.\nWith Azure Cosmos DB, you can add or remove the regions associated with your account at any time. Your application doesn't need to be paused or redeployed to add or remove a region.\nKey benefits of global distribution\nWith its novel multi-master replication protocol, every region supports both writes and reads. The multi-master capability also enables:\nUnlimited elastic write and read scalability.\n99.999% read and write availability all around the world.\nGuaranteed reads and writes served in less than 10 milliseconds at the 99th percentile.\nYour application can perform near real-time reads and writes against all the regions you chose for your database. Azure Cosmos DB internally handles the data replication between regions with consistency level guarantees of the level you selected.\nRunning a database in multiple regions worldwide increases the availability of a database. If one region is unavailable, other regions automatically handle application requests. Azure Cosmos DB offers 99.999% read and write availability for multi-region databases.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Identify key benefits of Azure Cosmos DB"
                },
                {
                  "level": 2,
                  "text": "Key benefits of global distribution"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 3,
              "title": "Explore the resource hierarchy",
              "url": "https://learn.microsoft.com/en-us/training/modules/explore-azure-cosmos-db/3-cosmos-db-resource-hierarchy",
              "href": "3-cosmos-db-resource-hierarchy",
              "content": "Read in English\nAdd\nAdd to plan\nExplore the resource hierarchy\nCompleted\n3 minutes\nThe Azure Cosmos DB account is the fundamental unit of global distribution and high availability. Your Azure Cosmos DB account contains a unique Domain Name System (DNS) name and you can manage an account by using the Azure portal or the Azure CLI, or by using different language-specific SDKs. For globally distributing your data and throughput across multiple Azure regions, you can add and remove Azure regions to your account at any time.\nElements in an Azure Cosmos DB account\nAn Azure Cosmos DB container is the fundamental unit of scalability. You can virtually have an unlimited provisioned throughput (RU/s) and storage on a container. Azure Cosmos DB transparently partitions your container using the logical partition key that you specify in order to elastically scale your provisioned throughput and storage.\nCurrently, you can create a maximum of 50 Azure Cosmos DB accounts under an Azure subscription (can be increased via support request). After you create an account under your Azure subscription, you can manage the data in your account by creating databases, containers, and items.\nThe following image shows the hierarchy of different entities in an Azure Cosmos DB account:\nAzure Cosmos DB databases\nYou can create one or multiple Azure Cosmos DB databases under your account. A database is analogous to a namespace. A database is the unit of management for a set of Azure Cosmos DB containers.\nAzure Cosmos DB containers\nAn Azure Cosmos DB container is where data is stored. Unlike most relational databases, which scale up with larger sizes of virtual machines, Azure Cosmos DB scales out.\nData is stored on one or more servers called\npartitions\n. To increase partitions, you increase throughput, or they grow automatically as storage increases. This relationship provides a virtually unlimited amount of throughput and storage for a container.\nWhen you create a container, you need to supply a partition key. The partition key is a property that you select from your items to help Azure Cosmos DB distribute the data efficiently across partitions. Azure Cosmos DB uses the value of this property to route data to the appropriate partition to be written, updated, or deleted. You can also use the partition key in the\nWHERE\nclause in queries for efficient data retrieval.\nThe underlying storage mechanism for data in Azure Cosmos DB is called a\nphysical partition\n. Physical partitions can have a throughput amount up to 10,000 Request Units per second, and they can store up to 50 GB of data. Azure Cosmos DB abstracts this partitioning concept with a logical partition, which can store up to 20 GB of data.\nWhen you create a container, you configure throughput in one of the following modes:\nDedicated throughput\n: The throughput on a container is exclusively reserved for that container. There are two types of dedicated throughput: standard and autoscale.\nShared throughput\n: Throughput is specified at the database level and then shared with up to 25 containers within the database. Sharing of throughput excludes containers that are configured with their own dedicated throughput.\nAzure Cosmos DB items\nDepending on which API you use, individual data entities can be represented in various ways:\nAzure Cosmos DB entity\nAPI for NoSQL\nAPI for Cassandra\nAPI for MongoDB\nAPI for Gremlin\nAPI for Table\nAzure Cosmos DB item\nItem\nRow\nDocument\nNode or edge\nItem\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Explore the resource hierarchy"
                },
                {
                  "level": 2,
                  "text": "Elements in an Azure Cosmos DB account"
                },
                {
                  "level": 2,
                  "text": "Azure Cosmos DB databases"
                },
                {
                  "level": 2,
                  "text": "Azure Cosmos DB containers"
                },
                {
                  "level": 2,
                  "text": "Azure Cosmos DB items"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [
                {
                  "src": "../../wwl-azure/explore-azure-cosmos-db/media/cosmos-entities.png",
                  "absolute_url": "https://learn.microsoft.com/en-us/wwl-azure/explore-azure-cosmos-db/media/cosmos-entities.png",
                  "alt_text": "Image showing the hierarchy of Azure Cosmos DB entities: Database accounts are at the top, databases are grouped under accounts, and containers are grouped under databases.",
                  "title": "",
                  "filename": "cosmos-entities.png",
                  "image_type": "diagram",
                  "context": {
                    "preceding_heading": "Elements in an Azure Cosmos DB account",
                    "following_text": "You can create one or multiple Azure Cosmos DB databases under your account. A database is analogous to a namespace. A database is the unit of management for a set of Azure Cosmos DB containers.",
                    "figure_caption": "",
                    "parent_section": ""
                  }
                }
              ],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 4,
              "title": "Explore consistency levels",
              "url": "https://learn.microsoft.com/en-us/training/modules/explore-azure-cosmos-db/4-cosmos-db-consistency-levels-overview",
              "href": "4-cosmos-db-consistency-levels-overview",
              "content": "Read in English\nAdd\nAdd to plan\nExplore consistency levels\nCompleted\n3 minutes\nAzure Cosmos DB approaches data consistency as a spectrum of choices instead of two extremes. Strong consistency and eventual consistency are at the ends of the spectrum, but there are many consistency choices along the spectrum. Developers can use these options to make precise choices and granular tradeoffs with respect to high availability and performance.\nAzure Cosmos DB offers five well-defined levels. From strongest to weakest, the levels are:\nStrong\nBounded staleness\nSession\nConsistent prefix\nEventual\nEach level provides availability and performance tradeoffs. The following image shows the different consistency levels as a spectrum.\nThe consistency levels are region-agnostic and are guaranteed for all operations, regardless of:\nThe region where the reads and writes are served\nThe number of regions associated with your Azure Cosmos DB account\nWhether your account is configured with a single or multiple write regions.\nRead consistency applies to a single read operation scoped within a partition-key range or a logical partition.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Explore consistency levels"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [
                {
                  "src": "../../wwl-azure/explore-azure-cosmos-db/media/five-consistency-levels.png",
                  "absolute_url": "https://learn.microsoft.com/en-us/wwl-azure/explore-azure-cosmos-db/media/five-consistency-levels.png",
                  "alt_text": "Image showing data consistency as a spectrum.",
                  "title": "",
                  "filename": "five-consistency-levels.png",
                  "image_type": "illustration",
                  "context": {
                    "preceding_heading": "Explore consistency levels",
                    "following_text": "The consistency levels are region-agnostic and are guaranteed for all operations, regardless of:",
                    "figure_caption": "",
                    "parent_section": ""
                  }
                }
              ],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 5,
              "title": "Choose the right consistency level",
              "url": "https://learn.microsoft.com/en-us/training/modules/explore-azure-cosmos-db/5-choose-cosmos-db-consistency-level",
              "href": "5-choose-cosmos-db-consistency-level",
              "content": "Read in English\nAdd\nAdd to plan\nChoose the right consistency level\nCompleted\n3 minutes\nEach of the consistency models can be used for specific real-world scenarios. Each provides precise availability and performance tradeoffs backed by comprehensive SLAs. The following simple considerations help you make the right choice in many common scenarios.\nConfigure the default consistency level\nYou can configure the default consistency level on your Azure Cosmos DB account at any time. The default consistency level configured on your account applies to all Azure Cosmos DB databases and containers under that account. All reads and queries issued against a container or a database use the specified consistency level by default.\nRead consistency applies to a single read operation scoped within a logical partition. The read operation can be issued by a remote client or a stored procedure.\nGuarantees associated with consistency levels\nAzure Cosmos DB guarantees that 100 percent of read requests meet the consistency guarantee for the consistency level chosen. The precise definitions of the five consistency levels in Azure Cosmos DB using the TLA+ specification language are provided in the\nazure-cosmos-tla\nGitHub repo.\nStrong consistency\nStrong consistency offers a linearizability guarantee. Linearizability refers to serving requests concurrently. The reads are guaranteed to return the most recent committed version of an item. A client never sees an uncommitted or partial write. Users are always guaranteed to read the latest committed write.\nBounded staleness consistency\nIn bounded staleness consistency, the lag of data between any two regions is always less than a specified amount. The amount can be\nK\nversions (that is,\nupdates\n) of an item or by\nT\ntime intervals, whichever is reached first. In other words, when you choose bounded staleness, the maximum \"staleness\" of the data in any region can be configured in two ways:\nThe number of versions (\nK\n) of the item\nThe time interval (\nT\n) reads might lag behind the writes\nBounded Staleness is beneficial primarily to single-region write accounts with two or more regions. If the data lag in a region (determined per physical partition) exceeds the configured staleness value, writes for that partition are throttled until staleness is back within the configured upper bound.\nFor a single-region account, Bounded Staleness provides the same write consistency guarantees as Session and Eventual Consistency. With Bounded Staleness, data is replicated to a local majority (three replicas in a four replica set) in the single region.\nSession consistency\nIn session consistency, within a single client session, reads are guaranteed to honor the read-your-writes, and write-follows-reads guarantees. This guarantee assumes a single âwriter\" session or sharing the session token for multiple writers.\nLike all consistency levels weaker than Strong, writes are replicated to a minimum of three replicas (in a four replica set) in the local region, with asynchronous replication to all other regions.\nConsistent prefix consistency\nIn consistent prefix, updates made as single document writes see eventual consistency. Updates made as a batch within a transaction, are returned consistent to the transaction in which they were committed. Write operations within a transaction of multiple documents are always visible together.\nAssume two write operations are performed on documents\nDoc 1\nand\nDoc 2\n, within transactions T1 and T2. When client does a read in any replica, the user sees either \"\nDoc 1\nv1 and\nDoc 2\nv1\" or \"\nDoc 1\nv2 and\nDoc 2\nv2,\" but never \"\nDoc 1\nv1 and\nDoc 2\nv2\" or \"\nDoc 1\nv2 and\nDoc 2\nv1\" for the same read or query operation.\nEventual consistency\nIn eventual consistency, there's no ordering guarantee for reads. In the absence of any further writes, the replicas eventually converge.\nEventual consistency is the weakest form of consistency because a client might read the values that are older than the ones it read before. Eventual consistency is ideal where the application doesn't require any ordering guarantees. Examples include count of Retweets, Likes, or nonthreaded comments\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Choose the right consistency level"
                },
                {
                  "level": 2,
                  "text": "Configure the default consistency level"
                },
                {
                  "level": 2,
                  "text": "Guarantees associated with consistency levels"
                },
                {
                  "level": 3,
                  "text": "Strong consistency"
                },
                {
                  "level": 3,
                  "text": "Bounded staleness consistency"
                },
                {
                  "level": 3,
                  "text": "Session consistency"
                },
                {
                  "level": 3,
                  "text": "Consistent prefix consistency"
                },
                {
                  "level": 3,
                  "text": "Eventual consistency"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                },
                {
                  "url": "https://github.com/Azure/azure-cosmos-tla",
                  "text": "azure-cosmos-tla"
                }
              ]
            },
            {
              "number": 6,
              "title": "Explore supported APIs",
              "url": "https://learn.microsoft.com/en-us/training/modules/explore-azure-cosmos-db/6-cosmos-db-supported-apis",
              "href": "6-cosmos-db-supported-apis",
              "content": "Read in English\nAdd\nAdd to plan\nExplore supported APIs\nCompleted\n3 minutes\nAzure Cosmos DB offers multiple database APIs, which include NoSQL, MongoDB, PostgreSQL, Cassandra, Gremlin, and Table. By using these APIs, you can model real world data using documents, key-value, graph, and column-family data models. These APIs allow your applications to treat Azure Cosmos DB as if it were various other databases technologies, without the overhead of management, and scaling approaches. Azure Cosmos DB helps you to use the ecosystems, tools, and skills you already have for data modeling and querying with its various APIs.\nAll the APIs offer automatic scaling of storage and throughput, flexibility, and performance guarantees. There's no one best API, and you can choose any one of the APIs to build your application\nConsiderations when choosing an API\nAPI for NoSQL is native to Azure Cosmos DB.\nAPI for MongoDB, PostgreSQL, Cassandra, Gremlin, and Table implement the wire protocol of open-source database engines. These APIs are best suited if the following conditions are true:\nIf you have existing MongoDB, PostgreSQL, Cassandra, or Gremlin applications\nIf you don't want to rewrite your entire data access layer\nIf you want to use the open-source developer ecosystem, client-drivers, expertise, and resources for your database\nAPI for NoSQL\nThe Azure Cosmos DB API for NoSQL stores data in document format. It offers the best end-to-end experience as we have full control over the interface, service, and the SDK client libraries. Any new feature that is rolled out to Azure Cosmos DB is first available on API for NoSQL accounts. NoSQL accounts provide support for querying items using the Structured Query Language (SQL) syntax.\nAPI for MongoDB\nThe Azure Cosmos DB API for MongoDB stores data in a document structure, via BSON format. It's compatible with MongoDB wire protocol; however, it doesn't use any native MongoDB related code. The API for MongoDB is a great choice if you want to use the broader MongoDB ecosystem and skills, without compromising on using Azure Cosmos DB features.\nAPI for PostgreSQL\nAzure Cosmos DB for PostgreSQL is a managed service for running PostgreSQL at any scale, with the\nCitus open source\nsuperpower of distributed tables. It stores data either on a single node, or distributed in a multi-node configuration.\nAPI for Apache Cassandra\nThe Azure Cosmos DB API for Cassandra stores data in column-oriented schema. Apache Cassandra offers a highly distributed, horizontally scaling approach to storing large volumes of data while offering a flexible approach to a column-oriented schema. API for Cassandra in Azure Cosmos DB aligns with this philosophy to approaching distributed NoSQL databases. This API for Cassandra is wire protocol compatible with native Apache Cassandra.\nAPI for Apache Gremlin\nThe Azure Cosmos DB API for Gremlin allows users to make graph queries and stores data as edges and vertices.\nUse the API for Gremlin for scenarios:\nInvolving dynamic data\nInvolving data with complex relations\nInvolving data that is too complex to be modeled with relational databases\nIf you want to use the existing Gremlin ecosystem and skills\nAPI for Table\nThe Azure Cosmos DB API for Table stores data in key/value format. If you're currently using Azure Table storage, you might see some limitations in latency, scaling, throughput, global distribution, index management, low query performance. API for Table overcomes these limitations and the recommendation is to migrate your app if you want to use the benefits of Azure Cosmos DB. API for Table only supports OLTP scenarios.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Explore supported APIs"
                },
                {
                  "level": 2,
                  "text": "Considerations when choosing an API"
                },
                {
                  "level": 2,
                  "text": "API for NoSQL"
                },
                {
                  "level": 2,
                  "text": "API for MongoDB"
                },
                {
                  "level": 2,
                  "text": "API for PostgreSQL"
                },
                {
                  "level": 2,
                  "text": "API for Apache Cassandra"
                },
                {
                  "level": 2,
                  "text": "API for Apache Gremlin"
                },
                {
                  "level": 2,
                  "text": "API for Table"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                },
                {
                  "url": "https://github.com/citusdata/citus",
                  "text": "Citus open source"
                }
              ]
            },
            {
              "number": 7,
              "title": "Discover request units",
              "url": "https://learn.microsoft.com/en-us/training/modules/explore-azure-cosmos-db/7-cosmos-db-request-units",
              "href": "7-cosmos-db-request-units",
              "content": "Read in English\nAdd\nAdd to plan\nDiscover request units\nCompleted\n3 minutes\nWith Azure Cosmos DB, you pay for the throughput you provision and the storage you consume on an hourly basis. Throughput must be provisioned to ensure that sufficient system resources are available for your Azure Cosmos database always.\nThe cost of all database operations is normalized in Azure Cosmos DB and expressed by\nrequest units\n(or RUs, for short). A request unit represents the system resources such as CPU, IOPS, and memory that are required to perform the database operations supported by Azure Cosmos DB.\nThe cost to do a point read, which is fetching a single item by its ID and partition key value, for a 1-KB item is 1RU. All other database operations are similarly assigned a cost using RUs. No matter which API you use to interact with your Azure Cosmos container, costs are measured by RUs. Whether the database operation is a write, point read, or query, costs are measured in RUs.\nThe following image shows the high-level idea of RUs:\nThe type of Azure Cosmos DB account you're using determines the way consumed RUs get charged. There are two modes for account creation:\nProvisioned throughput mode\n: In this mode, you provision the number of RUs for your application on a per-second basis in increments of 100 RUs per second. To scale the provisioned throughput for your application, you can increase or decrease the number of RUs at any time in increments or decrements of 100 RUs. You can make your changes either programmatically or by using the Azure portal. You can provision throughput at container and database granularity level.\nServerless mode\n: In this mode, you don't have to provision any throughput when creating resources in your Azure Cosmos DB account. At the end of your billing period, you get billed for the number of request units consumed by your database operations.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Discover request units"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [
                {
                  "src": "../../wwl-azure/explore-azure-cosmos-db/media/request-units.png",
                  "absolute_url": "https://learn.microsoft.com/en-us/wwl-azure/explore-azure-cosmos-db/media/request-units.png",
                  "alt_text": "Image showing how database operations consume request units.",
                  "title": "",
                  "filename": "request-units.png",
                  "image_type": "illustration",
                  "context": {
                    "preceding_heading": "Discover request units",
                    "following_text": "The type of Azure Cosmos DB account you're using determines the way consumed RUs get charged. There are two modes for account creation:",
                    "figure_caption": "",
                    "parent_section": ""
                  }
                }
              ],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 8,
              "title": "Exercise: Create Azure Cosmos DB resources by using the Azure portal",
              "url": "https://learn.microsoft.com/en-us/training/modules/explore-azure-cosmos-db/8-create-cosmos-db-resources-portal",
              "href": "8-create-cosmos-db-resources-portal",
              "content": "Read in English\nAdd\nAdd to plan\nExercise: Create Azure Cosmos DB resources by using the Azure portal\nCompleted\n10 minutes\nIn this exercise you learn how to perform the following actions in the Azure portal:\nCreate an Azure Cosmos DB account\nAdd a database and a container\nAdd data to your database\nClean up resources\nPrerequisites\nAn Azure account with an active subscription. If you don't already have one, you can sign up for a free trial at\nhttps://azure.com/free\n.\nCreate an Azure Cosmos DB account\nSign-in to the\nAzure portal\n.\nFrom the Azure portal navigation pane, select\n+ Create a resource\n.\nSearch for\nAzure Cosmos DB\n, then select\nCreate/Azure Cosmos DB\nto get started.\nOn the\nWhich API best suits your workload?\npage, select\nCreate\nin the\nAzure Cosmos DB for NoSQL\nbox.\nIn the\nCreate Azure Cosmos DB Account - Azure Cosmos DB for NoSQL\npage, enter the basic settings for the new Azure Cosmos DB account.\nSubscription\n: Select the subscription you want to use.\nResource Group\n: Select\nCreate new\n, then enter\naz204-cosmos-rg\n.\nAccount Name\n: Enter a\nunique\nname to identify your Azure Cosmos\naccount. The name can only contain lowercase letters, numbers, and the hyphen (-) character. It must be between 3-31 characters in length.\nAvailability Zones\n: Select\nDisable\n.\nLocation\n: Use the location that is closest to your users to give them the fastest access to the data.\nCapacity mode\n: Select\nServerless\n.\nSelect\nReview + create\n.\nReview the account settings, and then select\nCreate\n. It takes a few minutes to create the account. Wait for the portal page to display\nYour deployment is complete\n.\nSelect\nGo to resource\nto go to the Azure Cosmos DB account page.\nAdd a database and a container\nYou can use the Data Explorer in the Azure portal to create a database and container.\nSelect\nData Explorer\nfrom the left navigation on your Azure Cosmos DB account page, and then select\nNew Container\n.\nIn the\nNew container\npane, enter the settings for the new container.\nDatabase ID\n: Select\nCreate new\n, and enter\nToDoList\n.\nContainer ID\n: Enter\nItems\nPartition key\n: Enter\n/category\n. The samples in this demo use\n/category\nas the partition key.\nSelect\nOK\n. The Data Explorer displays the new database and the container that you created.\nAdd data to your database\nAdd data to your new database using Data Explorer.\nIn\nData Explorer\n, expand the\nToDoList\ndatabase, and expand the\nItems\ncontainer. Next, select\nItems\n, and then select\nNew Item\n.\nAdd the following structure to the item on the right side of the\nItems\npane:\n{\n    \"id\": \"1\",\n    \"category\": \"personal\",\n    \"name\": \"groceries\",\n    \"description\": \"Pick up apples and strawberries.\",\n    \"isComplete\": false\n}\nSelect\nSave\n.\nSelect\nNew Item\nagain, and create and save another item with a unique\nid\n, and any other properties and values you want. Your items can have any structure, because Azure Cosmos DB doesn't impose any schema on your data.\nClean up resources\nSelect\nOverview\nfrom the left navigation on your Azure Cosmos DB account page.\nSelect the\naz204-cosmos-rg\nresource group link in the Essentials group.\nSelect\nDelete\nresource group and follow the directions to delete the resource group and all of the resources it contains.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Exercise: Create Azure Cosmos DB resources by using the Azure portal"
                },
                {
                  "level": 2,
                  "text": "Prerequisites"
                },
                {
                  "level": 2,
                  "text": "Create an Azure Cosmos DB account"
                },
                {
                  "level": 2,
                  "text": "Add a database and a container"
                },
                {
                  "level": 2,
                  "text": "Add data to your database"
                },
                {
                  "level": 2,
                  "text": "Clean up resources"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [
                "{\n    \"id\": \"1\",\n    \"category\": \"personal\",\n    \"name\": \"groceries\",\n    \"description\": \"Pick up apples and strawberries.\",\n    \"isComplete\": false\n}",
                "{\n    \"id\": \"1\",\n    \"category\": \"personal\",\n    \"name\": \"groceries\",\n    \"description\": \"Pick up apples and strawberries.\",\n    \"isComplete\": false\n}"
              ],
              "images": [
                {
                  "src": "../../wwl-azure/explore-azure-cosmos-db/media/portal-cosmos-new-container.png",
                  "absolute_url": "https://learn.microsoft.com/en-us/wwl-azure/explore-azure-cosmos-db/media/portal-cosmos-new-container.png",
                  "alt_text": "You can add a container using the Data Explorer.",
                  "title": "",
                  "filename": "portal-cosmos-new-container.png",
                  "image_type": "illustration",
                  "context": {
                    "preceding_heading": "Add a database and a container",
                    "following_text": "In theNew containerpane, enter the settings for the new container.",
                    "figure_caption": "",
                    "parent_section": ""
                  }
                },
                {
                  "src": "../../wwl-azure/explore-azure-cosmos-db/media/portal-cosmos-new-data.png",
                  "absolute_url": "https://learn.microsoft.com/en-us/wwl-azure/explore-azure-cosmos-db/media/portal-cosmos-new-data.png",
                  "alt_text": "Create new item in the database.",
                  "title": "",
                  "filename": "portal-cosmos-new-data.png",
                  "image_type": "illustration",
                  "context": {
                    "preceding_heading": "Add data to your database",
                    "following_text": "Add the following structure to the item on the right side of theItemspane:",
                    "figure_caption": "",
                    "parent_section": ""
                  }
                }
              ],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                },
                {
                  "url": "https://azure.com/free",
                  "text": "https://azure.com/free"
                },
                {
                  "url": "https://portal.azure.com",
                  "text": "Azure portal"
                }
              ]
            },
            {
              "number": 9,
              "title": "Module assessment",
              "url": "https://learn.microsoft.com/en-us/training/modules/explore-azure-cosmos-db/9-knowledge-check",
              "href": "9-knowledge-check",
              "content": "Read in English\nAdd\nAdd to plan\nModule assessment\nCompleted\n5 minutes\nCheck your knowledge\n1.\nWhich of the following consistency levels offers the greatest throughput?\nStrong\nSession\nEventual\n2.\nWhat are request units (RUs) in Azure Cosmos DB?\nA unit of measurement used to express the cost of all database operations in Azure Cosmos DB.\nA unit of time used to measure the duration of database operations.\nA unit of storage used to measure the amount of data stored in Azure Cosmos DB.\nYou must answer all questions before checking your work.\nYou must answer all questions before checking your work.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Module assessment"
                },
                {
                  "level": 2,
                  "text": "Check your knowledge"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 10,
              "title": "Summary",
              "url": "https://learn.microsoft.com/en-us/training/modules/explore-azure-cosmos-db/10-summary",
              "href": "10-summary",
              "content": "Read in English\nAdd\nAdd to plan\nSummary\nCompleted\n3 minutes\nIn this module you learned how to:\nIdentify the key benefits provided by Azure Cosmos DB\nDescribe the elements in an Azure Cosmos DB account and how they're organized\nExplain the different consistency levels and choose the correct one for your project\nExplore the APIs supported in Azure Cosmos DB and choose the appropriate API for your solution\nDescribe how request units impact costs\nCreate Azure Cosmos DB resources by using the Azure portal.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Summary"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            }
          ]
        },
        {
          "title": "Work with Azure Cosmos DB",
          "url": "https://learn.microsoft.com/en-us/training/modules/work-with-cosmos-db/",
          "description": "",
          "learning_objectives": [],
          "prerequisites": [],
          "units": [
            {
              "number": 1,
              "title": "Introduction",
              "url": "https://learn.microsoft.com/en-us/training/modules/work-with-cosmos-db/1-introduction",
              "href": "1-introduction",
              "content": "Read in English\nAdd\nAdd to plan\nIntroduction\nCompleted\n3 minutes\nThis module is an introduction to both client and server-side programming on Azure Cosmos DB.\nAfter completing this module, you'll be able to:\nIdentify classes and methods used to create resources\nCreate resources in Azure Cosmos DB for NoSQL using .NET\nWrite stored procedures, triggers, and user-defined functions by using JavaScript\nImplement change feed notifications\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Introduction"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 2,
              "title": "Explore Microsoft .NET SDK v3 for Azure Cosmos DB",
              "url": "https://learn.microsoft.com/en-us/training/modules/work-with-cosmos-db/2-cosmos-db-dotnet-overview",
              "href": "2-cosmos-db-dotnet-overview",
              "content": "Read in English\nAdd\nAdd to plan\nExplore Microsoft .NET SDK v3 for Azure Cosmos DB\nCompleted\n3 minutes\nThis\nunit\nfocuses on Azure Cosmos DB .NET SDK v3 for API for NoSQL. (\nMicrosoft.Azure.Cosmos\nNuGet package.) If you're familiar with the previous version of the .NET SDK, you might be familiar with the terms collection and document.\nThe\nazure-cosmos-dotnet-v3\nGitHub repository includes the latest .NET sample solutions. You use these solutions to perform CRUD (create, read, update, and delete) and other common operations on Azure Cosmos DB resources.\nBecause Azure Cosmos DB supports multiple API models, version 3 of the .NET SDK uses the generic terms\ncontainer\nand\nitem\n. A\ncontainer\ncan be a collection, graph, or table. An\nitem\ncan be a document, edge/vertex, or row, and is the content inside a container.\nFollowing are examples showing some of the key operations you should be familiar with. For more examples, please visit the GitHub link shown earlier. The examples below all use the async version of the methods.\nCosmosClient\nCreates a new\nCosmosClient\nwith a connection string.\nCosmosClient\nis thread-safe. The recommendation is to maintain a single instance of\nCosmosClient\nper lifetime of the application that enables efficient connection management and performance.\nCosmosClient client = new CosmosClient(endpoint, key);\nDatabase examples\nCreate a database\nThe\nCosmosClient.CreateDatabaseAsync\nmethod throws an exception if a database with the same name already exists.\n// New instance of Database class referencing the server-side database\nDatabase database1 = await client.CreateDatabaseAsync(\n    id: \"adventureworks-1\"\n);\nThe\nCosmosClient.CreateDatabaseIfNotExistsAsync\nchecks if a database exists, and if it doesn't, creates it. Only the database\nid\nis used to verify if there's an existing database.\n// New instance of Database class referencing the server-side database\nDatabase database2 = await client.CreateDatabaseIfNotExistsAsync(\n    id: \"adventureworks-2\"\n);\nRead a database by ID\nReads a database from the Azure Cosmos DB service as an asynchronous operation.\n// Reads a Database resource with the ID property of the Database resource you wish to read.\nDatabase database = this.cosmosClient.GetDatabase(database_id);\nDatabaseResponse response = await database.ReadAsync();\nDelete a database\nDelete a Database as an asynchronous operation.\nawait database.DeleteAsync();\nContainer examples\nCreate a container\nThe\nDatabase.CreateContainerIfNotExistsAsync\nmethod checks if a container exists, and if it doesn't, it creates it. Only the container\nid\nis used to verify if there's an existing container.\n// Set throughput to the minimum value of 400 RU/s\nContainerResponse simpleContainer = await database.CreateContainerIfNotExistsAsync(\n    id: containerId,\n    partitionKeyPath: partitionKey,\n    throughput: 400);\nGet a container by ID\nContainer container = database.GetContainer(containerId);\nContainerProperties containerProperties = await container.ReadContainerAsync();\nDelete a container\nDelete a Container as an asynchronous operation.\nawait database.GetContainer(containerId).DeleteContainerAsync();\nItem examples\nCreate an item\nUse the\nContainer.CreateItemAsync\nmethod to create an item. The method requires a JSON serializable object that must contain an\nid\nproperty, and a\npartitionKey\n.\nItemResponse<SalesOrder> response = await container.CreateItemAsync(salesOrder, new PartitionKey(salesOrder.AccountNumber));\nRead an item\nUse the\nContainer.ReadItemAsync\nmethod to read an item. The method requires type to serialize the item to along with an\nid\nproperty, and a\npartitionKey\n.\nstring id = \"[id]\";\nstring accountNumber = \"[partition-key]\";\nItemResponse<SalesOrder> response = await container.ReadItemAsync(id, new PartitionKey(accountNumber));\nQuery an item\nThe\nContainer.GetItemQueryIterator\nmethod creates a query for items under a container in an Azure Cosmos database using a SQL statement with parameterized values. It returns a\nFeedIterator\n.\nQueryDefinition query = new QueryDefinition(\n    \"select * from sales s where s.AccountNumber = @AccountInput \")\n    .WithParameter(\"@AccountInput\", \"Account1\");\n\nFeedIterator<SalesOrder> resultSet = container.GetItemQueryIterator<SalesOrder>(\n    query,\n    requestOptions: new QueryRequestOptions()\n    {\n        PartitionKey = new PartitionKey(\"Account1\"),\n        MaxItemCount = 1\n    });\nOther resources\nThe\nazure-cosmos-dotnet-v3\nGitHub repository includes the latest .NET sample solutions to perform CRUD and other common operations on Azure Cosmos DB resources.\nVisit this article\nAzure Cosmos DB.NET V3 SDK (Microsoft.Azure.Cosmos) examples for the SQL API\nfor direct links to specific examples in the GitHub repository.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Explore Microsoft .NET SDK v3 for Azure Cosmos DB"
                },
                {
                  "level": 2,
                  "text": "CosmosClient"
                },
                {
                  "level": 2,
                  "text": "Database examples"
                },
                {
                  "level": 3,
                  "text": "Create a database"
                },
                {
                  "level": 3,
                  "text": "Read a database by ID"
                },
                {
                  "level": 3,
                  "text": "Delete a database"
                },
                {
                  "level": 2,
                  "text": "Container examples"
                },
                {
                  "level": 3,
                  "text": "Create a container"
                },
                {
                  "level": 3,
                  "text": "Get a container by ID"
                },
                {
                  "level": 3,
                  "text": "Delete a container"
                },
                {
                  "level": 2,
                  "text": "Item examples"
                },
                {
                  "level": 3,
                  "text": "Create an item"
                },
                {
                  "level": 3,
                  "text": "Read an item"
                },
                {
                  "level": 3,
                  "text": "Query an item"
                },
                {
                  "level": 2,
                  "text": "Other resources"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [
                "CosmosClient",
                "CosmosClient",
                "CosmosClient",
                "CosmosClient client = new CosmosClient(endpoint, key);",
                "CosmosClient client = new CosmosClient(endpoint, key);",
                "CosmosClient.CreateDatabaseAsync",
                "// New instance of Database class referencing the server-side database\nDatabase database1 = await client.CreateDatabaseAsync(\n    id: \"adventureworks-1\"\n);",
                "// New instance of Database class referencing the server-side database\nDatabase database1 = await client.CreateDatabaseAsync(\n    id: \"adventureworks-1\"\n);",
                "CosmosClient.CreateDatabaseIfNotExistsAsync",
                "// New instance of Database class referencing the server-side database\nDatabase database2 = await client.CreateDatabaseIfNotExistsAsync(\n    id: \"adventureworks-2\"\n);",
                "// New instance of Database class referencing the server-side database\nDatabase database2 = await client.CreateDatabaseIfNotExistsAsync(\n    id: \"adventureworks-2\"\n);",
                "// Reads a Database resource with the ID property of the Database resource you wish to read.\nDatabase database = this.cosmosClient.GetDatabase(database_id);\nDatabaseResponse response = await database.ReadAsync();",
                "// Reads a Database resource with the ID property of the Database resource you wish to read.\nDatabase database = this.cosmosClient.GetDatabase(database_id);\nDatabaseResponse response = await database.ReadAsync();",
                "await database.DeleteAsync();",
                "await database.DeleteAsync();",
                "Database.CreateContainerIfNotExistsAsync",
                "// Set throughput to the minimum value of 400 RU/s\nContainerResponse simpleContainer = await database.CreateContainerIfNotExistsAsync(\n    id: containerId,\n    partitionKeyPath: partitionKey,\n    throughput: 400);",
                "// Set throughput to the minimum value of 400 RU/s\nContainerResponse simpleContainer = await database.CreateContainerIfNotExistsAsync(\n    id: containerId,\n    partitionKeyPath: partitionKey,\n    throughput: 400);",
                "Container container = database.GetContainer(containerId);\nContainerProperties containerProperties = await container.ReadContainerAsync();",
                "Container container = database.GetContainer(containerId);\nContainerProperties containerProperties = await container.ReadContainerAsync();",
                "await database.GetContainer(containerId).DeleteContainerAsync();",
                "await database.GetContainer(containerId).DeleteContainerAsync();",
                "Container.CreateItemAsync",
                "partitionKey",
                "ItemResponse<SalesOrder> response = await container.CreateItemAsync(salesOrder, new PartitionKey(salesOrder.AccountNumber));",
                "ItemResponse<SalesOrder> response = await container.CreateItemAsync(salesOrder, new PartitionKey(salesOrder.AccountNumber));",
                "Container.ReadItemAsync",
                "partitionKey",
                "string id = \"[id]\";\nstring accountNumber = \"[partition-key]\";\nItemResponse<SalesOrder> response = await container.ReadItemAsync(id, new PartitionKey(accountNumber));",
                "string id = \"[id]\";\nstring accountNumber = \"[partition-key]\";\nItemResponse<SalesOrder> response = await container.ReadItemAsync(id, new PartitionKey(accountNumber));",
                "Container.GetItemQueryIterator",
                "FeedIterator",
                "QueryDefinition query = new QueryDefinition(\n    \"select * from sales s where s.AccountNumber = @AccountInput \")\n    .WithParameter(\"@AccountInput\", \"Account1\");\n\nFeedIterator<SalesOrder> resultSet = container.GetItemQueryIterator<SalesOrder>(\n    query,\n    requestOptions: new QueryRequestOptions()\n    {\n        PartitionKey = new PartitionKey(\"Account1\"),\n        MaxItemCount = 1\n    });",
                "QueryDefinition query = new QueryDefinition(\n    \"select * from sales s where s.AccountNumber = @AccountInput \")\n    .WithParameter(\"@AccountInput\", \"Account1\");\n\nFeedIterator<SalesOrder> resultSet = container.GetItemQueryIterator<SalesOrder>(\n    query,\n    requestOptions: new QueryRequestOptions()\n    {\n        PartitionKey = new PartitionKey(\"Account1\"),\n        MaxItemCount = 1\n    });"
              ],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                },
                {
                  "url": "https://github.com/Azure/azure-cosmos-dotnet-v3/tree/master/Microsoft.Azure.Cosmos.Samples/Usage",
                  "text": "azure-cosmos-dotnet-v3"
                },
                {
                  "url": "https://github.com/Azure/azure-cosmos-dotnet-v3/tree/master/Microsoft.Azure.Cosmos.Samples/Usage",
                  "text": "azure-cosmos-dotnet-v3"
                },
                {
                  "url": "/en-us/azure/cosmos-db/sql-api-dotnet-v3sdk-samples",
                  "text": "Azure Cosmos DB.NET V3 SDK (Microsoft.Azure.Cosmos) examples for the SQL API"
                }
              ]
            },
            {
              "number": 3,
              "title": "Exercise - Create resources in Azure Cosmos DB for NoSQL using .NET",
              "url": "https://learn.microsoft.com/en-us/training/modules/work-with-cosmos-db/3-exercise-work-cosmos-db-dotnet",
              "href": "3-exercise-work-cosmos-db-dotnet",
              "content": "Read in English\nAdd\nAdd to plan\nExercise - Create resources in Azure Cosmos DB for NoSQL using .NET\nCompleted\n30 minutes\nIn this exercise, you create an Azure Cosmos DB account and build a .NET console application that uses the Microsoft Azure Cosmos DB SDK to create a database, container, and sample item. You learn how to configure authentication, perform database operations programmatically, and verify your results in the Azure portal.\nTasks performed in this exercise:\nCreate an Azure Cosmos DB account\nCreate a console app that creates a database, container, and an item\nRun the console app and verify results\nThis exercise takes approximately\n30\nminutes to complete.\nBefore you start\nTo complete the exercise, you need:\nAn Azure subscription. If you don't already have one, you can sign up for one\nhttps://azure.microsoft.com/\n.\nGet started\nSelect the\nLaunch Exercise\nbutton to open the exercise instructions in a new browser window. When you're finished with the exercise, return here to:\nComplete the module\nEarn a badge for completing this module\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Exercise - Create resources in Azure Cosmos DB for NoSQL using .NET"
                },
                {
                  "level": 2,
                  "text": "Before you start"
                },
                {
                  "level": 2,
                  "text": "Get started"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [
                {
                  "src": "../../wwl-azure/work-with-cosmos-db/media/launch-exercise.png",
                  "absolute_url": "https://learn.microsoft.com/en-us/wwl-azure/work-with-cosmos-db/media/launch-exercise.png",
                  "alt_text": "Button to launch exercise.",
                  "title": "",
                  "filename": "launch-exercise.png",
                  "image_type": "icon",
                  "context": {
                    "preceding_heading": "Get started",
                    "following_text": "Was this page helpful?",
                    "figure_caption": "",
                    "parent_section": ""
                  }
                }
              ],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                },
                {
                  "url": "https://azure.microsoft.com/",
                  "text": "https://azure.microsoft.com/"
                }
              ]
            },
            {
              "number": 4,
              "title": "Create stored procedures",
              "url": "https://learn.microsoft.com/en-us/training/modules/work-with-cosmos-db/4-cosmos-db-stored-procedures",
              "href": "4-cosmos-db-stored-procedures",
              "content": "Read in English\nAdd\nAdd to plan\nCreate stored procedures\nCompleted\n3 minutes\nAzure Cosmos DB provides language-integrated, transactional execution of JavaScript that lets you write\nstored procedures\n,\ntriggers\n, and\nuser-defined functions (UDFs)\n. To call a stored procedure, trigger, or user-defined function, you need to register it. For more information, see\nHow to work with stored procedures, triggers, user-defined functions in Azure Cosmos DB\n.\nNote\nThis unit focuses on stored procedures, the following unit covers triggers and user-defined functions.\nWriting stored procedures\nStored procedures  can create, update, read, query, and delete items inside an Azure Cosmos container. Stored procedures are registered per collection, and can operate on any document or an attachment present in that collection.\nHere's a simple stored procedure that returns a \"Hello World\" response.\nvar helloWorldStoredProc = {\n    id: \"helloWorld\",\n    serverScript: function () {\n        var context = getContext();\n        var response = context.getResponse();\n\n        response.setBody(\"Hello, World\");\n    }\n}\nThe context object provides access to all operations that can be performed in Azure Cosmos DB, and access to the request and response objects. In this case, you use the response object to set the body of the response to be sent back to the client.\nCreate an item using stored procedure\nWhen you create an item by using a stored procedure, the item is inserted into the Azure Cosmos DB container and an ID for the newly created item is returned. Creating an item is an asynchronous operation and depends on the JavaScript callback functions. The callback function has two parameters: one for the error object in case the operation fails, and another for a return value, in this case, the created object. Inside the callback, you can either handle the exception or throw an error. If a callback isn't provided and there's an error, the Azure Cosmos DB runtime throws an error.\nThe stored procedure also includes a parameter to set the description as a boolean value. When the parameter is set to true and the description is missing, the stored procedure throws an exception. Otherwise, the rest of the stored procedure continues to run.\nThis stored procedure takes as input\ndocumentToCreate\n, the body of a document to be created in the current collection. All such operations are asynchronous and depend on JavaScript function callbacks.\nvar createDocumentStoredProc = {\n    id: \"createMyDocument\",\n    body: function createMyDocument(documentToCreate) {\n        var context = getContext();\n        var collection = context.getCollection();\n        var accepted = collection.createDocument(collection.getSelfLink(),\n              documentToCreate,\n              function (err, documentCreated) {\n                  if (err) throw new Error('Error' + err.message);\n                  context.getResponse().setBody(documentCreated.id)\n              });\n        if (!accepted) return;\n    }\n}\nArrays as input parameters for stored procedures\nWhen defining a stored procedure in the Azure portal, input parameters are always sent as a string to the stored procedure. Even if you pass an array of strings as an input, the array is converted to string and sent to the stored procedure. To work around this, you can define a function within your stored procedure to parse the string as an array. The following code shows how to parse a string input parameter as an array:\nfunction sample(arr) {\n    if (typeof arr === \"string\") arr = JSON.parse(arr);\n\n    arr.forEach(function(a) {\n        // do something here\n        console.log(a);\n    });\n}\nBounded execution\nAll Azure Cosmos DB operations must complete within a limited amount of time. Stored procedures have a limited amount of time to run on the server. All collection functions return a Boolean value that represents whether that operation completes or not\nTransactions within stored procedures\nYou can implement transactions on items within a container by using a stored procedure. JavaScript functions can implement a continuation-based model to batch or resume execution. The continuation value can be any value of your choice and your applications can then use this value to resume a transaction from a new starting point. The following diagram depicts how the transaction continuation model can be used to repeat a server-side function until the function finishes its entire processing workload.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Create stored procedures"
                },
                {
                  "level": 2,
                  "text": "Writing stored procedures"
                },
                {
                  "level": 2,
                  "text": "Create an item using stored procedure"
                },
                {
                  "level": 2,
                  "text": "Arrays as input parameters for stored procedures"
                },
                {
                  "level": 2,
                  "text": "Bounded execution"
                },
                {
                  "level": 2,
                  "text": "Transactions within stored procedures"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [
                "var helloWorldStoredProc = {\n    id: \"helloWorld\",\n    serverScript: function () {\n        var context = getContext();\n        var response = context.getResponse();\n\n        response.setBody(\"Hello, World\");\n    }\n}",
                "var helloWorldStoredProc = {\n    id: \"helloWorld\",\n    serverScript: function () {\n        var context = getContext();\n        var response = context.getResponse();\n\n        response.setBody(\"Hello, World\");\n    }\n}",
                "documentToCreate",
                "var createDocumentStoredProc = {\n    id: \"createMyDocument\",\n    body: function createMyDocument(documentToCreate) {\n        var context = getContext();\n        var collection = context.getCollection();\n        var accepted = collection.createDocument(collection.getSelfLink(),\n              documentToCreate,\n              function (err, documentCreated) {\n                  if (err) throw new Error('Error' + err.message);\n                  context.getResponse().setBody(documentCreated.id)\n              });\n        if (!accepted) return;\n    }\n}",
                "var createDocumentStoredProc = {\n    id: \"createMyDocument\",\n    body: function createMyDocument(documentToCreate) {\n        var context = getContext();\n        var collection = context.getCollection();\n        var accepted = collection.createDocument(collection.getSelfLink(),\n              documentToCreate,\n              function (err, documentCreated) {\n                  if (err) throw new Error('Error' + err.message);\n                  context.getResponse().setBody(documentCreated.id)\n              });\n        if (!accepted) return;\n    }\n}",
                "function sample(arr) {\n    if (typeof arr === \"string\") arr = JSON.parse(arr);\n\n    arr.forEach(function(a) {\n        // do something here\n        console.log(a);\n    });\n}",
                "function sample(arr) {\n    if (typeof arr === \"string\") arr = JSON.parse(arr);\n\n    arr.forEach(function(a) {\n        // do something here\n        console.log(a);\n    });\n}"
              ],
              "images": [
                {
                  "src": "../../wwl-azure/work-with-cosmos-db/media/transaction-continuation-model.png",
                  "absolute_url": "https://learn.microsoft.com/en-us/wwl-azure/work-with-cosmos-db/media/transaction-continuation-model.png",
                  "alt_text": "This diagram depicts how the transaction continuation model can be used to repeat a server-side function until the function finishes its entire processing workload.",
                  "title": "",
                  "filename": "transaction-continuation-model.png",
                  "image_type": "diagram",
                  "context": {
                    "preceding_heading": "Transactions within stored procedures",
                    "following_text": "Was this page helpful?",
                    "figure_caption": "",
                    "parent_section": ""
                  }
                }
              ],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                },
                {
                  "url": "/en-us/azure/cosmos-db/sql/how-to-use-stored-procedures-triggers-udfs",
                  "text": "How to work with stored procedures, triggers, user-defined functions in Azure Cosmos DB"
                }
              ]
            },
            {
              "number": 5,
              "title": "Create triggers and user-defined functions",
              "url": "https://learn.microsoft.com/en-us/training/modules/work-with-cosmos-db/5-cosmos-db-triggers-user-defined-functions",
              "href": "5-cosmos-db-triggers-user-defined-functions",
              "content": "Read in English\nAdd\nAdd to plan\nCreate triggers and user-defined functions\nCompleted\n3 minutes\nAzure Cosmos DB supports pretriggers and post-triggers. Pretriggers are executed before modifying a database item and post-triggers are executed after modifying a database item. Triggers aren't automatically executed. They must be specified for each database operation where you want them to execute. After you define a trigger, you should register it by using the Azure Cosmos DB SDKs.\nFor examples of how to register and call a trigger, see\npretriggers\nand\npost-triggers\n.\nPretriggers\nThe following example shows how a pretrigger is used to validate the properties of an Azure Cosmos item that is being created. It adds a timestamp property to a newly added item if it doesn't contain one.\nfunction validateToDoItemTimestamp() {\n    var context = getContext();\n    var request = context.getRequest();\n\n    // item to be created in the current operation\n    var itemToCreate = request.getBody();\n\n    // validate properties\n    if (!(\"timestamp\" in itemToCreate)) {\n        var ts = new Date();\n        itemToCreate[\"timestamp\"] = ts.getTime();\n    }\n\n    // update the item that will be created\n    request.setBody(itemToCreate);\n}\nPretriggers can't have any input parameters. The request object in the trigger is used to manipulate the request message associated with the operation. In the previous example, the pretrigger is run when creating an Azure Cosmos item and the request message body contains the item to be created in JSON format.\nWhen triggers are registered, you can specify the operations that it can run with. This trigger should be created with a\nTriggerOperation\nvalue of\nTriggerOperation.Create\n, using the trigger in a replace operation isn't permitted.\nFor examples of how to register and call a pretrigger, visit the\npretriggers\narticle.\nPost-triggers\nThe following example shows a post-trigger. This trigger queries for the metadata item and updates it with details about the newly created item.\nfunction updateMetadata() {\nvar context = getContext();\nvar container = context.getCollection();\nvar response = context.getResponse();\n\n// item that was created\nvar createdItem = response.getBody();\n\n// query for metadata document\nvar filterQuery = 'SELECT * FROM root r WHERE r.id = \"_metadata\"';\nvar accept = container.queryDocuments(container.getSelfLink(), filterQuery,\n    updateMetadataCallback);\nif(!accept) throw \"Unable to update metadata, abort\";\n\nfunction updateMetadataCallback(err, items, responseOptions) {\n    if(err) throw new Error(\"Error\" + err.message);\n        if(items.length != 1) throw 'Unable to find metadata document';\n\n        var metadataItem = items[0];\n\n        // update metadata\n        metadataItem.createdItems += 1;\n        metadataItem.createdNames += \" \" + createdItem.id;\n        var accept = container.replaceDocument(metadataItem._self,\n            metadataItem, function(err, itemReplaced) {\n                    if(err) throw \"Unable to update metadata, abort\";\n            });\n        if(!accept) throw \"Unable to update metadata, abort\";\n        return;\n    }\n}\nOne thing that is important to note is the transactional execution of triggers in Azure Cosmos DB. The post-trigger runs as part of the same transaction for the underlying item itself. An exception during the post-trigger execution fails the whole transaction. Anything committed is rolled back and an exception returned.\nUser-defined functions\nThe following sample creates a UDF to calculate income tax for various income brackets. This user-defined function would then be used inside a query. For the purposes of this example assume there's a container called \"Incomes\" with properties as follows:\n{\n   \"name\": \"User One\",\n   \"country\": \"USA\",\n   \"income\": 70000\n}\nThe following code sample is a function definition to calculate income tax for various income brackets:\nfunction tax(income) {\n\n        if(income == undefined)\n            throw 'no input';\n\n        if (income < 1000)\n            return income * 0.1;\n        else if (income < 10000)\n            return income * 0.2;\n        else\n            return income * 0.4;\n    }\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Create triggers and user-defined functions"
                },
                {
                  "level": 2,
                  "text": "Pretriggers"
                },
                {
                  "level": 2,
                  "text": "Post-triggers"
                },
                {
                  "level": 2,
                  "text": "User-defined functions"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [
                "function validateToDoItemTimestamp() {\n    var context = getContext();\n    var request = context.getRequest();\n\n    // item to be created in the current operation\n    var itemToCreate = request.getBody();\n\n    // validate properties\n    if (!(\"timestamp\" in itemToCreate)) {\n        var ts = new Date();\n        itemToCreate[\"timestamp\"] = ts.getTime();\n    }\n\n    // update the item that will be created\n    request.setBody(itemToCreate);\n}",
                "function validateToDoItemTimestamp() {\n    var context = getContext();\n    var request = context.getRequest();\n\n    // item to be created in the current operation\n    var itemToCreate = request.getBody();\n\n    // validate properties\n    if (!(\"timestamp\" in itemToCreate)) {\n        var ts = new Date();\n        itemToCreate[\"timestamp\"] = ts.getTime();\n    }\n\n    // update the item that will be created\n    request.setBody(itemToCreate);\n}",
                "TriggerOperation",
                "TriggerOperation.Create",
                "function updateMetadata() {\nvar context = getContext();\nvar container = context.getCollection();\nvar response = context.getResponse();\n\n// item that was created\nvar createdItem = response.getBody();\n\n// query for metadata document\nvar filterQuery = 'SELECT * FROM root r WHERE r.id = \"_metadata\"';\nvar accept = container.queryDocuments(container.getSelfLink(), filterQuery,\n    updateMetadataCallback);\nif(!accept) throw \"Unable to update metadata, abort\";\n\nfunction updateMetadataCallback(err, items, responseOptions) {\n    if(err) throw new Error(\"Error\" + err.message);\n        if(items.length != 1) throw 'Unable to find metadata document';\n\n        var metadataItem = items[0];\n\n        // update metadata\n        metadataItem.createdItems += 1;\n        metadataItem.createdNames += \" \" + createdItem.id;\n        var accept = container.replaceDocument(metadataItem._self,\n            metadataItem, function(err, itemReplaced) {\n                    if(err) throw \"Unable to update metadata, abort\";\n            });\n        if(!accept) throw \"Unable to update metadata, abort\";\n        return;\n    }\n}",
                "function updateMetadata() {\nvar context = getContext();\nvar container = context.getCollection();\nvar response = context.getResponse();\n\n// item that was created\nvar createdItem = response.getBody();\n\n// query for metadata document\nvar filterQuery = 'SELECT * FROM root r WHERE r.id = \"_metadata\"';\nvar accept = container.queryDocuments(container.getSelfLink(), filterQuery,\n    updateMetadataCallback);\nif(!accept) throw \"Unable to update metadata, abort\";\n\nfunction updateMetadataCallback(err, items, responseOptions) {\n    if(err) throw new Error(\"Error\" + err.message);\n        if(items.length != 1) throw 'Unable to find metadata document';\n\n        var metadataItem = items[0];\n\n        // update metadata\n        metadataItem.createdItems += 1;\n        metadataItem.createdNames += \" \" + createdItem.id;\n        var accept = container.replaceDocument(metadataItem._self,\n            metadataItem, function(err, itemReplaced) {\n                    if(err) throw \"Unable to update metadata, abort\";\n            });\n        if(!accept) throw \"Unable to update metadata, abort\";\n        return;\n    }\n}",
                "{\n   \"name\": \"User One\",\n   \"country\": \"USA\",\n   \"income\": 70000\n}",
                "{\n   \"name\": \"User One\",\n   \"country\": \"USA\",\n   \"income\": 70000\n}",
                "function tax(income) {\n\n        if(income == undefined)\n            throw 'no input';\n\n        if (income < 1000)\n            return income * 0.1;\n        else if (income < 10000)\n            return income * 0.2;\n        else\n            return income * 0.4;\n    }",
                "function tax(income) {\n\n        if(income == undefined)\n            throw 'no input';\n\n        if (income < 1000)\n            return income * 0.1;\n        else if (income < 10000)\n            return income * 0.2;\n        else\n            return income * 0.4;\n    }"
              ],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                },
                {
                  "url": "/en-us/azure/cosmos-db/sql/how-to-use-stored-procedures-triggers-udfs#pre-triggers",
                  "text": "pretriggers"
                },
                {
                  "url": "/en-us/azure/cosmos-db/sql/how-to-use-stored-procedures-triggers-udfs#post-triggers",
                  "text": "post-triggers"
                },
                {
                  "url": "/en-us/azure/cosmos-db/sql/how-to-use-stored-procedures-triggers-udfs#pre-triggers",
                  "text": "pretriggers"
                }
              ]
            },
            {
              "number": 6,
              "title": "Explore change feed in Azure Cosmos DB",
              "url": "https://learn.microsoft.com/en-us/training/modules/work-with-cosmos-db/6-cosmos-db-change-feed",
              "href": "6-cosmos-db-change-feed",
              "content": "Read in English\nAdd\nAdd to plan\nExplore change feed in Azure Cosmos DB\nCompleted\n5 minutes\nChange feed in Azure Cosmos DB is a persistent record of changes to a container in the order they occur. Change feed support in Azure Cosmos DB works by listening to an Azure Cosmos DB container for any changes. It then outputs the sorted list of documents that were changed in the order in which they were modified. The persisted changes can be processed asynchronously and incrementally, and the output can be distributed across one or more consumers for parallel processing.\nChange feed and different operations\nToday, you see all inserts and updates in the change feed. You can't filter the change feed for a specific type of operation. Currently change feed doesn't log delete operations. As a workaround, you can add a soft marker on the items that are being deleted. For example, you can add an attribute in the item called \"deleted,\" set its value to \"true,\" and then set a time-to-live (TTL) value on the item. Setting the TTL ensures that the item is automatically deleted.\nReading Azure Cosmos DB change feed\nYou can work with the Azure Cosmos DB change feed using either a push model or a pull model. With a push model, the change feed processor pushes work to a client that has business logic for processing this work. However, the complexity in checking for work and storing state for the last processed work is handled within the change feed processor.\nWith a pull model, the client has to pull the work from the server. In this case, the client has business logic for processing work and also stores state for the last processed work. The client handles load balancing across multiple clients processing work in parallel, and handling errors.\nNote\nIt's recommended to use the push model because you won't need to worry about polling the change feed for future changes, storing state for the last processed change, and other benefits.\nMost scenarios that use the Azure Cosmos DB change feed use one of the push model options. However, there are some scenarios where you might want the extra low level control of the pull model. The extra low-level control includes:\nReading changes from a particular partition key\nControlling the pace at which your client receives changes for processing\nDoing a one-time read of the existing data in the change feed (for example, to do a data migration)\nReading change feed with a push model\nThere are two ways you can read from the change feed with a push model: Azure Functions Azure Cosmos DB triggers, and the change feed processor library. Azure Functions uses the change feed processor behind the scenes, so they're both similar ways to read the change feed. Think of Azure Functions as simply a hosting platform for the change feed processor, not an entirely different way of reading the change feed. Azure Functions uses the change feed processor behind the scenes. It automatically parallelizes change processing across your container's partitions.\nAzure Functions\nYou can create small reactive Azure Functions that are automatically triggered on each new event in your Azure Cosmos DB container's change feed. With the\nAzure Functions trigger for Azure Cosmos DB\n, you can use the Change Feed Processor's scaling and reliable event detection functionality without the need to maintain any worker infrastructure.\nChange feed processor\nThe change feed processor is part of the Azure Cosmos DB\n.NET V3\nand\nJava V4\nSDKs. It simplifies the process of reading the change feed and distributes the event processing across multiple consumers effectively.\nThere are four main components of implementing the change feed processor:\nThe monitored container\n: The monitored container has the data from which the change feed is generated. Any inserts and updates to the monitored container are reflected in the change feed of the container.\nThe lease container\n: The lease container acts as a state storage and coordinates processing the change feed across multiple workers. The lease container can be stored in the same account as the monitored container or in a separate account.\nThe compute instance\n: A compute instance hosts the change feed processor to listen for changes. Depending on the platform, it might be represented by a VM, a kubernetes pod, an Azure App Service instance, an actual physical machine. It has a unique identifier referenced as the instance name throughout this article.\nThe delegate\n: The delegate is the code that defines what you, the developer, want to do with each batch of changes that the change feed processor reads.\nWhen implementing the change feed processor the point of entry is always the monitored container, from a\nContainer\ninstance you call\nGetChangeFeedProcessorBuilder\n:\n/// <summary>\n/// Start the Change Feed Processor to listen for changes and process them with the HandleChangesAsync implementation.\n/// </summary>\nprivate static async Task<ChangeFeedProcessor> StartChangeFeedProcessorAsync(\n    CosmosClient cosmosClient,\n    IConfiguration configuration)\n{\n    string databaseName = configuration[\"SourceDatabaseName\"];\n    string sourceContainerName = configuration[\"SourceContainerName\"];\n    string leaseContainerName = configuration[\"LeasesContainerName\"];\n\n    Container leaseContainer = cosmosClient.GetContainer(databaseName, leaseContainerName);\n    ChangeFeedProcessor changeFeedProcessor = cosmosClient.GetContainer(databaseName, sourceContainerName)\n        .GetChangeFeedProcessorBuilder<ToDoItem>(processorName: \"changeFeedSample\", onChangesDelegate: HandleChangesAsync)\n            .WithInstanceName(\"consoleHost\")\n            .WithLeaseContainer(leaseContainer)\n            .Build();\n\n    Console.WriteLine(\"Starting Change Feed Processor...\");\n    await changeFeedProcessor.StartAsync();\n    Console.WriteLine(\"Change Feed Processor started.\");\n    return changeFeedProcessor;\n}\nWhere the first parameter is a distinct name that describes the goal of this processor and the second parameter is the delegate implementation that handles changes. Following is an example of a delegate:\n/// <summary>\n/// The delegate receives batches of changes as they are generated in the change feed and can process them.\n/// </summary>\nstatic async Task HandleChangesAsync(\n    ChangeFeedProcessorContext context,\n    IReadOnlyCollection<ToDoItem> changes,\n    CancellationToken cancellationToken)\n{\n    Console.WriteLine($\"Started handling changes for lease {context.LeaseToken}...\");\n    Console.WriteLine($\"Change Feed request consumed {context.Headers.RequestCharge} RU.\");\n    // SessionToken if needed to enforce Session consistency on another client instance\n    Console.WriteLine($\"SessionToken ${context.Headers.Session}\");\n\n    // We may want to track any operation's Diagnostics that took longer than some threshold\n    if (context.Diagnostics.GetClientElapsedTime() > TimeSpan.FromSeconds(1))\n    {\n        Console.WriteLine($\"Change Feed request took longer than expected. Diagnostics:\" + context.Diagnostics.ToString());\n    }\n\n    foreach (ToDoItem item in changes)\n    {\n        Console.WriteLine($\"Detected operation for item with id {item.id}, created at {item.creationTime}.\");\n        // Simulate some asynchronous operation\n        await Task.Delay(10);\n    }\n\n    Console.WriteLine(\"Finished handling changes.\");\n}\nAfterwards, you define the compute instance name or unique identifier with\nWithInstanceName\n, this should be unique and different in each compute instance you're deploying, and finally, which is the container to maintain the lease state with\nWithLeaseContainer\n.\nCalling\nBuild\ngives you the processor instance that you can start by calling\nStartAsync\n.\nThe normal life cycle of a host instance is:\nRead the change feed.\nIf there are no changes, sleep for a predefined amount of time (customizable with\nWithPollInterval\nin the\nBuilder\n) and go to #1.\nIf there are changes, send them to the delegate.\nWhen the delegate finishes processing the changes successfully, update the lease store with the latest processed point in time and go to #1.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Explore change feed in Azure Cosmos DB"
                },
                {
                  "level": 2,
                  "text": "Change feed and different operations"
                },
                {
                  "level": 2,
                  "text": "Reading Azure Cosmos DB change feed"
                },
                {
                  "level": 2,
                  "text": "Reading change feed with a push model"
                },
                {
                  "level": 3,
                  "text": "Azure Functions"
                },
                {
                  "level": 3,
                  "text": "Change feed processor"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [
                "GetChangeFeedProcessorBuilder",
                "/// <summary>\n/// Start the Change Feed Processor to listen for changes and process them with the HandleChangesAsync implementation.\n/// </summary>\nprivate static async Task<ChangeFeedProcessor> StartChangeFeedProcessorAsync(\n    CosmosClient cosmosClient,\n    IConfiguration configuration)\n{\n    string databaseName = configuration[\"SourceDatabaseName\"];\n    string sourceContainerName = configuration[\"SourceContainerName\"];\n    string leaseContainerName = configuration[\"LeasesContainerName\"];\n\n    Container leaseContainer = cosmosClient.GetContainer(databaseName, leaseContainerName);\n    ChangeFeedProcessor changeFeedProcessor = cosmosClient.GetContainer(databaseName, sourceContainerName)\n        .GetChangeFeedProcessorBuilder<ToDoItem>(processorName: \"changeFeedSample\", onChangesDelegate: HandleChangesAsync)\n            .WithInstanceName(\"consoleHost\")\n            .WithLeaseContainer(leaseContainer)\n            .Build();\n\n    Console.WriteLine(\"Starting Change Feed Processor...\");\n    await changeFeedProcessor.StartAsync();\n    Console.WriteLine(\"Change Feed Processor started.\");\n    return changeFeedProcessor;\n}",
                "/// <summary>\n/// Start the Change Feed Processor to listen for changes and process them with the HandleChangesAsync implementation.\n/// </summary>\nprivate static async Task<ChangeFeedProcessor> StartChangeFeedProcessorAsync(\n    CosmosClient cosmosClient,\n    IConfiguration configuration)\n{\n    string databaseName = configuration[\"SourceDatabaseName\"];\n    string sourceContainerName = configuration[\"SourceContainerName\"];\n    string leaseContainerName = configuration[\"LeasesContainerName\"];\n\n    Container leaseContainer = cosmosClient.GetContainer(databaseName, leaseContainerName);\n    ChangeFeedProcessor changeFeedProcessor = cosmosClient.GetContainer(databaseName, sourceContainerName)\n        .GetChangeFeedProcessorBuilder<ToDoItem>(processorName: \"changeFeedSample\", onChangesDelegate: HandleChangesAsync)\n            .WithInstanceName(\"consoleHost\")\n            .WithLeaseContainer(leaseContainer)\n            .Build();\n\n    Console.WriteLine(\"Starting Change Feed Processor...\");\n    await changeFeedProcessor.StartAsync();\n    Console.WriteLine(\"Change Feed Processor started.\");\n    return changeFeedProcessor;\n}",
                "/// <summary>\n/// The delegate receives batches of changes as they are generated in the change feed and can process them.\n/// </summary>\nstatic async Task HandleChangesAsync(\n    ChangeFeedProcessorContext context,\n    IReadOnlyCollection<ToDoItem> changes,\n    CancellationToken cancellationToken)\n{\n    Console.WriteLine($\"Started handling changes for lease {context.LeaseToken}...\");\n    Console.WriteLine($\"Change Feed request consumed {context.Headers.RequestCharge} RU.\");\n    // SessionToken if needed to enforce Session consistency on another client instance\n    Console.WriteLine($\"SessionToken ${context.Headers.Session}\");\n\n    // We may want to track any operation's Diagnostics that took longer than some threshold\n    if (context.Diagnostics.GetClientElapsedTime() > TimeSpan.FromSeconds(1))\n    {\n        Console.WriteLine($\"Change Feed request took longer than expected. Diagnostics:\" + context.Diagnostics.ToString());\n    }\n\n    foreach (ToDoItem item in changes)\n    {\n        Console.WriteLine($\"Detected operation for item with id {item.id}, created at {item.creationTime}.\");\n        // Simulate some asynchronous operation\n        await Task.Delay(10);\n    }\n\n    Console.WriteLine(\"Finished handling changes.\");\n}",
                "/// <summary>\n/// The delegate receives batches of changes as they are generated in the change feed and can process them.\n/// </summary>\nstatic async Task HandleChangesAsync(\n    ChangeFeedProcessorContext context,\n    IReadOnlyCollection<ToDoItem> changes,\n    CancellationToken cancellationToken)\n{\n    Console.WriteLine($\"Started handling changes for lease {context.LeaseToken}...\");\n    Console.WriteLine($\"Change Feed request consumed {context.Headers.RequestCharge} RU.\");\n    // SessionToken if needed to enforce Session consistency on another client instance\n    Console.WriteLine($\"SessionToken ${context.Headers.Session}\");\n\n    // We may want to track any operation's Diagnostics that took longer than some threshold\n    if (context.Diagnostics.GetClientElapsedTime() > TimeSpan.FromSeconds(1))\n    {\n        Console.WriteLine($\"Change Feed request took longer than expected. Diagnostics:\" + context.Diagnostics.ToString());\n    }\n\n    foreach (ToDoItem item in changes)\n    {\n        Console.WriteLine($\"Detected operation for item with id {item.id}, created at {item.creationTime}.\");\n        // Simulate some asynchronous operation\n        await Task.Delay(10);\n    }\n\n    Console.WriteLine(\"Finished handling changes.\");\n}",
                "WithInstanceName",
                "WithLeaseContainer",
                "WithPollInterval"
              ],
              "images": [
                {
                  "src": "../../wwl-azure/work-with-cosmos-db/media/functions-change-feed.png",
                  "absolute_url": "https://learn.microsoft.com/en-us/wwl-azure/work-with-cosmos-db/media/functions-change-feed.png",
                  "alt_text": "Diagram showing the change feed triggering Azure Functions for processing.",
                  "title": "",
                  "filename": "functions-change-feed.png",
                  "image_type": "diagram",
                  "context": {
                    "preceding_heading": "Azure Functions",
                    "following_text": "The change feed processor is part of the Azure Cosmos DB.NET V3andJava V4SDKs. It simplifies the process of reading the change feed and distributes the event processing across multiple consumers effec",
                    "figure_caption": "",
                    "parent_section": ""
                  }
                }
              ],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                },
                {
                  "url": "/en-us/azure/azure-functions/functions-bindings-cosmosdb-v2-trigger",
                  "text": "Azure Functions trigger for Azure Cosmos DB"
                },
                {
                  "url": "https://github.com/Azure/azure-cosmos-dotnet-v3",
                  "text": ".NET V3"
                },
                {
                  "url": "https://github.com/Azure/azure-sdk-for-java/tree/main/sdk/cosmos/azure-cosmos",
                  "text": "Java V4"
                }
              ]
            },
            {
              "number": 7,
              "title": "Module assessment",
              "url": "https://learn.microsoft.com/en-us/training/modules/work-with-cosmos-db/7-knowledge-check",
              "href": "7-knowledge-check",
              "content": "Read in English\nAdd\nAdd to plan\nModule assessment\nCompleted\n5 minutes\nCheck your knowledge\n1.\nWhat is the purpose of the context object in a stored procedure in Azure Cosmos DB?\nIt provides access to the database schema and metadata.\nIt allows for the creation of new collections within the database.\nIt provides access to all operations that can be performed in Azure Cosmos DB, and access to the request and response objects.\n2.\nWhat is the role of pretriggers in Azure Cosmos DB?\nPretriggers are automatically executed for each database operation.\nPretriggers are executed before modifying a database item and must be specified for each database operation where you want them to execute.\nPretriggers are used to execute operations after modifying a database item.\n3.\nWhat is the purpose of the lease container in the Azure Cosmos DB change feed processor?\nIt stores the data from which the change feed is generated.\nIt processes the change feed across multiple workers.\nIt acts as a state storage and coordinates processing the change feed across multiple workers.\nYou must answer all questions before checking your work.\nYou must answer all questions before checking your work.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Module assessment"
                },
                {
                  "level": 2,
                  "text": "Check your knowledge"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 8,
              "title": "Summary",
              "url": "https://learn.microsoft.com/en-us/training/modules/work-with-cosmos-db/8-summary",
              "href": "8-summary",
              "content": "Read in English\nAdd\nAdd to plan\nSummary\nCompleted\n3 minutes\nIn this module you learned how to:\nIdentify classes and methods used to create resources\nCreate resources in Azure Cosmos DB for NoSQL using .NET\nWrite stored procedures, triggers, and user-defined functions by using JavaScript\nImplement change feed notifications\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Summary"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "title": "Implement containerized solutions",
      "url": "https://learn.microsoft.com/en-us/training/paths/az-204-implement-iaas-solutions/",
      "learn_uid": "learn.wwl.az-204-implement-iaas-solutions",
      "modules": [
        {
          "title": "Manage container images in Azure Container Registry",
          "url": "https://learn.microsoft.com/en-us/training/modules/publish-container-image-to-azure-container-registry/",
          "description": "",
          "learning_objectives": [],
          "prerequisites": [],
          "units": [
            {
              "number": 1,
              "title": "Introduction",
              "url": "https://learn.microsoft.com/en-us/training/modules/publish-container-image-to-azure-container-registry/1-introduction",
              "href": "1-introduction",
              "content": "Read in English\nAdd\nAdd to plan\nIntroduction\nCompleted\n3 minutes\nAzure Container Registry (ACR) is a managed, private Docker registry service based on the open-source Docker Registry 2.0. Create and maintain Azure container registries to store and manage your private Docker container images.\nAfter completing this module, you'll be able to:\nExplain the features and benefits Azure Container Registry offers\nDescribe how to use ACR Tasks to automate builds and deployments\nExplain the elements in a Dockerfile\nBuild and run an image in the ACR using Azure CLI commands\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Introduction"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 2,
              "title": "Discover the Azure Container Registry",
              "url": "https://learn.microsoft.com/en-us/training/modules/publish-container-image-to-azure-container-registry/2-azure-container-registry-overview",
              "href": "2-azure-container-registry-overview",
              "content": "Read in English\nAdd\nAdd to plan\nDiscover the Azure Container Registry\nCompleted\n3 minutes\nAzure Container Registry (ACR) is a managed registry service based on the open-source Docker Registry 2.0. Create and maintain Azure container registries to store and manage your container images and related artifacts.\nUse the ACR service with your existing container development and deployment pipelines, or use Azure Container Registry Tasks to build container images in Azure. Build on demand, or fully automate builds with triggers such as source code commits and base image updates.\nUse cases\nPull images from an Azure container registry to various deployment targets:\nScalable orchestration systems\nthat manage containerized applications across clusters of hosts, including Kubernetes, DC/OS, and Docker Swarm.\nAzure services\nthat support building and running applications at scale, including Azure Kubernetes Service (AKS), App Service, Batch, and Service Fabric.\nDevelopers can also push to a container registry as part of a container development workflow. For example, target a container registry from a continuous integration and delivery tool such as Azure Pipelines or Jenkins.\nConfigure ACR Tasks to automatically rebuild application images when their base images are updated, or automate image builds when your team commits code to a Git repository. Create multi-step tasks to automate building, testing, and patching multiple container images in parallel in the cloud.\nAzure Container Registry service tiers\nAzure Container Registry is available in multiple service tiers. These tiers provide predictable pricing and several options for aligning to the capacity and usage patterns of your private Docker registry in Azure.\nTier\nDescription\nBasic\nA cost-optimized entry point for developers learning about Azure Container Registry. Basic registries have the same programmatic capabilities as Standard and Premium (such as Microsoft Entra authentication integration, image deletion, and webhooks). However, the included storage and image throughput are most appropriate for lower usage scenarios.\nStandard\nStandard registries offer the same capabilities as Basic, with increased included storage and image throughput. Standard registries should satisfy the needs of most production scenarios.\nPremium\nPremium registries provide the highest amount of included storage and concurrent operations, enabling high-volume scenarios. In addition to higher image throughput, Premium adds features such as: geo-replication for managing a single registry across multiple regions, content trust for image tag signing, and private link with private endpoints to restrict access to the registry.\nSupported images and artifacts\nWhen images are grouped in a repository, each image is a read-only snapshot of a Docker-compatible container. Azure container registries can include both Windows and Linux images. In addition to Docker container images, Azure Container Registry stores related content formats such as\nHelm charts\nand images built to the\nOpen Container Initiative (OCI) Image Format Specification\n.\nAutomated image builds\nUse\nAzure Container Registry Tasks\n(ACR Tasks) to streamline building, testing, pushing, and deploying images in Azure. Configure build tasks to automate your container OS and framework patching pipeline, and build images automatically when your team commits code to source control.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Discover the Azure Container Registry"
                },
                {
                  "level": 2,
                  "text": "Use cases"
                },
                {
                  "level": 2,
                  "text": "Azure Container Registry service tiers"
                },
                {
                  "level": 2,
                  "text": "Supported images and artifacts"
                },
                {
                  "level": 2,
                  "text": "Automated image builds"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                },
                {
                  "url": "/en-us/azure/container-registry/container-registry-helm-repos",
                  "text": "Helm charts"
                },
                {
                  "url": "https://github.com/opencontainers/image-spec/blob/master/spec.md",
                  "text": "Open Container Initiative (OCI) Image Format Specification"
                },
                {
                  "url": "/en-us/azure/container-registry/container-registry-tasks-overview",
                  "text": "Azure Container Registry Tasks"
                }
              ]
            },
            {
              "number": 3,
              "title": "Explore storage capabilities",
              "url": "https://learn.microsoft.com/en-us/training/modules/publish-container-image-to-azure-container-registry/3-azure-container-registry-storage",
              "href": "3-azure-container-registry-storage",
              "content": "Read in English\nAdd\nAdd to plan\nExplore storage capabilities\nCompleted\n3 minutes\nAll Azure Container Registry tiers benefit from advanced Azure storage features like encryption-at-rest for image data security and geo-redundancy for image data protection.\nEncryption-at-rest:\nAll container images and other artifacts in your registry are encrypted at rest. Azure automatically encrypts an image before storing it, and decrypts it on-the-fly when you or your applications and services pull the image. Optionally apply an extra encryption layer with a customer-managed key.\nRegional storage:\nAzure Container Registry stores data in the region where the registry is created, to help customers meet data residency and compliance requirements. In all regions except Brazil South and Southeast Asia, Azure might also store registry data in a paired region in the same geography. In the Brazil South and Southeast Asia regions, registry data is always confined to the region, to accommodate data residency requirements for those regions.\nIf a regional outage occurs, the registry data might become unavailable and isn't automatically recovered. Customers who wish to have their registry data stored in multiple regions for better performance across different geographies, or who wish to have resiliency in a regional outage event, should enable geo-replication.\nGeo-replication:\nFor scenarios requiring high-availability assurance, consider using the geo-replication feature of Premium registries. Geo-replication helps guard against losing access to your registry in a regional failure event. Geo-replication provides other benefits, too, like network-close image storage for faster pushes and pulls in distributed development or deployment scenarios.\nZone redundancy:\nA feature of the Premium service tier, zone redundancy uses Azure availability zones to replicate your registry to a minimum of three separate zones in each enabled region.\nScalable storage:\nAzure Container Registry allows you to create as many repositories, images, layers, or tags as you need, up to the registry\nstorage limit\n.\nHigh numbers of repositories and tags can impact the performance of your registry. Periodically delete unused repositories, tags, and images as part of your registry maintenance routine. Deleted registry resources like repositories, images, and tags\ncan't\nbe recovered after deletion.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Explore storage capabilities"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                },
                {
                  "url": "/en-us/azure/container-registry/container-registry-skus#service-tier-features-and-limits",
                  "text": "storage limit"
                }
              ]
            },
            {
              "number": 4,
              "title": "Build and manage containers with tasks",
              "url": "https://learn.microsoft.com/en-us/training/modules/publish-container-image-to-azure-container-registry/4-azure-container-registry-tasks",
              "href": "4-azure-container-registry-tasks",
              "content": "Read in English\nAdd\nAdd to plan\nBuild and manage containers with tasks\nCompleted\n3 minutes\nAzure Container Registry (ACR) tasks are a suite of features that:\nProvide cloud-based container image building for platforms like Linux, Windows, and Advanced RISC Machines (Arm).\nExtend the early parts of an application development cycle to the cloud with on-demand container image builds.\nEnable automated builds triggered by source code updates, updates to a container's base image, or timers.\nTask scenarios\nACR Tasks supports several scenarios to build and maintain container images and other artifacts.\nQuick task\n- Build and push a single container image to a container registry on-demand, in Azure, without needing a local Docker Engine installation. Think\ndocker build\n,\ndocker push\nin the cloud.\nAutomatically triggered tasks\n- Enable one or more\ntriggers\nto build an image:\nTrigger on source code update\nTrigger on base image update\nTrigger on a schedule\nMulti-step task\n- Extend the single image build-and-push capability of ACR Tasks with multi-step, multi-container-based workflows.\nEach ACR Task has an associated source code context - the location of a set of source files used to build a container image or other artifact. Example contexts include a Git repository or a local filesystem.\nQuick task\nThe\ninner-loop\ndevelopment cycle is the iterative process of writing code, building, and testing your application before committing to source control. It's really the beginning of container lifecycle management.\nACR Tasks's quick task feature can provide an integrated development experience by offloading your container image builds to Azure. With quick tasks, you can verify your automated build definitions and catch potential problems before committing your code.\nUsing the familiar\ndocker build\nformat, the\naz acr build\ncommand in the Azure CLI takes a context (the set of files to build), sends it to ACR Tasks and, by default, pushes the built image to its registry upon completion.\nTrigger task on source code update\nTrigger a container image build or multi-step task when code is committed, or a pull request is made or updated, to a Git repository in GitHub or Azure DevOps Services. For example, configure a build task with the Azure CLI command\naz acr task create\nby specifying a Git repository and optionally a branch and Dockerfile. When your team updates code in the repository, an ACR Tasks-created webhook triggers a build of the container image defined in the repo.\nTrigger on base image update\nYou can set up an ACR task to track a dependency on a base image when it builds an application image. When the updated base image is pushed to your registry, or a base image is updated in a public repo such as in Docker Hub, ACR Tasks can automatically build any application images based on it.\nSchedule a task\nOptionally schedule a task by setting up one or more timer triggers when you create or update the task. Scheduling a task is useful for running container workloads on a defined schedule, or running maintenance operations or tests on images pushed regularly to your registry.\nMulti-step tasks\nMulti-step tasks, defined in a\nYAML file\nspecify individual build and push operations for container images or other artifacts. They can also define the execution of one or more containers, with each step using the container as its execution environment. For example, you can create a multi-step task that automates the following actions:\nBuild a web application image\nRun the web application container\nBuild a web application test image\nRun the web application test container, which performs tests against the running application container\nIf the tests pass, build a Helm chart archive package\nPerform a\nhelm upgrade\nusing the new Helm chart archive package\nImage platforms\nBy default, ACR Tasks builds images for the Linux OS and the amd64 architecture. Specify the\n--platform\ntag to build Windows images or Linux images for other architectures. Specify the OS and optionally a supported architecture in OS/architecture format (for example,\n--platform Linux/arm\n). For Azure Resource Manager architectures, optionally specify a variant in OS/architecture/variant format (for example,\n--platform Linux/arm64/v8\n):\nOS\nArchitecture\nLinux\nAMD64\nArm\nArm64\n386\nWindows\nAMD64\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Build and manage containers with tasks"
                },
                {
                  "level": 2,
                  "text": "Task scenarios"
                },
                {
                  "level": 2,
                  "text": "Quick task"
                },
                {
                  "level": 2,
                  "text": "Trigger task on source code update"
                },
                {
                  "level": 2,
                  "text": "Trigger on base image update"
                },
                {
                  "level": 2,
                  "text": "Schedule a task"
                },
                {
                  "level": 2,
                  "text": "Multi-step tasks"
                },
                {
                  "level": 2,
                  "text": "Image platforms"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [
                "docker build",
                "docker push",
                "docker build",
                "az acr task create",
                "helm upgrade",
                "--platform Linux/arm",
                "--platform Linux/arm64/v8"
              ],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                },
                {
                  "url": "/en-us/cli/azure/acr#az-acr-build",
                  "text": "az acr build"
                },
                {
                  "url": "/en-us/azure/container-registry/container-registry-tasks-reference-yaml",
                  "text": "YAML file"
                }
              ]
            },
            {
              "number": 5,
              "title": "Explore elements of a Dockerfile",
              "url": "https://learn.microsoft.com/en-us/training/modules/publish-container-image-to-azure-container-registry/5-dockerfile-components",
              "href": "5-dockerfile-components",
              "content": "Read in English\nAdd\nAdd to plan\nExplore elements of a Dockerfile\nCompleted\n3 minutes\nA Dockerfile is a script that contains a series of instructions that are used to build a Docker image. Dockerfiles typically include the following information:\nThe base or parent image we use to create the new image\nCommands to update the base OS and install other software\nBuild artifacts to include, such as a developed application\nServices to expose, such a storage and network configuration\nCommand to run when the container is launched\nCreate a Dockerfile\nThe first step in creating a Dockerfile is choosing a base image that serves as the foundation for your application. For example, if you're building a .NET application, you might choose a Microsoft .NET image as your base.\n# Use the .NET 6 runtime as a base image\nFROM mcr.microsoft.com/dotnet/runtime:6.0\n\n# Set the working directory to /app\nWORKDIR /app\n\n# Copy the contents of the published app to the container's /app directory\nCOPY bin/Release/net6.0/publish/ .\n\n# Expose port 80 to the outside world\nEXPOSE 80\n\n# Set the command to run when the container starts\nCMD [\"dotnet\", \"MyApp.dll\"]\nLet's go through each line to see what it does:\nFROM mcr.microsoft.com/dotnet/runtime:6.0\n: This command sets the base image to the .NET 6 runtime, which is needed to run .NET 6 apps.\nWORKDIR /app\n: Sets the working directory to\n/app\n, which is where app files are copied.\nCOPY bin/Release/net6.0/publish/ .\n: Copies the contents of the published app to the container's\n/app\ndirectory. We assume that the .NET 6 app is built and published to the\nbin/Release/net6.0/publish\ndirectory.\nEXPOSE 80\n: Exposes port 80, which is the default HTTP port, to the outside world. Change this line accordingly if your app listens on a different port.\nCMD [\"dotnet\", \"MyApp.dll\"]\n: The command to run when the container starts. In this case, we're running the dotnet command with the name of our app's DLL file (\nMyApp.dll\n). Change this line to match your apps name and entry point.\nWe're not going to cover the Dockerfile file specification. Visit the\nDockerfile reference\nfor more information. Each of these steps creates a cached container image as we build the final container image. These temporary images are layered on top of the previous and presented as single image once all steps complete.\nResources\nDocker run reference (CLI)\nhttps://docs.docker.com/engine/reference/run/\nDocker build reference\nhttps://docs.docker.com/engine/reference/commandline/build/\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Explore elements of a Dockerfile"
                },
                {
                  "level": 2,
                  "text": "Create a Dockerfile"
                },
                {
                  "level": 2,
                  "text": "Resources"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [
                "# Use the .NET 6 runtime as a base image\nFROM mcr.microsoft.com/dotnet/runtime:6.0\n\n# Set the working directory to /app\nWORKDIR /app\n\n# Copy the contents of the published app to the container's /app directory\nCOPY bin/Release/net6.0/publish/ .\n\n# Expose port 80 to the outside world\nEXPOSE 80\n\n# Set the command to run when the container starts\nCMD [\"dotnet\", \"MyApp.dll\"]",
                "# Use the .NET 6 runtime as a base image\nFROM mcr.microsoft.com/dotnet/runtime:6.0\n\n# Set the working directory to /app\nWORKDIR /app\n\n# Copy the contents of the published app to the container's /app directory\nCOPY bin/Release/net6.0/publish/ .\n\n# Expose port 80 to the outside world\nEXPOSE 80\n\n# Set the command to run when the container starts\nCMD [\"dotnet\", \"MyApp.dll\"]",
                "FROM mcr.microsoft.com/dotnet/runtime:6.0",
                "WORKDIR /app",
                "COPY bin/Release/net6.0/publish/ .",
                "bin/Release/net6.0/publish",
                "CMD [\"dotnet\", \"MyApp.dll\"]"
              ],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                },
                {
                  "url": "https://docs.docker.com/engine/reference/builder/",
                  "text": "Dockerfile reference"
                },
                {
                  "url": "https://docs.docker.com/engine/reference/run/",
                  "text": "https://docs.docker.com/engine/reference/run/"
                },
                {
                  "url": "https://docs.docker.com/engine/reference/commandline/build/",
                  "text": "https://docs.docker.com/engine/reference/commandline/build/"
                }
              ]
            },
            {
              "number": 6,
              "title": "Exercise - Build and run a container image with Azure Container Registry Tasks",
              "url": "https://learn.microsoft.com/en-us/training/modules/publish-container-image-to-azure-container-registry/6-build-run-image-azure-container-registry",
              "href": "6-build-run-image-azure-container-registry",
              "content": "Read in English\nAdd\nAdd to plan\nExercise - Build and run a container image with Azure Container Registry Tasks\nCompleted\n20 minutes\nIn this exercise, you build a container image from your application code and push it to Azure Container Registry (ACR) using the Azure CLI. You learn how to prepare your app for containerization, create an ACR instance, and store your container image in Azure.\nTasks performed in this exercise:\nCreate an Azure Container Registry resource\nBuild and push an image from a Dockerfile\nVerify the results\nRun the image in the Azure Container Registry\nThis exercise takes approximately\n20\nminutes to complete.\nBefore you start\nTo complete the exercise, you need:\nAn Azure subscription. If you don't already have one, you can\nsign up for one\n.\nGet started\nSelect the\nLaunch Exercise\nbutton to open the exercise instructions in a new browser window. When you're finished with the exercise, return here to:\nComplete the module\nEarn a badge for completing this module\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Exercise - Build and run a container image with Azure Container Registry Tasks"
                },
                {
                  "level": 2,
                  "text": "Before you start"
                },
                {
                  "level": 2,
                  "text": "Get started"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [
                {
                  "src": "../../wwl-azure/publish-container-image-to-azure-container-registry/media/launch-exercise.png",
                  "absolute_url": "https://learn.microsoft.com/en-us/wwl-azure/publish-container-image-to-azure-container-registry/media/launch-exercise.png",
                  "alt_text": "Button to launch exercise.",
                  "title": "",
                  "filename": "launch-exercise.png",
                  "image_type": "icon",
                  "context": {
                    "preceding_heading": "Get started",
                    "following_text": "Was this page helpful?",
                    "figure_caption": "",
                    "parent_section": ""
                  }
                }
              ],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                },
                {
                  "url": "https://azure.microsoft.com/",
                  "text": "sign up for one"
                }
              ]
            },
            {
              "number": 7,
              "title": "Module assessment",
              "url": "https://learn.microsoft.com/en-us/training/modules/publish-container-image-to-azure-container-registry/7-knowledge-check",
              "href": "7-knowledge-check",
              "content": "Read in English\nAdd\nAdd to plan\nModule assessment\nCompleted\n3 minutes\nCheck your knowledge\n1.\nWhich of the following Azure Container Registry options support geo-replication to manage a single registry across multiple regions?\nBasic\nStandard\nPremium\n2.\nWhich Azure container registry tiers benefit from encryption-at-rest?\nBasic, Standard, and Premium\nBasic and Standard only\nPremium only\nYou must answer all questions before checking your work.\nYou must answer all questions before checking your work.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Module assessment"
                },
                {
                  "level": 2,
                  "text": "Check your knowledge"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 8,
              "title": "Summary",
              "url": "https://learn.microsoft.com/en-us/training/modules/publish-container-image-to-azure-container-registry/8-summary",
              "href": "8-summary",
              "content": "Read in English\nAdd\nAdd to plan\nSummary\nCompleted\n3 minutes\nIn this module, you learned how to:\nExplain the features and benefits Azure Container Registry (ACR) offers\nDescribe how to use ACR Tasks to automate builds and deployments\nExplain the elements in a Dockerfile\nBuild and run an image in the ACR using Azure CLI commands\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Summary"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            }
          ]
        },
        {
          "title": "Run container images in Azure Container Instances",
          "url": "https://learn.microsoft.com/en-us/training/modules/create-run-container-images-azure-container-instances/",
          "description": "",
          "learning_objectives": [],
          "prerequisites": [],
          "units": [
            {
              "number": 1,
              "title": "Introduction",
              "url": "https://learn.microsoft.com/en-us/training/modules/create-run-container-images-azure-container-instances/1-introduction",
              "href": "1-introduction",
              "content": "Read in English\nAdd\nAdd to plan\nIntroduction\nCompleted\n3 minutes\nAzure Container Instances (ACI) offers the fastest and simplest way to run a container in Azure, without having to manage any virtual machines and without having to adopt a higher-level service.\nAfter completing this module, you'll be able to:\nDescribe the benefits of Azure Container Instances and how resources are grouped\nDeploy a container instance in Azure by using the Azure CLI\nStart and stop containers using policies\nSet environment variables in your container instances\nMount file shares in your container instances\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Introduction"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 2,
              "title": "Explore Azure Container Instances",
              "url": "https://learn.microsoft.com/en-us/training/modules/create-run-container-images-azure-container-instances/2-azure-container-instances-overview",
              "href": "2-azure-container-instances-overview",
              "content": "Read in English\nAdd\nAdd to plan\nExplore Azure Container Instances\nCompleted\n3 minutes\nAzure Container Instances (ACI) is a great solution for any scenario that can operate in isolated containers, including simple applications, task automation, and build jobs. Here are some of the benefits:\nFast startup\n: ACI can start containers in Azure in seconds, without the need to create and manage a virtual machine (VM)\nContainer access\n: ACI enables exposing your container groups directly to the internet with an IP address and a fully qualified domain name (FQDN)\nHypervisor-level security\n: Isolate your application as completely as it would be in a VM\nCustomer data\n: The ACI service stores the minimum customer data required to ensure your container groups are running as expected\nCustom sizes\n: ACI provides optimum utilization by allowing exact specifications of CPU cores and memory\nPersistent storage\n: Mount Azure Files shares directly to a container to retrieve and persist state\nLinux and Windows\n: Schedule both Windows and Linux containers using the same API.\nFor scenarios where you need full container orchestration, including service discovery across multiple containers, automatic scaling, and coordinated application upgrades, we recommend\nAzure Kubernetes Service (AKS)\n.\nContainer groups\nThe top-level resource in Azure Container Instances is the\ncontainer group\n. A container group is a collection of containers that get scheduled on the same host machine. Containers in a container group share a lifecycle, resources, local network, and storage volumes. It's similar in concept to a\npod\nin Kubernetes.\nThe following diagram shows an example of a container group that includes multiple containers:\nThis example container group:\nIs scheduled on a single host machine.\nIs assigned a DNS name label.\nExposes a single public IP address, with one exposed port.\nConsists of two containers. One container listens on port 80, while the other listens on port 5000.\nIncludes two Azure file shares as volume mounts, and each container mounts one of the shares locally.\nNote\nMulti-container groups currently support only Linux containers. For Windows containers, Azure Container Instances only supports deployment of a single instance.\nDeployment\nThere are two common ways to deploy a multi-container group: use a Resource Manager template or a YAML file. A Resource Manager template is recommended when you need to deploy more Azure service resources when you deploy the container instances. Due to the YAML format's more concise nature, a YAML file is recommended when your deployment includes only container instances.\nResource allocation\nAzure Container Instances allocates resources such as CPUs, memory, and optionally GPUs (preview) to a container group by adding the resource requests of the instances in the group. If you create a container group with two instances, each requesting one CPU, then the container group is allocated two CPUs.\nNetworking\nContainer groups share an IP address and a port namespace on that IP address. To enable external clients to reach a container within the group, you must expose the port on the IP address and from the container. Because containers within the group share a port namespace, port mapping isn't supported. Containers within a group can reach each other via localhost on the ports that they exposed, even if those ports aren't exposed externally on the group's IP address.\nStorage\nYou can specify external volumes to mount within a container group. You can map those volumes into specific paths within the individual containers in a group. Supported volumes include:\nAzure file share\nSecret\nEmpty directory\nCloned git repo\nCommon scenarios\nMulti-container groups are useful in cases where you want to divide a single functional task into a few container images. An image might be delivered by different teams and have separate resource requirements.\nExample usage could include:\nA container serving a web application and a container pulling the latest content from source control.\nAn application container and a logging container. The logging container collects the logs and metrics output by the main application and writes them to long-term storage.\nAn application container and a monitoring container. The monitoring container periodically makes a request to the application to ensure that it's running and responding correctly, and raises an alert if it's not.\nA front-end container and a back-end container. The front end might serve a web application, with the back end running a service to retrieve data.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Explore Azure Container Instances"
                },
                {
                  "level": 2,
                  "text": "Container groups"
                },
                {
                  "level": 2,
                  "text": "Deployment"
                },
                {
                  "level": 2,
                  "text": "Resource allocation"
                },
                {
                  "level": 2,
                  "text": "Networking"
                },
                {
                  "level": 2,
                  "text": "Storage"
                },
                {
                  "level": 2,
                  "text": "Common scenarios"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [
                {
                  "src": "../../wwl-azure/create-run-container-images-azure-container-instances/media/container-groups-example.png",
                  "absolute_url": "https://learn.microsoft.com/en-us/wwl-azure/create-run-container-images-azure-container-instances/media/container-groups-example.png",
                  "alt_text": "Example container group with two containers, one listening on port 80 and the other listening on port 5000.",
                  "title": "",
                  "filename": "container-groups-example.png",
                  "image_type": "code_example",
                  "context": {
                    "preceding_heading": "Container groups",
                    "following_text": "This example container group:",
                    "figure_caption": "",
                    "parent_section": ""
                  }
                }
              ],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                },
                {
                  "url": "/en-us/azure/aks/",
                  "text": "Azure Kubernetes Service (AKS)"
                }
              ]
            },
            {
              "number": 3,
              "title": "Exercise - Deploy a container to Azure Container Instances using Azure CLI commands",
              "url": "https://learn.microsoft.com/en-us/training/modules/create-run-container-images-azure-container-instances/3-run-azure-container-instances-cloud-shell",
              "href": "3-run-azure-container-instances-cloud-shell",
              "content": "Read in English\nAdd\nAdd to plan\nExercise - Deploy a container to Azure Container Instances using Azure CLI commands\nCompleted\n15 minutes\nIn this exercise, you deploy and run a container in Azure Container Instances (ACI) using the Azure CLI. You learn how to create a container group, specify container settings, and verify that your containerized application is running in the cloud.\nTasks performed in this exercise:\nCreate Azure Container Instance resources in Azure\nCreate and deploy a container\nVerify the container is running\nThis exercise takes approximately\n15\nminutes to complete.\nBefore you start\nTo complete the exercise, you need:\nAn Azure subscription. If you don't already have one, you can\nsign up for one\n.\nGet started\nSelect the\nLaunch Exercise\nbutton to open the exercise instructions in a new browser window. When you're finished with the exercise, return here to:\nComplete the module\nEarn a badge for completing this module\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Exercise - Deploy a container to Azure Container Instances using Azure CLI commands"
                },
                {
                  "level": 2,
                  "text": "Before you start"
                },
                {
                  "level": 2,
                  "text": "Get started"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [
                {
                  "src": "../../wwl-azure/create-run-container-images-azure-container-instances/media/launch-exercise.png",
                  "absolute_url": "https://learn.microsoft.com/en-us/wwl-azure/create-run-container-images-azure-container-instances/media/launch-exercise.png",
                  "alt_text": "Button to launch exercise.",
                  "title": "",
                  "filename": "launch-exercise.png",
                  "image_type": "icon",
                  "context": {
                    "preceding_heading": "Get started",
                    "following_text": "Was this page helpful?",
                    "figure_caption": "",
                    "parent_section": ""
                  }
                }
              ],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                },
                {
                  "url": "https://azure.microsoft.com/",
                  "text": "sign up for one"
                }
              ]
            },
            {
              "number": 4,
              "title": "Run containerized tasks with restart policies",
              "url": "https://learn.microsoft.com/en-us/training/modules/create-run-container-images-azure-container-instances/4-run-containerized-tasks-restart-policies",
              "href": "4-run-containerized-tasks-restart-policies",
              "content": "Read in English\nAdd\nAdd to plan\nRun containerized tasks with restart policies\nCompleted\n3 minutes\nThe ease and speed of deploying containers in Azure Container Instances provides a compelling platform for executing run-once tasks like build, test, and image rendering in a container instance.\nWith a configurable restart policy, you can specify that your containers are stopped when their processes are completed. Because container instances are billed by the second, you're charged only for the compute resources used while the container executing your task is running.\nContainer restart policy\nWhen you create a container group in Azure Container Instances, you can specify one of three restart policy settings.\nRestart policy\nDescription\nAlways\nContainers in the container group are always restarted. This is the\ndefault\nsetting applied when no restart policy is specified at container creation.\nNever\nContainers in the container group are never restarted. The containers run at most once.\nOnFailure\nContainers in the container group are restarted only when the process executed in the container fails (when it terminates with a nonzero exit code). The containers are run at least once.\nSpecify a restart policy\nSpecify the\n--restart-policy\nparameter when you call\naz container create\n.\naz container create \\\n    --resource-group myResourceGroup \\\n    --name mycontainer \\\n    --image mycontainerimage \\\n    --restart-policy OnFailure\nRun to completion\nAzure Container Instances starts the container, and then stops it when its application, or script,  exits. When Azure Container Instances stops a container whose restart policy is\nNever\nor\nOnFailure\n, the container's status is set to\nTerminated\n.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Run containerized tasks with restart policies"
                },
                {
                  "level": 2,
                  "text": "Container restart policy"
                },
                {
                  "level": 2,
                  "text": "Specify a restart policy"
                },
                {
                  "level": 2,
                  "text": "Run to completion"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [
                "--restart-policy",
                "az container create",
                "az container create \\\n    --resource-group myResourceGroup \\\n    --name mycontainer \\\n    --image mycontainerimage \\\n    --restart-policy OnFailure",
                "az container create \\\n    --resource-group myResourceGroup \\\n    --name mycontainer \\\n    --image mycontainerimage \\\n    --restart-policy OnFailure"
              ],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 5,
              "title": "Set environment variables in container instances",
              "url": "https://learn.microsoft.com/en-us/training/modules/create-run-container-images-azure-container-instances/5-set-environment-variables-azure-container-instances",
              "href": "5-set-environment-variables-azure-container-instances",
              "content": "Read in English\nAdd\nAdd to plan\nSet environment variables in container instances\nCompleted\n3 minutes\nSetting environment variables in your container instances allows you to provide dynamic configuration of the application or script run by the container. These environment variables are similar to the\n--env\ncommand-line argument to\ndocker run\n.\nIf you need to pass secrets as environment variables, Azure Container Instances supports secure values for both Windows and Linux containers.\nIn the following example, two variables are passed to the container when it's created. The example is assuming you're running the CLI in a Bash shell or Cloud Shell, if you use the Windows Command Prompt, specify the variables with double-quotes, such as\n--environment-variables \"NumWords\"=\"5\" \"MinLength\"=\"8\"\n.\naz container create \\\n    --resource-group myResourceGroup \\\n    --name mycontainer2 \\\n    --image mcr.microsoft.com/azuredocs/aci-wordcount:latest \n    --restart-policy OnFailure \\\n    --environment-variables 'NumWords'='5' 'MinLength'='8'\\\nSecure values\nObjects with secure values are intended to hold sensitive information like passwords or keys for your application. Using secure values for environment variables is both safer and more flexible than including it in your container's image.\nEnvironment variables with secure values aren't visible in your container's properties. Their values can be accessed only from within the container. For example, container properties viewed in the Azure portal or Azure CLI display only a secure variable's name, not its value.\nSet a secure environment variable by specifying the\nsecureValue\nproperty instead of the regular\nvalue\nfor the variable's type. The two variables defined in the following YAML demonstrate the two variable types.\napiVersion: 2018-10-01\nlocation: eastus\nname: securetest\nproperties:\n  containers:\n  - name: mycontainer\n    properties:\n      environmentVariables:\n        - name: 'NOTSECRET'\n          value: 'my-exposed-value'\n        - name: 'SECRET'\n          secureValue: 'my-secret-value'\n      image: nginx\n      ports: []\n      resources:\n        requests:\n          cpu: 1.0\n          memoryInGB: 1.5\n  osType: Linux\n  restartPolicy: Always\ntags: null\ntype: Microsoft.ContainerInstance/containerGroups\nYou would run the following command to deploy the container group with YAML:\naz container create --resource-group myResourceGroup \\\n    --file secure-env.yaml \\\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Set environment variables in container instances"
                },
                {
                  "level": 2,
                  "text": "Secure values"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [
                "--environment-variables \"NumWords\"=\"5\" \"MinLength\"=\"8\"",
                "az container create \\\n    --resource-group myResourceGroup \\\n    --name mycontainer2 \\\n    --image mcr.microsoft.com/azuredocs/aci-wordcount:latest \n    --restart-policy OnFailure \\\n    --environment-variables 'NumWords'='5' 'MinLength'='8'\\",
                "az container create \\\n    --resource-group myResourceGroup \\\n    --name mycontainer2 \\\n    --image mcr.microsoft.com/azuredocs/aci-wordcount:latest \n    --restart-policy OnFailure \\\n    --environment-variables 'NumWords'='5' 'MinLength'='8'\\",
                "secureValue",
                "apiVersion: 2018-10-01\nlocation: eastus\nname: securetest\nproperties:\n  containers:\n  - name: mycontainer\n    properties:\n      environmentVariables:\n        - name: 'NOTSECRET'\n          value: 'my-exposed-value'\n        - name: 'SECRET'\n          secureValue: 'my-secret-value'\n      image: nginx\n      ports: []\n      resources:\n        requests:\n          cpu: 1.0\n          memoryInGB: 1.5\n  osType: Linux\n  restartPolicy: Always\ntags: null\ntype: Microsoft.ContainerInstance/containerGroups",
                "apiVersion: 2018-10-01\nlocation: eastus\nname: securetest\nproperties:\n  containers:\n  - name: mycontainer\n    properties:\n      environmentVariables:\n        - name: 'NOTSECRET'\n          value: 'my-exposed-value'\n        - name: 'SECRET'\n          secureValue: 'my-secret-value'\n      image: nginx\n      ports: []\n      resources:\n        requests:\n          cpu: 1.0\n          memoryInGB: 1.5\n  osType: Linux\n  restartPolicy: Always\ntags: null\ntype: Microsoft.ContainerInstance/containerGroups",
                "az container create --resource-group myResourceGroup \\\n    --file secure-env.yaml \\",
                "az container create --resource-group myResourceGroup \\\n    --file secure-env.yaml \\"
              ],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 6,
              "title": "Mount an Azure file share in Azure Container Instances",
              "url": "https://learn.microsoft.com/en-us/training/modules/create-run-container-images-azure-container-instances/6-mount-azure-file-share-azure-container-instances",
              "href": "6-mount-azure-file-share-azure-container-instances",
              "content": "Read in English\nAdd\nAdd to plan\nMount an Azure file share in Azure Container Instances\nCompleted\n3 minutes\nBy default, Azure Container Instances are stateless. If the container crashes or stops, all of its state is lost. To persist state beyond the lifetime of the container, you must mount a volume from an external store. As shown in this unit, Azure Container Instances can mount an Azure file share created with Azure Files. Azure Files offers fully managed file shares in the cloud that are accessible via the industry standard Server Message Block (SMB) protocol. Using an Azure file share with Azure Container Instances provides file-sharing features similar to using an Azure file share with Azure virtual machines.\nLimitations\nYou can only mount Azure Files shares to Linux containers.\nAzure file share volume mount requires the Linux container run as\nroot\n.\nAzure File share volume mounts are limited to CIFS support.\nDeploy container and mount volume\nTo mount an Azure file share as a volume in a container by using the Azure CLI, specify the share and volume mount point when you create the container with\naz container create\n. Following is an example of the command:\naz container create \\\n    --resource-group $ACI_PERS_RESOURCE_GROUP \\\n    --name hellofiles \\\n    --image mcr.microsoft.com/azuredocs/aci-hellofiles \\\n    --dns-name-label aci-demo \\\n    --ports 80 \\\n    --azure-file-volume-account-name $ACI_PERS_STORAGE_ACCOUNT_NAME \\\n    --azure-file-volume-account-key $STORAGE_KEY \\\n    --azure-file-volume-share-name $ACI_PERS_SHARE_NAME \\\n    --azure-file-volume-mount-path /aci/logs/\nThe\n--dns-name-label\nvalue must be unique within the Azure region where you create the container instance. Update the value in the preceding command if you receive a\nDNS name label\nerror message when you execute the command.\nDeploy container and mount volume - YAML\nYou can also deploy a container group and mount a volume in a container with the Azure CLI and a YAML template. Deploying by YAML template is the preferred method when deploying container groups consisting of multiple containers.\nThe following YAML template defines a container group with one container created with the\naci-hellofiles\nimage. The container mounts the Azure file share\nacishare\ncreated previously as a volume. Following is an example YAML file.\napiVersion: '2019-12-01'\nlocation: eastus\nname: file-share-demo\nproperties:\n  containers:\n  - name: hellofiles\n    properties:\n      environmentVariables: []\n      image: mcr.microsoft.com/azuredocs/aci-hellofiles\n      ports:\n      - port: 80\n      resources:\n        requests:\n          cpu: 1.0\n          memoryInGB: 1.5\n      volumeMounts:\n      - mountPath: /aci/logs/\n        name: filesharevolume\n  osType: Linux\n  restartPolicy: Always\n  ipAddress:\n    type: Public\n    ports:\n      - port: 80\n    dnsNameLabel: aci-demo\n  volumes:\n  - name: filesharevolume\n    azureFile:\n      sharename: acishare\n      storageAccountName: <Storage account name>\n      storageAccountKey: <Storage account key>\ntags: {}\ntype: Microsoft.ContainerInstance/containerGroups\nMount multiple volumes\nTo mount multiple volumes in a container instance, you must deploy using an Azure Resource Manager template or a YAML file. To use a template or YAML file, provide the share details and define the volumes by populating the\nvolumes\narray in the\nproperties\nsection of the template.\nFor example, if you created two Azure Files shares named\nshare1\nand\nshare2\nin storage account\nmyStorageAccount\n, the\nvolumes\narray in a Resource Manager template would appear similar to the following:\n\"volumes\": [{\n  \"name\": \"myvolume1\",\n  \"azureFile\": {\n    \"shareName\": \"share1\",\n    \"storageAccountName\": \"myStorageAccount\",\n    \"storageAccountKey\": \"<storage-account-key>\"\n  }\n},\n{\n  \"name\": \"myvolume2\",\n  \"azureFile\": {\n    \"shareName\": \"share2\",\n    \"storageAccountName\": \"myStorageAccount\",\n    \"storageAccountKey\": \"<storage-account-key>\"\n  }\n}]\nNext, for each container in the container group in which you'd like to mount the volumes, populate the\nvolumeMounts\narray in the\nproperties\nsection of the container definition. For example, this mounts the two volumes,\nmyvolume1\nand\nmyvolume2\n, previously defined:\n\"volumeMounts\": [{\n  \"name\": \"myvolume1\",\n  \"mountPath\": \"/mnt/share1/\"\n},\n{\n  \"name\": \"myvolume2\",\n  \"mountPath\": \"/mnt/share2/\"\n}]\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Mount an Azure file share in Azure Container Instances"
                },
                {
                  "level": 2,
                  "text": "Limitations"
                },
                {
                  "level": 2,
                  "text": "Deploy container and mount volume"
                },
                {
                  "level": 2,
                  "text": "Deploy container and mount volume - YAML"
                },
                {
                  "level": 2,
                  "text": "Mount multiple volumes"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [
                "az container create",
                "az container create \\\n    --resource-group $ACI_PERS_RESOURCE_GROUP \\\n    --name hellofiles \\\n    --image mcr.microsoft.com/azuredocs/aci-hellofiles \\\n    --dns-name-label aci-demo \\\n    --ports 80 \\\n    --azure-file-volume-account-name $ACI_PERS_STORAGE_ACCOUNT_NAME \\\n    --azure-file-volume-account-key $STORAGE_KEY \\\n    --azure-file-volume-share-name $ACI_PERS_SHARE_NAME \\\n    --azure-file-volume-mount-path /aci/logs/",
                "az container create \\\n    --resource-group $ACI_PERS_RESOURCE_GROUP \\\n    --name hellofiles \\\n    --image mcr.microsoft.com/azuredocs/aci-hellofiles \\\n    --dns-name-label aci-demo \\\n    --ports 80 \\\n    --azure-file-volume-account-name $ACI_PERS_STORAGE_ACCOUNT_NAME \\\n    --azure-file-volume-account-key $STORAGE_KEY \\\n    --azure-file-volume-share-name $ACI_PERS_SHARE_NAME \\\n    --azure-file-volume-mount-path /aci/logs/",
                "--dns-name-label",
                "aci-hellofiles",
                "apiVersion: '2019-12-01'\nlocation: eastus\nname: file-share-demo\nproperties:\n  containers:\n  - name: hellofiles\n    properties:\n      environmentVariables: []\n      image: mcr.microsoft.com/azuredocs/aci-hellofiles\n      ports:\n      - port: 80\n      resources:\n        requests:\n          cpu: 1.0\n          memoryInGB: 1.5\n      volumeMounts:\n      - mountPath: /aci/logs/\n        name: filesharevolume\n  osType: Linux\n  restartPolicy: Always\n  ipAddress:\n    type: Public\n    ports:\n      - port: 80\n    dnsNameLabel: aci-demo\n  volumes:\n  - name: filesharevolume\n    azureFile:\n      sharename: acishare\n      storageAccountName: <Storage account name>\n      storageAccountKey: <Storage account key>\ntags: {}\ntype: Microsoft.ContainerInstance/containerGroups",
                "apiVersion: '2019-12-01'\nlocation: eastus\nname: file-share-demo\nproperties:\n  containers:\n  - name: hellofiles\n    properties:\n      environmentVariables: []\n      image: mcr.microsoft.com/azuredocs/aci-hellofiles\n      ports:\n      - port: 80\n      resources:\n        requests:\n          cpu: 1.0\n          memoryInGB: 1.5\n      volumeMounts:\n      - mountPath: /aci/logs/\n        name: filesharevolume\n  osType: Linux\n  restartPolicy: Always\n  ipAddress:\n    type: Public\n    ports:\n      - port: 80\n    dnsNameLabel: aci-demo\n  volumes:\n  - name: filesharevolume\n    azureFile:\n      sharename: acishare\n      storageAccountName: <Storage account name>\n      storageAccountKey: <Storage account key>\ntags: {}\ntype: Microsoft.ContainerInstance/containerGroups",
                "\"volumes\": [{\n  \"name\": \"myvolume1\",\n  \"azureFile\": {\n    \"shareName\": \"share1\",\n    \"storageAccountName\": \"myStorageAccount\",\n    \"storageAccountKey\": \"<storage-account-key>\"\n  }\n},\n{\n  \"name\": \"myvolume2\",\n  \"azureFile\": {\n    \"shareName\": \"share2\",\n    \"storageAccountName\": \"myStorageAccount\",\n    \"storageAccountKey\": \"<storage-account-key>\"\n  }\n}]",
                "\"volumes\": [{\n  \"name\": \"myvolume1\",\n  \"azureFile\": {\n    \"shareName\": \"share1\",\n    \"storageAccountName\": \"myStorageAccount\",\n    \"storageAccountKey\": \"<storage-account-key>\"\n  }\n},\n{\n  \"name\": \"myvolume2\",\n  \"azureFile\": {\n    \"shareName\": \"share2\",\n    \"storageAccountName\": \"myStorageAccount\",\n    \"storageAccountKey\": \"<storage-account-key>\"\n  }\n}]",
                "volumeMounts",
                "\"volumeMounts\": [{\n  \"name\": \"myvolume1\",\n  \"mountPath\": \"/mnt/share1/\"\n},\n{\n  \"name\": \"myvolume2\",\n  \"mountPath\": \"/mnt/share2/\"\n}]",
                "\"volumeMounts\": [{\n  \"name\": \"myvolume1\",\n  \"mountPath\": \"/mnt/share1/\"\n},\n{\n  \"name\": \"myvolume2\",\n  \"mountPath\": \"/mnt/share2/\"\n}]"
              ],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 7,
              "title": "Module assessment",
              "url": "https://learn.microsoft.com/en-us/training/modules/create-run-container-images-azure-container-instances/7-knowledge-check",
              "href": "7-knowledge-check",
              "content": "Read in English\nAdd\nAdd to plan\nModule assessment\nCompleted\n3 minutes\nCheck your knowledge\n1.\nWhich of the following methods is recommended when deploying a multi-container group that includes only containers?\nAzure Resource Management template\nYAML file\naz container creates\ncommand\n2.\nWhat is the purpose of a restart policy in Azure Container Instances?\nTo charge customers more for compute resources used while the container is running.\nTo ensure that containers are never restarted, even if the process fails.\nTo specify when and how containers should be restarted, based on the desired behavior.\n3.\nIf you want to mount multiple volumes, what options are at your disposal for deployment?\nYAML file only\nAzure Resource Manager template and YAML file\nAzure Resource Manager template and PowerShell\nYou must answer all questions before checking your work.\nYou must answer all questions before checking your work.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Module assessment"
                },
                {
                  "level": 2,
                  "text": "Check your knowledge"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [
                "az container creates"
              ],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 8,
              "title": "Summary",
              "url": "https://learn.microsoft.com/en-us/training/modules/create-run-container-images-azure-container-instances/8-summary",
              "href": "8-summary",
              "content": "Read in English\nAdd\nAdd to plan\nSummary\nCompleted\n3 minutes\nIn this module, you learned how to:\nDescribe the benefits of Azure Container Instances and how resources are grouped\nDeploy a container instance in Azure by using the Azure CLI\nStart and stop containers using policies\nSet environment variables in your container instances\nMount file shares in your container instances\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Summary"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            }
          ]
        },
        {
          "title": "Implement Azure Container Apps",
          "url": "https://learn.microsoft.com/en-us/training/modules/implement-azure-container-apps/",
          "description": "",
          "learning_objectives": [],
          "prerequisites": [],
          "units": [
            {
              "number": 1,
              "title": "Introduction",
              "url": "https://learn.microsoft.com/en-us/training/modules/implement-azure-container-apps/1-introduction",
              "href": "1-introduction",
              "content": "Read in English\nAdd\nAdd to plan\nIntroduction\nCompleted\n1 minute\nAzure Container Apps is a serverless container service that supports microservice applications and robust autoscaling capabilities without the overhead of managing complex infrastructure.\nAfter completing this module, you'll be able to:\nDescribe the features benefits of Azure Container Apps\nDeploy container app in Azure by using the Azure CLI\nUtilize Azure Container Apps built-in authentication and authorization\nCreate revisions and implement app secrets\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Introduction"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 2,
              "title": "Explore Azure Container Apps",
              "url": "https://learn.microsoft.com/en-us/training/modules/implement-azure-container-apps/2-explore-azure-container-apps",
              "href": "2-explore-azure-container-apps",
              "content": "Read in English\nAdd\nAdd to plan\nExplore Azure Container Apps\nCompleted\n3 minutes\nAzure Container Apps enables you to run microservices and containerized applications on a serverless platform that runs on top of Azure Kubernetes Service. Common uses of Azure Container Apps include:\nDeploying API endpoints\nHosting background processing applications\nHandling event-driven processing\nRunning microservices\nApplications built on Azure Container Apps can dynamically scale based on: HTTP traffic, event-driven processing, CPU or memory load, and any\nKEDA-supported scaler\n.\nWith Azure Container Apps, you can:\nRun multiple container revisions and manage the container app's application lifecycle.\nAutoscale your apps based on any KEDA-supported scale trigger. Most applications can scale to zero. (Applications that scale on CPU or memory load can't scale to zero.)\nEnable HTTPS ingress without having to manage other Azure infrastructure.\nSplit traffic across multiple versions of an application for Blue/Green deployments and A/B testing scenarios.\nUse internal ingress and service discovery for secure internal-only endpoints with built-in DNS-based service discovery.\nBuild microservices with\nDapr\nand access its rich set of APIs.\nRun containers from any registry, public or private, including Docker Hub and Azure Container Registry (ACR).\nUse the Azure CLI extension, Azure portal, or ARM templates to manage your applications.\nProvide an existing virtual network when creating an environment for your container apps.\nSecurely manage secrets directly in your application.\nMonitor logs using Azure Log Analytics.\nAzure Container Apps environments\nIndividual container apps are deployed to a single Container Apps environment, which acts as a secure boundary around groups of container apps. Container Apps in the same environment are deployed in the same virtual network and write logs to the same Log Analytics workspace. You might provide an existing virtual network when you create an environment.\nReasons to deploy container apps to the same environment include situations when you need to:\nManage related services\nDeploy different applications to the same virtual network\nInstrument\nDapr\napplications that communicate via the Dapr service invocation API\nHave applications to share the same Dapr configuration\nHave applications share the same log analytics workspace\nReasons to deploy container apps to different environments include situations when you want to ensure:\nTwo applications never share the same compute resources\nTwo Dapr applications can't communicate via the Dapr service invocation API\nMicroservices with Azure Container Apps\nMicroservice architectures allow you to independently develop, upgrade, version, and scale core areas of functionality in an overall system. Azure Container Apps provides the foundation for deploying microservices featuring:\nIndependent scaling, versioning, and upgrades\nService discovery\nNative\nDapr\nintegration\nDapr integration\nWhen you implement a system composed of microservices, function calls are spread across the network. To support the distributed nature of microservices, you need to account for failures, retries, and timeouts. While Container Apps features the building blocks for running microservices, use of Dapr provides an even richer microservices programming model. Dapr includes features like observability, pub/sub, and service-to-service invocation with mutual TLS, retries, and more.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Explore Azure Container Apps"
                },
                {
                  "level": 2,
                  "text": "Azure Container Apps environments"
                },
                {
                  "level": 2,
                  "text": "Microservices with Azure Container Apps"
                },
                {
                  "level": 2,
                  "text": "Dapr integration"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                },
                {
                  "url": "https://keda.sh/docs/scalers/",
                  "text": "KEDA-supported scaler"
                },
                {
                  "url": "https://docs.dapr.io/concepts/overview/",
                  "text": "Dapr"
                },
                {
                  "url": "https://docs.dapr.io/concepts/overview/",
                  "text": "Dapr"
                },
                {
                  "url": "https://docs.dapr.io/concepts/overview/",
                  "text": "Dapr"
                }
              ]
            },
            {
              "number": 3,
              "title": "Exercise - Deploy a container to Azure Container Apps with the Azure CLI",
              "url": "https://learn.microsoft.com/en-us/training/modules/implement-azure-container-apps/3-exercise-deploy-app",
              "href": "3-exercise-deploy-app",
              "content": "Read in English\nAdd\nAdd to plan\nExercise - Deploy a container to Azure Container Apps with the Azure CLI\nCompleted\n15 minutes\nIn this exercise, you deploy a containerized application to Azure Container Apps using the Azure CLI. You learn how to create a container app environment, deploy your container, and verify that your application is running in Azure.\nTasks performed in this exercise:\nCreate resources in Azure\nCreate an Azure Container Apps environment\nDeploy a container app to the environment\nThis exercise takes approximately\n15\nminutes to complete.\nBefore you start\nTo complete the exercise, you need:\nAn Azure subscription. If you don't already have one, you can\nsign up for one\n.\nGet started\nSelect the\nLaunch Exercise\nbutton to open the exercise instructions in a new browser window. When you're finished with the exercise, return here to:\nComplete the module\nEarn a badge for completing this module\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Exercise - Deploy a container to Azure Container Apps with the Azure CLI"
                },
                {
                  "level": 2,
                  "text": "Before you start"
                },
                {
                  "level": 2,
                  "text": "Get started"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [
                {
                  "src": "../../wwl-azure/implement-azure-container-apps/media/launch-exercise.png",
                  "absolute_url": "https://learn.microsoft.com/en-us/wwl-azure/implement-azure-container-apps/media/launch-exercise.png",
                  "alt_text": "Button to launch exercise.",
                  "title": "",
                  "filename": "launch-exercise.png",
                  "image_type": "icon",
                  "context": {
                    "preceding_heading": "Get started",
                    "following_text": "Was this page helpful?",
                    "figure_caption": "",
                    "parent_section": ""
                  }
                }
              ],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                },
                {
                  "url": "https://azure.microsoft.com/",
                  "text": "sign up for one"
                }
              ]
            },
            {
              "number": 4,
              "title": "Explore containers in Azure Container Apps",
              "url": "https://learn.microsoft.com/en-us/training/modules/implement-azure-container-apps/4-container-apps-containers",
              "href": "4-container-apps-containers",
              "content": "Read in English\nAdd\nAdd to plan\nExplore containers in Azure Container Apps\nCompleted\n5 minutes\nAzure Container Apps manages the details of Kubernetes and container orchestration for you. Containers in Azure Container Apps can use any runtime, programming language, or development stack of your choice.\nAzure Container Apps supports any Linux-based x86-64 (\nlinux/amd64\n) container image. There's no required base container image, and if a container crashes it automatically restarts.\nConfiguration\nThe following code is an example of the\ncontainers\narray in the\nproperties.template\nsection of a container app resource template. The excerpt shows some of the available configuration options when setting up a container when using Azure Resource Manager (ARM) templates. Changes to the template ARM configuration section trigger a new container app revision.\n\"containers\": [\n  {\n       \"name\": \"main\",\n       \"image\": \"[parameters('container_image')]\",\n    \"env\": [\n      {\n        \"name\": \"HTTP_PORT\",\n        \"value\": \"80\"\n      },\n      {\n        \"name\": \"SECRET_VAL\",\n        \"secretRef\": \"mysecret\"\n      }\n    ],\n    \"resources\": {\n      \"cpu\": 0.5,\n      \"memory\": \"1Gi\"\n    },\n    \"volumeMounts\": [\n      {\n        \"mountPath\": \"/myfiles\",\n        \"volumeName\": \"azure-files-volume\"\n      }\n    ]\n    \"probes\":[\n        {\n            \"type\":\"liveness\",\n            \"httpGet\":{\n            \"path\":\"/health\",\n            \"port\":8080,\n            \"httpHeaders\":[\n                {\n                    \"name\":\"Custom-Header\",\n                    \"value\":\"liveness probe\"\n                }]\n            },\n            \"initialDelaySeconds\":7,\n            \"periodSeconds\":3\n// file is truncated for brevity\nMultiple containers\nYou can define multiple containers in a single container app to implement the\nsidecar pattern\n. The containers in a container app share hard disk and network resources and experience the same application lifecycle.\nExamples of sidecar containers include:\nAn agent that reads logs from the primary app container on a shared volume and forwards them to a logging service.\nA background process that refreshes a cache used by the primary app container in a shared volume.\nNote\nRunning multiple containers in a single container app is an advanced use case. In most situations where you want to run multiple containers, such as when implementing a microservice architecture, deploy each service as a separate container app.\nTo run multiple containers in a container app, add more than one container in the containers array of the container app template.\nContainer registries\nYou can deploy images hosted on private registries by providing credentials in the Container Apps configuration.\nTo use a container registry, you define the required fields in registries array in the properties.configuration section of the container app resource template. The passwordSecretRef field identifies the name of the secret in the secrets array name where you defined the password.\n{\n  ...\n  \"registries\": [{\n    \"server\": \"docker.io\",\n    \"username\": \"my-registry-user-name\",\n    \"passwordSecretRef\": \"my-password-secret-name\"\n  }]\n}\nWith the registry information added, the saved credentials can be used to pull a container image from the private registry when your app is deployed.\nLimitations\nAzure Container Apps has the following limitations:\nPrivileged containers\n: Azure Container Apps can't run privileged containers. If your program attempts to run a process that requires root access, the application inside the container experiences a runtime error.\nOperating system\n: Linux-based (\nlinux/amd64\n) container images are required.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Explore containers in Azure Container Apps"
                },
                {
                  "level": 2,
                  "text": "Configuration"
                },
                {
                  "level": 2,
                  "text": "Multiple containers"
                },
                {
                  "level": 2,
                  "text": "Container registries"
                },
                {
                  "level": 2,
                  "text": "Limitations"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [
                "linux/amd64",
                "properties.template",
                "\"containers\": [\n  {\n       \"name\": \"main\",\n       \"image\": \"[parameters('container_image')]\",\n    \"env\": [\n      {\n        \"name\": \"HTTP_PORT\",\n        \"value\": \"80\"\n      },\n      {\n        \"name\": \"SECRET_VAL\",\n        \"secretRef\": \"mysecret\"\n      }\n    ],\n    \"resources\": {\n      \"cpu\": 0.5,\n      \"memory\": \"1Gi\"\n    },\n    \"volumeMounts\": [\n      {\n        \"mountPath\": \"/myfiles\",\n        \"volumeName\": \"azure-files-volume\"\n      }\n    ]\n    \"probes\":[\n        {\n            \"type\":\"liveness\",\n            \"httpGet\":{\n            \"path\":\"/health\",\n            \"port\":8080,\n            \"httpHeaders\":[\n                {\n                    \"name\":\"Custom-Header\",\n                    \"value\":\"liveness probe\"\n                }]\n            },\n            \"initialDelaySeconds\":7,\n            \"periodSeconds\":3\n// file is truncated for brevity",
                "\"containers\": [\n  {\n       \"name\": \"main\",\n       \"image\": \"[parameters('container_image')]\",\n    \"env\": [\n      {\n        \"name\": \"HTTP_PORT\",\n        \"value\": \"80\"\n      },\n      {\n        \"name\": \"SECRET_VAL\",\n        \"secretRef\": \"mysecret\"\n      }\n    ],\n    \"resources\": {\n      \"cpu\": 0.5,\n      \"memory\": \"1Gi\"\n    },\n    \"volumeMounts\": [\n      {\n        \"mountPath\": \"/myfiles\",\n        \"volumeName\": \"azure-files-volume\"\n      }\n    ]\n    \"probes\":[\n        {\n            \"type\":\"liveness\",\n            \"httpGet\":{\n            \"path\":\"/health\",\n            \"port\":8080,\n            \"httpHeaders\":[\n                {\n                    \"name\":\"Custom-Header\",\n                    \"value\":\"liveness probe\"\n                }]\n            },\n            \"initialDelaySeconds\":7,\n            \"periodSeconds\":3\n// file is truncated for brevity",
                "{\n  ...\n  \"registries\": [{\n    \"server\": \"docker.io\",\n    \"username\": \"my-registry-user-name\",\n    \"passwordSecretRef\": \"my-password-secret-name\"\n  }]\n}",
                "{\n  ...\n  \"registries\": [{\n    \"server\": \"docker.io\",\n    \"username\": \"my-registry-user-name\",\n    \"passwordSecretRef\": \"my-password-secret-name\"\n  }]\n}",
                "linux/amd64"
              ],
              "images": [
                {
                  "src": "../../wwl-azure/implement-azure-container-apps/media/azure-container-apps-containers.png",
                  "absolute_url": "https://learn.microsoft.com/en-us/wwl-azure/implement-azure-container-apps/media/azure-container-apps-containers.png",
                  "alt_text": "Diagram showing how containers for an Azure Container App are grouped together in pods inside revision snapshots.",
                  "title": "",
                  "filename": "azure-container-apps-containers.png",
                  "image_type": "diagram",
                  "context": {
                    "preceding_heading": "Explore containers in Azure Container Apps",
                    "following_text": "Azure Container Apps supports any Linux-based x86-64 (linux/amd64) container image. There's no required base container image, and if a container crashes it automatically restarts.",
                    "figure_caption": "",
                    "parent_section": ""
                  }
                }
              ],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                },
                {
                  "url": "/en-us/azure/architecture/patterns/sidecar",
                  "text": "sidecar pattern"
                }
              ]
            },
            {
              "number": 5,
              "title": "Implement authentication and authorization in Azure Container Apps",
              "url": "https://learn.microsoft.com/en-us/training/modules/implement-azure-container-apps/5-container-apps-authentication",
              "href": "5-container-apps-authentication",
              "content": "Read in English\nAdd\nAdd to plan\nImplement authentication and authorization in Azure Container Apps\nCompleted\n5 minutes\nAzure Container Apps provides built-in authentication and authorization features to secure your external ingress-enabled container app with minimal or no code. The built-in authentication feature for Container Apps can save you time and effort by providing out-of-the-box authentication with federated identity providers, allowing you to focus on the rest of your application.\nAzure Container Apps provides access to various built-in authentication providers.\nThe built-in auth features donât require any particular language, SDK, security expertise, or even any code that you have to write.\nThis feature should only be used with HTTPS. Ensure\nallowInsecure\nis disabled on your container app's ingress configuration. You can configure your container app for authentication with or without restricting access to your site content and APIs.\nTo restrict app access only to authenticated users, set its\nRestrict access\nsetting to\nRequire authentication\n.\nTo authenticate but not restrict access, set its\nRestrict access\nsetting to\nAllow unauthenticated\naccess.\nIdentity providers\nContainer Apps uses federated identity, in which a third-party identity provider manages the user identities and authentication flow for you. The following identity providers are available by default:\nProvider\nSign-in endpoint\nHow-To guidance\nMicrosoft Identity Platform\n/.auth/login/aad\nMicrosoft Identity Platform\nFacebook\n/.auth/login/facebook\nFacebook\nGitHub\n/.auth/login/github\nGitHub\nGoogle\n/.auth/login/google\nGoogle\nX\n/.auth/login/twitter\nX\nAny OpenID Connect provider\n/.auth/login/<providerName>\nOpenID Connect\nWhen you use one of these providers, the sign-in endpoint is available for user authentication and authentication token validation from the provider. You can provide your users with any number of these provider options.\nFeature architecture\nThe authentication and authorization middleware component is a feature of the platform that runs as a sidecar container on each replica in your application. When enabled, every incoming HTTP request passes through the security layer before being handled by your application.\nThe platform middleware handles several things for your app:\nAuthenticates users and clients with the specified identity providers\nManages the authenticated session\nInjects identity information into HTTP request headers\nThe authentication and authorization module runs in a separate container, isolated from your application code. As the security container doesn't run in-process, no direct integration with specific language frameworks is possible. However, relevant information your app needs is provided in request headers.\nAuthentication flow\nThe authentication flow is the same for all providers, but differs depending on whether you want to sign in with the provider's SDK:\nWithout provider SDK\n(server-directed flow or server flow): The application delegates federated sign-in to Container Apps. Delegation is typically the case with browser apps, which presents the provider's sign-in page to the user.\nWith provider SDK\n(client-directed flow or client flow): The application signs users in to the provider manually and then submits the authentication token to Container Apps for validation. This approach is typical for browser-less apps that don't present the provider's sign-in page to the user. An example is a native mobile app that signs users in using the provider's SDK.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Implement authentication and authorization in Azure Container Apps"
                },
                {
                  "level": 2,
                  "text": "Identity providers"
                },
                {
                  "level": 2,
                  "text": "Feature architecture"
                },
                {
                  "level": 2,
                  "text": "Authentication flow"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [
                "allowInsecure",
                "/.auth/login/aad",
                "/.auth/login/facebook",
                "/.auth/login/github",
                "/.auth/login/google",
                "/.auth/login/twitter",
                "/.auth/login/<providerName>"
              ],
              "images": [
                {
                  "src": "../../wwl-azure/implement-azure-container-apps/media/container-apps-authorization-architecture.png",
                  "absolute_url": "https://learn.microsoft.com/en-us/wwl-azure/implement-azure-container-apps/media/container-apps-authorization-architecture.png",
                  "alt_text": "Diagram showing requests being intercepted by a sidecar container interacting with identity providers, before allowing traffic to the app container.",
                  "title": "",
                  "filename": "container-apps-authorization-architecture.png",
                  "image_type": "diagram",
                  "context": {
                    "preceding_heading": "Feature architecture",
                    "following_text": "The platform middleware handles several things for your app:",
                    "figure_caption": "",
                    "parent_section": ""
                  }
                }
              ],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                },
                {
                  "url": "/en-us/azure/container-apps/authentication-azure-active-directory",
                  "text": "Microsoft Identity Platform"
                },
                {
                  "url": "/en-us/azure/container-apps/authentication-facebook",
                  "text": "Facebook"
                },
                {
                  "url": "/en-us/azure/container-apps/authentication-github",
                  "text": "GitHub"
                },
                {
                  "url": "/en-us/azure/container-apps/authentication-google",
                  "text": "Google"
                },
                {
                  "url": "/en-us/azure/container-apps/authentication-twitter",
                  "text": "X"
                },
                {
                  "url": "/en-us/azure/container-apps/authentication-openid",
                  "text": "OpenID Connect"
                }
              ]
            },
            {
              "number": 6,
              "title": "Manage revisions and secrets in Azure Container Apps",
              "url": "https://learn.microsoft.com/en-us/training/modules/implement-azure-container-apps/6-container-apps-revisions-secrets",
              "href": "6-container-apps-revisions-secrets",
              "content": "Read in English\nAdd\nAdd to plan\nManage revisions and secrets in Azure Container Apps\nCompleted\n5 minutes\nAzure Container Apps implements container app versioning by creating revisions. A revision is an immutable snapshot of a container app version. You can use revisions to release a new version of your app, or quickly revert to an earlier version of your app. New revisions are created when you update your application with\nrevision-scope changes\n. You can also update your container app based on a specific revision.\nYou can control which revisions are active, and the external traffic that is routed to each active revision. Revision names are used to identify a revision, and in the revision's URL. You can customize the revision name by setting the revision suffix.\nBy default, Container Apps creates a unique revision name with a suffix consisting of a semi-random string of alphanumeric characters. For example, for a container app named\nalbum-api\n, setting the revision suffix name to\n1st-revision\nwould create a revision with the name\nalbum-api--1st-revision\n. You can set the revision suffix in the ARM template, through the Azure CLI\naz containerapp create\nand\naz containerapp update\ncommands, or when creating a revision via the Azure portal.\nUpdating your container app\nWith the\naz containerapp update\ncommand you can modify environment variables, compute resources, scale parameters, and deploy a different image. If your container app update includes\nrevision-scope changes\n, a new revision is generated.\naz containerapp update \\\n  --name <APPLICATION_NAME> \\\n  --resource-group <RESOURCE_GROUP_NAME> \\\n  --image <IMAGE_NAME>\nYou can list all revisions associated with your container app with the\naz containerapp revision list\ncommand.\naz containerapp revision list \\\n  --name <APPLICATION_NAME> \\\n  --resource-group <RESOURCE_GROUP_NAME> \\\n  -o table\nFor more information about Container Apps commands, visit the\naz containerapp\nreference.\nManage secrets in Azure Container Apps\nAzure Container Apps allows your application to securely store sensitive configuration values. Once secrets are defined at the application level, secured values are available to container apps. Specifically, you can reference secured values inside scale rules.\nSecrets are scoped to an application, outside of any specific revision of an application.\nAdding, removing, or changing secrets doesn't generate new revisions.\nEach application revision can reference one or more secrets.\nMultiple revisions can reference the same secrets.\nAn updated or deleted secret doesn't automatically affect existing revisions in your app. When a secret is updated or deleted, you can respond to changes in one of two ways:\nDeploy a new revision.\nRestart an existing revision.\nBefore you delete a secret, deploy a new revision that no longer references the old secret. Then deactivate all revisions that reference the secret.\nNote\nContainer Apps doesn't support Azure Key Vault integration. Instead, enable managed identity in the container app and use the Key Vault SDK in your app to access secrets.\nDefining secrets\nWhen you create a container app, secrets are defined using the\n--secrets\nparameter.\nThe parameter accepts a space-delimited set of name/value pairs.\nEach pair is delimited by an equals sign (\n=\n).\nIn the following example, a connection string to a queue storage account is declared in the\n--secrets\nparameter. The value for queue-connection-string comes from an environment variable named\n$CONNECTION_STRING\n.\naz containerapp create \\\n  --resource-group \"my-resource-group\" \\\n  --name queuereader \\\n  --environment \"my-environment-name\" \\\n  --image demos/queuereader:v1 \\\n  --secrets \"queue-connection-string=$CONNECTION_STRING\"\nAfter declaring secrets at the application level, you can reference them in environment variables when you create a new revision in your container app. When an environment variable references a secret, its value is populated with the value defined in the secret. To reference a secret in an environment variable in the Azure CLI, set its value to\nsecretref:\n, followed by the name of the secret.\nThe following example shows an application that declares a connection string at the application level. This connection is referenced in a container environment variable.\naz containerapp create \\\n  --resource-group \"my-resource-group\" \\\n  --name myQueueApp \\\n  --environment \"my-environment-name\" \\\n  --image demos/myQueueApp:v1 \\\n  --secrets \"queue-connection-string=$CONNECTIONSTRING\" \\\n  --env-vars \"QueueName=myqueue\" \"ConnectionString=secretref:queue-connection-string\"\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Manage revisions and secrets in Azure Container Apps"
                },
                {
                  "level": 3,
                  "text": "Updating your container app"
                },
                {
                  "level": 2,
                  "text": "Manage secrets in Azure Container Apps"
                },
                {
                  "level": 3,
                  "text": "Defining secrets"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [
                "az containerapp create",
                "az containerapp update",
                "az containerapp update",
                "az containerapp update \\\n  --name <APPLICATION_NAME> \\\n  --resource-group <RESOURCE_GROUP_NAME> \\\n  --image <IMAGE_NAME>",
                "az containerapp update \\\n  --name <APPLICATION_NAME> \\\n  --resource-group <RESOURCE_GROUP_NAME> \\\n  --image <IMAGE_NAME>",
                "az containerapp revision list",
                "az containerapp revision list \\\n  --name <APPLICATION_NAME> \\\n  --resource-group <RESOURCE_GROUP_NAME> \\\n  -o table",
                "az containerapp revision list \\\n  --name <APPLICATION_NAME> \\\n  --resource-group <RESOURCE_GROUP_NAME> \\\n  -o table",
                "az containerapp",
                "$CONNECTION_STRING",
                "az containerapp create \\\n  --resource-group \"my-resource-group\" \\\n  --name queuereader \\\n  --environment \"my-environment-name\" \\\n  --image demos/queuereader:v1 \\\n  --secrets \"queue-connection-string=$CONNECTION_STRING\"",
                "az containerapp create \\\n  --resource-group \"my-resource-group\" \\\n  --name queuereader \\\n  --environment \"my-environment-name\" \\\n  --image demos/queuereader:v1 \\\n  --secrets \"queue-connection-string=$CONNECTION_STRING\"",
                "az containerapp create \\\n  --resource-group \"my-resource-group\" \\\n  --name myQueueApp \\\n  --environment \"my-environment-name\" \\\n  --image demos/myQueueApp:v1 \\\n  --secrets \"queue-connection-string=$CONNECTIONSTRING\" \\\n  --env-vars \"QueueName=myqueue\" \"ConnectionString=secretref:queue-connection-string\"",
                "az containerapp create \\\n  --resource-group \"my-resource-group\" \\\n  --name myQueueApp \\\n  --environment \"my-environment-name\" \\\n  --image demos/myQueueApp:v1 \\\n  --secrets \"queue-connection-string=$CONNECTIONSTRING\" \\\n  --env-vars \"QueueName=myqueue\" \"ConnectionString=secretref:queue-connection-string\""
              ],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                },
                {
                  "url": "/en-us/azure/container-apps/revisions#revision-scope-changes",
                  "text": "revision-scope changes"
                },
                {
                  "url": "/en-us/azure/container-apps/revisions#revision-scope-changes",
                  "text": "revision-scope changes"
                },
                {
                  "url": "/en-us/cli/azure/containerapp",
                  "text": "az containerapp"
                }
              ]
            },
            {
              "number": 7,
              "title": "Explore Dapr integration with Azure Container Apps",
              "url": "https://learn.microsoft.com/en-us/training/modules/implement-azure-container-apps/7-explore-distributed-application-runtime",
              "href": "7-explore-distributed-application-runtime",
              "content": "Read in English\nAdd\nAdd to plan\nExplore Dapr integration with Azure Container Apps\nCompleted\n5 minutes\nThe Distributed Application Runtime (Dapr) is a set of incrementally adoptable features that simplify the authoring of distributed, microservice-based applications. Dapr provides capabilities for enabling application intercommunication through messaging via pub/sub or reliable and secure service-to-service calls.\nDapr is an open source,\nCloud Native Computing Foundation (CNCF)\nproject. The CNCF is part of the Linux Foundation and provides support, oversight, and direction for fast-growing, cloud native projects. As an alternative to deploying and managing the Dapr OSS project yourself, the Container Apps platform:\nProvides a managed and supported Dapr integration\nHandles Dapr version upgrades seamlessly\nExposes a simplified Dapr interaction model to increase developer productivity\nDapr APIs\nDapr API\nDescription\nService-to-service invocation\nDiscover services and perform reliable, direct service-to-service calls with automatic mTLS authentication and encryption.\nState management\nProvides state management capabilities for transactions and CRUD operations.\nPub/sub\nAllows publisher and subscriber container apps to intercommunicate via an intermediary message broker.\nBindings\nTrigger your applications based on events\nActors\nDapr actors are message-driven, single-threaded, units of work designed to quickly scale. For example, in burst-heavy workload situations.\nObservability\nSend tracing information to an Application Insights backend.\nSecrets\nAccess secrets from your application code or reference secure values in your Dapr components.\nConfiguration\nRetrieve and subscribe to application configuration items for supported configuration stores.\nNote\nThe table covers stable Dapr APIs. To learn more about using alpha APIs and features,\nvisit limitations\n.\nDapr core concepts\nThe following example based on the Pub/sub API is used to illustrate core concepts related to Dapr in Azure Container Apps.\nLabel\nDapr settings\nDescription\n1\nContainer Apps with Dapr enabled\nDapr is enabled at the container app level by configuring a set of Dapr arguments. These values apply to all revisions of a given container app when running in multiple revisions mode.\n2\nDapr\nThe fully managed Dapr APIs are exposed to each container app through a Dapr sidecar. The Dapr APIs can be invoked from your container app via HTTP or gRPC. The Dapr sidecar runs on HTTP port 3500 and gRPC port 50001.\n3\nDapr component configuration\nDapr uses a modular design where functionality is delivered as a component. Dapr components can be shared across multiple container apps. The Dapr app identifiers provided in the scopes array dictate which dapr-enabled container apps load a given component at runtime.\nDapr enablement\nYou can configure Dapr using various\narguments and annotations\nbased on the runtime context. Azure Container Apps provides three channels through which you can configure Dapr:\nContainer Apps CLI\nInfrastructure as Code (IaC) templates, as in Bicep or Azure Resource Manager (ARM) templates\nThe Azure portal\nDapr components and scopes\nDapr uses a modular design where functionality is delivered as a component. The use of Dapr components is optional and dictated exclusively by the needs of your application.\nDapr components in container apps are environment-level resources that:\nCan provide a pluggable abstraction model for connecting to supporting external services.\nCan be shared across container apps or scoped to specific container apps.\nCan use Dapr secrets to securely retrieve configuration metadata.\nBy default, all Dapr-enabled container apps within the same environment load the full set of deployed components. To ensure components are loaded at runtime by only the appropriate container apps, application scopes should be used.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Explore Dapr integration with Azure Container Apps"
                },
                {
                  "level": 2,
                  "text": "Dapr APIs"
                },
                {
                  "level": 2,
                  "text": "Dapr core concepts"
                },
                {
                  "level": 2,
                  "text": "Dapr enablement"
                },
                {
                  "level": 2,
                  "text": "Dapr components and scopes"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [
                {
                  "src": "../../wwl-azure/implement-azure-container-apps/media/azure-container-apps-dapr-building-blocks.png",
                  "absolute_url": "https://learn.microsoft.com/en-us/wwl-azure/implement-azure-container-apps/media/azure-container-apps-dapr-building-blocks.png",
                  "alt_text": "Screenshot of Decorative.",
                  "title": "",
                  "filename": "azure-container-apps-dapr-building-blocks.png",
                  "image_type": "screenshot",
                  "context": {
                    "preceding_heading": "Dapr APIs",
                    "following_text": "",
                    "figure_caption": "",
                    "parent_section": ""
                  }
                },
                {
                  "src": "../../wwl-azure/implement-azure-container-apps/media/distributed-application-runtime-container-apps.png",
                  "absolute_url": "https://learn.microsoft.com/en-us/wwl-azure/implement-azure-container-apps/media/distributed-application-runtime-container-apps.png",
                  "alt_text": "Diagram that shows the Dapr pub/sub A P I and how it works in Container Apps.",
                  "title": "",
                  "filename": "distributed-application-runtime-container-apps.png",
                  "image_type": "diagram",
                  "context": {
                    "preceding_heading": "Dapr core concepts",
                    "following_text": "You can configure Dapr using variousarguments and annotationsbased on the runtime context. Azure Container Apps provides three channels through which you can configure Dapr:",
                    "figure_caption": "",
                    "parent_section": ""
                  }
                }
              ],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                },
                {
                  "url": "https://www.cncf.io/projects/dapr/",
                  "text": "Cloud Native Computing Foundation (CNCF)"
                },
                {
                  "url": "https://docs.dapr.io/developing-applications/building-blocks/service-invocation/service-invocation-overview/",
                  "text": "Service-to-service invocation"
                },
                {
                  "url": "https://docs.dapr.io/developing-applications/building-blocks/state-management/state-management-overview/",
                  "text": "State management"
                },
                {
                  "url": "https://docs.dapr.io/developing-applications/building-blocks/pubsub/pubsub-overview",
                  "text": "Pub/sub"
                },
                {
                  "url": "https://docs.dapr.io/developing-applications/building-blocks/bindings/bindings-overview/",
                  "text": "Bindings"
                },
                {
                  "url": "https://docs.dapr.io/developing-applications/building-blocks/actors/actors-overview/",
                  "text": "Actors"
                },
                {
                  "url": "/en-us/azure/container-apps/observability",
                  "text": "Observability"
                },
                {
                  "url": "https://docs.dapr.io/developing-applications/building-blocks/secrets/secrets-overview/",
                  "text": "Secrets"
                },
                {
                  "url": "https://docs.dapr.io/developing-applications/building-blocks/configuration/",
                  "text": "Configuration"
                },
                {
                  "url": "/en-us/azure/container-apps/dapr-overview?tabs=bicep1%2Cyaml#unsupported-dapr-capabilities",
                  "text": "visit limitations"
                },
                {
                  "url": "https://docs.dapr.io/reference/arguments-annotations-overview/",
                  "text": "arguments and annotations"
                }
              ]
            },
            {
              "number": 8,
              "title": "Module assessment",
              "url": "https://learn.microsoft.com/en-us/training/modules/implement-azure-container-apps/8-knowledge-check",
              "href": "8-knowledge-check",
              "content": "Read in English\nAdd\nAdd to plan\nModule assessment\nCompleted\n3 minutes\nCheck your knowledge\n1.\nWhich of the following options is true about the built-in authentication feature in Azure Container Apps?\nIt can only be configured to restrict access to authenticated users.\nIt allows for out-of-the-box authentication with federated identity providers.\nIt requires the use of a specific language or SDK.\n2.\nWhat is a revision in Azure Container Apps?\nA dynamic snapshot of a container app version.\nA version of a container app that is actively being used.\nAn immutable snapshot of a container app version.\nYou must answer all questions before checking your work.\nYou must answer all questions before checking your work.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Module assessment"
                },
                {
                  "level": 2,
                  "text": "Check your knowledge"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 9,
              "title": "Summary",
              "url": "https://learn.microsoft.com/en-us/training/modules/implement-azure-container-apps/9-summary",
              "href": "9-summary",
              "content": "Read in English\nAdd\nAdd to plan\nSummary\nCompleted\n3 minutes\nIn this module, you learned how to:\nDescribe the features benefits of Azure Container Apps\nDeploy container app in Azure by using the Azure CLI\nUtilize Azure Container Apps built-in authentication and authorization\nCreate revisions and implement app secrets\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Summary"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "title": "Implement user authentication and authorization",
      "url": "https://learn.microsoft.com/en-us/training/paths/az-204-implement-authentication-authorization/",
      "learn_uid": "learn.wwl.az-204-implement-authentication-authorization",
      "modules": [
        {
          "title": "Explore the Microsoft identity platform",
          "url": "https://learn.microsoft.com/en-us/training/modules/explore-microsoft-identity-platform/",
          "description": "",
          "learning_objectives": [],
          "prerequisites": [],
          "units": [
            {
              "number": 1,
              "title": "Introduction",
              "url": "https://learn.microsoft.com/en-us/training/modules/explore-microsoft-identity-platform/1-introduction",
              "href": "1-introduction",
              "content": "Read in English\nAdd\nAdd to plan\nIntroduction\nCompleted\n3 minutes\nThe Microsoft identity platform for developers is a set of tools that includes authentication service, open-source libraries, and application management tools.\nAfter completing this module, you'll be able to:\nIdentify the components of the Microsoft identity platform\nDescribe the three types of service principals and how they relate to application objects\nExplain how permissions and user consent operate, and how conditional access impacts your application\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Introduction"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 2,
              "title": "Explore the Microsoft identity platform",
              "url": "https://learn.microsoft.com/en-us/training/modules/explore-microsoft-identity-platform/2-microsoft-identity-platform-overview",
              "href": "2-microsoft-identity-platform-overview",
              "content": "Read in English\nAdd\nAdd to plan\nExplore the Microsoft identity platform\nCompleted\n3 minutes\nThe Microsoft identity platform helps you build applications your users and customers can sign in to using their Microsoft identities or social accounts, and provide authorized access to your own APIs or Microsoft APIs like Microsoft Graph.\nThere are several components that make up the Microsoft identity platform:\nOAuth 2.0 and OpenID Connect standard-compliant authentication service\nenabling developers to authenticate several identity types, including:\nWork or school accounts, provisioned through Microsoft Entra ID\nPersonal Microsoft account, like Skype, Xbox, and Outlook.com\nSocial or local accounts, by using Azure Active Directory B2C\nSocial or local customer accounts, by using Microsoft Entra External ID\nOpen-source libraries\n: Microsoft Authentication Libraries (MSAL) and support for other standards-compliant libraries\nMicrosoft identity platform endpoint\n: Works with the Microsoft Authentication Libraries (MSAL) or any other standards-compliant library. It implements human readable scopes, in accordance with industry standards.\nApplication management portal\n: A registration and configuration experience in the Azure portal, along with the other Azure management capabilities.\nApplication configuration API and PowerShell\n: Programmatic configuration of your applications through the Microsoft Graph API and PowerShell so you can automate your DevOps tasks.\nFor developers, the Microsoft identity platform offers integration of modern innovations in the identity and security space like passwordless authentication, step-up authentication, and Conditional Access. You donât need to implement such functionality yourself: applications integrated with the Microsoft identity platform natively take advantage of such innovations.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Explore the Microsoft identity platform"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 3,
              "title": "Explore service principals",
              "url": "https://learn.microsoft.com/en-us/training/modules/explore-microsoft-identity-platform/3-app-service-principals",
              "href": "3-app-service-principals",
              "content": "Read in English\nAdd\nAdd to plan\nExplore service principals\nCompleted\n3 minutes\nTo delegate Identity and Access Management functions to Microsoft Entra ID, an application must be registered with a Microsoft Entra tenant. When you register your application with Microsoft Entra ID, you're creating an identity configuration for your application that allows it to integrate with Microsoft Entra ID. When you register an app in the Azure portal, you choose whether it is:\nSingle tenant\n: only accessible in your tenant\nMulti-tenant\n: accessible in other tenants\nIf you register an application in the portal, an application object (the globally unique instance of the app) and a service principal object are automatically created in your home tenant. You also have a globally unique ID for your app (the app or client ID). In the portal, you can then add secrets or certificates and scopes to make your app work, customize the branding of your app in the sign-in dialog, and more.\nNote\nYou can also create service principal objects in a tenant using Azure PowerShell, Azure CLI, Microsoft Graph, and other tools.\nApplication object\nA Microsoft Entra application is scoped to its one and only application object. The application object resides in the Microsoft Entra tenant where the application was registered (known as the application's \"home\" tenant). An application object is used as a template or blueprint to create one or more service principal objects. A service principal is created in every tenant where the application is used. Similar to a class in object-oriented programming, the application object has some static properties that are applied to all the created service principals (or application instances).\nThe application object describes three aspects of an application:\nHow the service can issue tokens in order to access the application.\nResources that the application might need to access.\nThe actions that the application can take.\nThe Microsoft Graph\nApplication entity\ndefines the schema for an application object's properties.\nService principal object\nTo access resources secured by a Microsoft Entra tenant, the entity that is requesting access must be represented by a security principal. This is true for both users (user principal) and applications (service principal).\nThe security principal defines the access policy and permissions for the user/application in the Microsoft Entra tenant. This enables core features such as authentication of the user/application during sign-in, and authorization during resource access.\nThere are three types of service principal:\nApplication\n- This type of service principal is the local representation, or application instance, of a global application object in a single tenant or directory. A service principal is created in each tenant where the application is used, and references the globally unique app object.  The service principal object defines what the app can actually do in the specific tenant, who can access the app, and what resources the app can access.\nManaged identity\n- This type of service principal is used to represent a\nmanaged identity\n. Managed identities provide an identity for applications to use when connecting to resources that support Microsoft Entra authentication. When a managed identity is enabled, a service principal representing that managed identity is created in your tenant. Service principals representing managed identities can be granted access and permissions, but can't be updated or modified directly.\nLegacy\n- This type of service principal represents a legacy app, which is an app created before app registrations were introduced or an app created through legacy experiences. A legacy service principal can have:\ncredentials\nservice principal names\nreply URLs\nand other properties that an authorized user can edit, but doesn't have an associated app registration.\nRelationship between application objects and service principals\nThe application object is the\nglobal\nrepresentation of your application for use across all tenants, and the service principal is the\nlocal\nrepresentation for use in a specific tenant. The application object serves as the template from which common and default properties are\nderived\nfor use in creating corresponding service principal objects.\nAn application object has:\nA one to one relationship with the software application, and\nA one to many relationships with its corresponding service principal objects.\nA service principal must be created in each tenant where the application is used to establish an identity for sign-in and/or access to resources being secured by the tenant. A single-tenant application has only one service principal (in its home tenant), created and consented for use during application registration. A multitenant application also has a service principal created in each tenant where a user from that tenant consented to its use.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Explore service principals"
                },
                {
                  "level": 2,
                  "text": "Application object"
                },
                {
                  "level": 2,
                  "text": "Service principal object"
                },
                {
                  "level": 2,
                  "text": "Relationship between application objects and service principals"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                },
                {
                  "url": "/en-us/graph/api/resources/application",
                  "text": "Application entity"
                },
                {
                  "url": "/en-us/azure/active-directory/managed-identities-azure-resources/overview",
                  "text": "managed identity"
                }
              ]
            },
            {
              "number": 4,
              "title": "Discover permissions and consent",
              "url": "https://learn.microsoft.com/en-us/training/modules/explore-microsoft-identity-platform/4-permission-consent",
              "href": "4-permission-consent",
              "content": "Read in English\nAdd\nAdd to plan\nDiscover permissions and consent\nCompleted\n3 minutes\nApplications that integrate with the Microsoft identity platform follow an authorization model that gives users and administrators control over how data can be accessed.\nThe Microsoft identity platform implements the\nOAuth 2.0\nauthorization protocol. OAuth 2.0 is a method through which a third-party app can access web-hosted resources on behalf of a user. Any web-hosted resource that integrates with the Microsoft identity platform has a resource identifier, or\napplication ID URI\n.\nHere are some examples of Microsoft web-hosted resources:\nMicrosoft Graph:\nhttps://graph.microsoft.com\nMicrosoft 365 Mail API:\nhttps://outlook.office.com\nAzure Key Vault:\nhttps://vault.azure.net\nThe same is true for any third-party resources that are integrated with the Microsoft identity platform. Any of these resources also can define a set of permissions that can be used to divide the functionality of that resource into smaller chunks. When a resource's functionality is chunked into small permission sets, third-party apps can be built to request only the permissions that they need to perform their function. Users and administrators can know what data the app can access.\nIn OAuth 2.0, these types of permission sets are called\nscopes\n. They're also often referred to as\npermissions\n. In the Microsoft identity platform, a permission is represented as a string value. An app requests the permissions it needs by specifying the permission in the\nscope\nquery parameter. Identity platform supports several well-defined\nOpenID Connect scopes\nand resource-based permissions (each permission is indicated by appending the permission value to the resource's identifier or application ID URI). For example, the permission string\nhttps://graph.microsoft.com/Calendars.Read\nis used to request permission to read users calendars in Microsoft Graph.\nAn app most commonly requests these permissions by specifying the scopes in requests to the Microsoft identity platform authorize endpoint. However, some high-privilege permissions can be granted only through administrator consent. They can be requested or granted by using the\nadministrator consent endpoint\n.\nNote\nIn requests to the authorization, token or consent endpoints for the Microsoft Identity platform, if the resource identifier is omitted in the scope parameter, the resource is assumed to be Microsoft Graph. For example,\nscope=User.Read\nis equivalent to\nhttps://graph.microsoft.com/User.Read\n.\nPermission types\nThe Microsoft identity platform supports two types of permissions:\ndelegated access\nand\napp-only access\n.\nDelegated access\nare used by apps that have a signed-in user present. For these apps, either the user or an administrator consents to the permissions that the app requests. The app is delegated with the permission to act as a signed-in user when it makes calls to the target resource.\nApp-only access permissions\nare used by apps that run without a signed-in user present, for example, apps that run as background services or daemons. Only an administrator can consent to app-only access permissions.\nConsent types\nApplications in Microsoft identity platform rely on consent in order to gain access to necessary resources or APIs. There are many kinds of consent that your app might need to know about in order to be successful. If you're defining permissions, you'll also need to understand how your users gain access to your app or API.\nThere are three consent types:\nstatic user consent\n,\nincremental and dynamic user consent\n, and\nadmin consent\n.\nStatic user consent\nIn the static user consent scenario, you must specify all the permissions it needs in the app's configuration in the Azure portal. If the user (or administrator, as appropriate) hasn't granted consent for this app, then Microsoft identity platform prompts the user to provide consent at this time. Static permissions also enable administrators to consent on behalf of all users in the organization.\nWhile static permissions of the app defined in the Azure portal keep the code nice and simple, it presents some possible issues for developers:\nThe app needs to request all the permissions it would ever need upon the user's first sign-in. This can lead to a long list of permissions that discourages end users from approving the app's access on initial sign-in.\nThe app needs to know all of the resources it would ever access ahead of time. It's difficult to create apps that could access an arbitrary number of resources.\nIncremental and dynamic user consent\nWith the Microsoft identity platform endpoint, you can ignore the static permissions defined in the app registration information in the Azure portal and request permissions incrementally instead. You can ask for a minimum set of permissions upfront and request more over time as the customer uses more app features.\nTo do so, you can specify the scopes your app needs at any time by including the new scopes in the\nscope\nparameter when requesting an access token - without the need to predefine them in the application registration information. If the user hasn't yet consented to new scopes added to the request, they're prompted to consent only to the new permissions. Incremental, or dynamic consent, only applies to delegated permissions and not to app-only access permissions.\nImportant\nDynamic consent can be convenient, but presents a big challenge for permissions that require admin consent, since the admin consent experience doesn't know about those permissions at consent time. If you require admin privileged permissions or if your app uses dynamic consent, you must register all of the permissions in the Azure portal (not just the subset of permissions that require admin consent). This enables tenant admins to consent on behalf of all their users.\nAdmin consent\nAdmin consent is required when your app needs access to certain high-privilege permissions. Admin consent ensures that administrators have some other controls before authorizing apps or users to access highly privileged data from the organization.\nAdmin consent done on behalf of an organization still requires the static permissions registered for the app. Set those permissions for apps in the app registration portal if you need an admin to give consent on behalf of the entire organization. This reduces the cycles required by the organization admin to set up the application.\nRequesting individual user consent\nIn an OpenID Connect or OAuth 2.0 authorization request, an app can request the permissions it needs by using the scope query parameter. For example, when a user signs in to an app, the app sends a request like the following example. Line breaks are added for legibility.\nGET https://login.microsoftonline.com/common/oauth2/v2.0/authorize?\nclient_id=00001111-aaaa-2222-bbbb-3333cccc4444\n&response_type=code\n&redirect_uri=http%3A%2F%2Flocalhost%2Fmyapp%2F\n&response_mode=query\n&scope=\nhttps%3A%2F%2Fgraph.microsoft.com%2Fcalendars.read%20\nhttps%3A%2F%2Fgraph.microsoft.com%2Fmail.send\n&state=12345\nThe\nscope\nparameter is a space-separated list of delegated permissions that the app is requesting. Each permission is indicated by appending the permission value to the resource's identifier (the application ID URI). In the request example, the app needs permission to read the user's calendar and send mail as the user.\nAfter the user enters their credentials, the Microsoft identity platform checks for a matching record of\nuser consent\n. If the user hasn't consented to any of the requested permissions in the past, and if the administrator hasn't consented to these permissions on behalf of the entire organization, the Microsoft identity platform asks the user to grant the requested permissions.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Discover permissions and consent"
                },
                {
                  "level": 2,
                  "text": "Permission types"
                },
                {
                  "level": 2,
                  "text": "Consent types"
                },
                {
                  "level": 3,
                  "text": "Static user consent"
                },
                {
                  "level": 3,
                  "text": "Incremental and dynamic user consent"
                },
                {
                  "level": 3,
                  "text": "Admin consent"
                },
                {
                  "level": 2,
                  "text": "Requesting individual user consent"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [
                "https://graph.microsoft.com",
                "https://outlook.office.com",
                "https://vault.azure.net",
                "https://graph.microsoft.com/Calendars.Read",
                "scope=User.Read",
                "https://graph.microsoft.com/User.Read",
                "GET https://login.microsoftonline.com/common/oauth2/v2.0/authorize?\nclient_id=00001111-aaaa-2222-bbbb-3333cccc4444\n&response_type=code\n&redirect_uri=http%3A%2F%2Flocalhost%2Fmyapp%2F\n&response_mode=query\n&scope=\nhttps%3A%2F%2Fgraph.microsoft.com%2Fcalendars.read%20\nhttps%3A%2F%2Fgraph.microsoft.com%2Fmail.send\n&state=12345",
                "GET https://login.microsoftonline.com/common/oauth2/v2.0/authorize?\nclient_id=00001111-aaaa-2222-bbbb-3333cccc4444\n&response_type=code\n&redirect_uri=http%3A%2F%2Flocalhost%2Fmyapp%2F\n&response_mode=query\n&scope=\nhttps%3A%2F%2Fgraph.microsoft.com%2Fcalendars.read%20\nhttps%3A%2F%2Fgraph.microsoft.com%2Fmail.send\n&state=12345"
              ],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                },
                {
                  "url": "/en-us/azure/active-directory/develop/active-directory-v2-protocols",
                  "text": "OAuth 2.0"
                },
                {
                  "url": "/en-us/azure/active-directory/develop/v2-permissions-and-consent#openid-connect-scopes",
                  "text": "OpenID Connect scopes"
                },
                {
                  "url": "/en-us/azure/active-directory/develop/v2-permissions-and-consent#admin-restricted-permissions",
                  "text": "administrator consent endpoint"
                }
              ]
            },
            {
              "number": 5,
              "title": "Discover conditional access",
              "url": "https://learn.microsoft.com/en-us/training/modules/explore-microsoft-identity-platform/5-conditional-access",
              "href": "5-conditional-access",
              "content": "Read in English\nAdd\nAdd to plan\nDiscover conditional access\nCompleted\n3 minutes\nThe Conditional Access feature in Microsoft Entra ID offers one of several ways that you can use to secure your app and protect a service. Conditional Access enables developers and enterprise customers to protect services in a multitude of ways including:\nMultifactor authentication\nAllowing only Intune enrolled devices to access specific services\nRestricting user locations and IP ranges\nHow does Conditional Access impact an app?\nIn most common cases, Conditional Access doesn't change an app's behavior or require any changes from the developer. Only in certain cases when an app indirectly or silently requests a token for a service does an app require code changes to handle Conditional Access challenges. It may be as simple as performing an interactive sign-in request.\nSpecifically, the following scenarios require code to handle Conditional Access challenges:\nApps performing the on-behalf-of flow\nApps accessing multiple services/resources\nSingle-page apps using MSAL.js\nWeb apps calling a resource\nConditional Access policies can be applied to the app and also a web API your app accesses. Depending on the scenario, an enterprise customer can apply and remove Conditional Access policies at any time. For your app to continue functioning when a new policy is applied, implement challenge handling.\nConditional Access examples\nSome scenarios require code changes to handle Conditional Access whereas others work as is. Here are a few scenarios using Conditional Access to do multifactor authentication that gives some insight into the difference.\nYou're building a single-tenant iOS app and apply a Conditional Access policy. The app signs in a user and doesn't request access to an API. When the user signs in, the policy is automatically invoked and the user needs to perform multifactor authentication.\nYou're building an app that uses a middle tier service to access a downstream API. An enterprise customer at the company using this app applies a policy to the downstream API. When an end user signs in, the app requests access to the middle tier and sends the token. The middle tier performs on-behalf-of flow to request access to the downstream API. At this point, a claims \"challenge\" is presented to the middle tier. The middle tier sends the challenge back to the app, which needs to comply with the Conditional Access policy.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Discover conditional access"
                },
                {
                  "level": 2,
                  "text": "How does Conditional Access impact an app?"
                },
                {
                  "level": 2,
                  "text": "Conditional Access examples"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                },
                {
                  "url": "/en-us/azure/active-directory/authentication/concept-mfa-howitworks",
                  "text": "Multifactor authentication"
                }
              ]
            },
            {
              "number": 6,
              "title": "Module assessment",
              "url": "https://learn.microsoft.com/en-us/training/modules/explore-microsoft-identity-platform/6-knowledge-check",
              "href": "6-knowledge-check",
              "content": "Read in English\nAdd\nAdd to plan\nModule assessment\nCompleted\n3 minutes\nCheck your knowledge\n1.\nWhich of the types of permissions supported by the Microsoft identity platform is used by apps that have a signed-in user present?\nDelegated permissions\nApp-only access permissions\nBoth delegated and app-only access permissions\n2.\nWhich of the following app scenarios require code to handle Conditional Access challenges?\nApps performing the device-code flow\nApps performing the on-behalf-of flow\nApps performing the Integrated Windows authentication flow\nYou must answer all questions before checking your work.\nYou must answer all questions before checking your work.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Module assessment"
                },
                {
                  "level": 2,
                  "text": "Check your knowledge"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 7,
              "title": "Summary",
              "url": "https://learn.microsoft.com/en-us/training/modules/explore-microsoft-identity-platform/7-summary",
              "href": "7-summary",
              "content": "Read in English\nAdd\nAdd to plan\nSummary\nCompleted\n3 minutes\nIn this module, you learned how to:\nIdentify the components of the Microsoft identity platform\nDescribe the three types of service principals and how they relate to application objects\nExplain how permissions and user consent operate, and how conditional access impacts your application\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Summary"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            }
          ]
        },
        {
          "title": "Implement authentication by using the Microsoft Authentication Library",
          "url": "https://learn.microsoft.com/en-us/training/modules/implement-authentication-by-using-microsoft-authentication-library/",
          "description": "",
          "learning_objectives": [],
          "prerequisites": [],
          "units": [
            {
              "number": 1,
              "title": "Introduction",
              "url": "https://learn.microsoft.com/en-us/training/modules/implement-authentication-by-using-microsoft-authentication-library/1-introduction",
              "href": "1-introduction",
              "content": "Read in English\nAdd\nAdd to plan\nIntroduction\nCompleted\n3 minutes\nThe Microsoft Authentication Library (MSAL) enables developers to acquire tokens from the Microsoft identity platform in order to authenticate users and access secured web APIs.\nAfter completing this module, you'll be able to:\nExplain the benefits of using MSAL and the application types and scenarios it supports\nInstantiate both public and confidential client apps from code\nRegister an app with the Microsoft identity platform\nCreate an app that retrieves a token with the MSAL.NET SDK\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Introduction"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 2,
              "title": "Explore the Microsoft Authentication Library",
              "url": "https://learn.microsoft.com/en-us/training/modules/implement-authentication-by-using-microsoft-authentication-library/2-microsoft-authentication-library-overview",
              "href": "2-microsoft-authentication-library-overview",
              "content": "Read in English\nAdd\nAdd to plan\nExplore the Microsoft Authentication Library\nCompleted\n3 minutes\nThe Microsoft Authentication Library (MSAL) enables developers to acquire security tokens from the Microsoft identity platform to authenticate users and access secured web APIs. It can be used to provide secure access to Microsoft Graph, other Microsoft APIs, third-party web APIs, or your own web API. MSAL supports many different application architectures and platforms including .NET, JavaScript, Java, Python, Android, and iOS.\nMSAL gives you many ways to get tokens, with a consistent API for many platforms. Using MSAL provides the following benefits:\nNo need to directly use the OAuth libraries or code against the protocol in your application.\nAcquires tokens on behalf of a user or on behalf of an application (when applicable to the platform).\nMaintains a token cache and refreshes tokens for you when they're close to expire. You don't need to handle token expiration on your own.\nHelps you specify which audience you want your application to sign in.\nHelps you set up your application from configuration files.\nHelps you troubleshoot your app by exposing actionable exceptions, logging, and telemetry.\nApplication types and scenarios\nWithin MSAL, a token can be acquired from many application types: web applications, web APIs, single-page apps (JavaScript), mobile and native applications, and daemons and server-side applications. MSAL currently supports the platforms and frameworks listed in the following table.\nLibrary\nSupported platforms and frameworks\nMSAL for Android\nAndroid\nMSAL Angular\nSingle-page apps with Angular and Angular.js frameworks\nMSAL for iOS and macOS\niOS and macOS\nMSAL Go (Preview)\nWindows, macOS, Linux\nMSAL Java\nWindows, macOS, Linux\nMSAL.js\nJavaScript/TypeScript frameworks such as Vue.js, Ember.js, or Durandal.js\nMSAL.NET\n.NET Framework, .NET, .NET MAUI, WINUI, Xamarin Android, Xamarin iOS, Universal Windows Platform\nMSAL Node\nWeb apps with Express, desktop apps with Electron, Cross-platform console apps\nMSAL Python\nWindows, macOS, Linux\nMSAL React\nSingle-page apps with React and React-based libraries (Next.js, Gatsby.js)\nAuthentication flows\nThe following table shows some of the different authentication flows provided by Microsoft Authentication Library (MSAL). These flows can be used in various application scenarios.\nAuthentication flow\nEnables\nSupported application types\nAuthorization code\nUser sign-in and access to web APIs on behalf of the user.\nDesktop, Mobile, Single-page app (SPA) (requires PKCE), Web\nClient credentials\nAccess to web APIs by using the identity of the application itself. Typically used for server-to-server communication and automated scripts requiring no user interaction.\nDaemon\nDevice code\nUser sign-in and access to web APIs on behalf of the user on input-constrained devices like smart TVs and IoT devices. Also used by command line interface (CLI) applications.\nDesktop, Mobile\nImplicit grant\nUser sign-in and access to web APIs on behalf of the user.\nThe implicit grant flow is no longer recommended - use authorization code with PKCE instead.\nSingle-page app (SPA), Web\nOn-behalf-of (OBO)\nAccess from an \"upstream\" web API to a \"downstream\" web API on behalf of the user. The user's identity and delegated permissions are passed through to the downstream API from the upstream API.\nWeb API\nUsername/password (ROPC)\nAllows an application to sign in the user by directly handling their password.\nThe ROPC flow is NOT recommended.\nDesktop, Mobile\nIntegrated Windows authentication (IWA)\nAllows applications on domain or Microsoft Entra joined computers to acquire a token silently (without any UI interaction from the user).\nDesktop, Mobile\nPublic client and confidential client applications\nThe Microsoft Authentication Library (MSAL) defines two types of clients; public clients and confidential clients. A client is a software entity that has a unique identifier assigned by an identity provider. The client types differ based their ability to authenticate securely with the authorization server and to hold sensitive, identity proving information so that it can't be accessed or known to a user within the scope of its access.\nWhen examining the public or confidential nature of a given client, we're evaluating the ability of that client to prove its identity to the authorization server. This is important because the authorization server must be able to trust the identity of the client in order to issue access tokens.\nPublic client applications\nrun on devices, such as desktop, browserless APIs, mobile or client-side browser apps. They can't be trusted to safely keep application secrets, so they can only access web APIs on behalf of the user. Anytime the source, or compiled bytecode of a given app, is transmitted anywhere it can be read, disassembled, or otherwise inspected by untrusted parties. As they also only support public client flows and can't hold configuration-time secrets, they can't have client secrets.\nConfidential client applications\nrun on servers, such as web apps, web API apps, or service/daemon apps. They're considered difficult to access by users or attackers, and therefore can adequately hold configuration-time secrets to assert proof of its identity. The client ID is exposed through the web browser, but the secret is passed only in the back channel and never directly exposed.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Explore the Microsoft Authentication Library"
                },
                {
                  "level": 2,
                  "text": "Application types and scenarios"
                },
                {
                  "level": 2,
                  "text": "Authentication flows"
                },
                {
                  "level": 2,
                  "text": "Public client and confidential client applications"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                },
                {
                  "url": "https://github.com/AzureAD/microsoft-authentication-library-for-android",
                  "text": "MSAL for Android"
                },
                {
                  "url": "https://github.com/AzureAD/microsoft-authentication-library-for-js/tree/dev/lib/msal-angular",
                  "text": "MSAL Angular"
                },
                {
                  "url": "https://github.com/AzureAD/microsoft-authentication-library-for-objc",
                  "text": "MSAL for iOS and macOS"
                },
                {
                  "url": "https://github.com/AzureAD/microsoft-authentication-library-for-go",
                  "text": "MSAL Go (Preview)"
                },
                {
                  "url": "https://github.com/AzureAD/microsoft-authentication-library-for-java",
                  "text": "MSAL Java"
                },
                {
                  "url": "https://github.com/AzureAD/microsoft-authentication-library-for-js/tree/dev/lib/msal-browser",
                  "text": "MSAL.js"
                },
                {
                  "url": "https://github.com/AzureAD/microsoft-authentication-library-for-dotnet",
                  "text": "MSAL.NET"
                },
                {
                  "url": "https://github.com/AzureAD/microsoft-authentication-library-for-js/tree/dev/lib/msal-node",
                  "text": "MSAL Node"
                },
                {
                  "url": "https://github.com/AzureAD/microsoft-authentication-library-for-python",
                  "text": "MSAL Python"
                },
                {
                  "url": "https://github.com/AzureAD/microsoft-authentication-library-for-js/tree/dev/lib/msal-react",
                  "text": "MSAL React"
                }
              ]
            },
            {
              "number": 3,
              "title": "Initialize client applications",
              "url": "https://learn.microsoft.com/en-us/training/modules/implement-authentication-by-using-microsoft-authentication-library/3-initialize-client-applications",
              "href": "3-initialize-client-applications",
              "content": "Read in English\nAdd\nAdd to plan\nInitialize client applications\nCompleted\n3 minutes\nWith MSAL.NET 3.x, the recommended way to instantiate an application is by using the application builders:\nPublicClientApplicationBuilder\nand\nConfidentialClientApplicationBuilder\n. They offer a powerful mechanism to configure the application either from the code, or from a configuration file, or even by mixing both approaches.\nBefore initializing an application, you first need to register it so that your app can be integrated with the Microsoft identity platform. After registration, you might need the following information (which can be found in the Azure portal):\nApplication (client) ID\n- This is a string representing a GUID.\nDirectory (tenant) ID\n- Provides identity and access management (IAM) capabilities to applications and resources used by your organization. It can specify if you're writing a line of business application solely for your organization (also named single-tenant application).\nThe identity provider URL (named the\ninstance\n) and the sign-in audience for your application. These two parameters are collectively known as the authority.\nClient credentials\n- which can take the form of an application secret (client secret string) or certificate (of type\nX509Certificate2\n) if it's a confidential client app.\nFor web apps, and sometimes for public client apps (in particular when your app needs to use a broker), you need to set the\nRedirect URI\nwhere the identity provider sends the security token back to your application.\nInitializing public and confidential client applications from code\nThe following code instantiates a public client application, signing-in users in the Microsoft Azure public cloud, with their work and school accounts, or their personal Microsoft accounts.\nIPublicClientApplication app = PublicClientApplicationBuilder.Create(clientId).Build();\nIn the same way, the following code instantiates a confidential application (a Web app located at\nhttps://myapp.azurewebsites.net\n) handling tokens from users in the Microsoft Azure public cloud, with their work and school accounts, or their personal Microsoft accounts. The application is identified with the identity provider by sharing a client secret:\nstring redirectUri = \"https://myapp.azurewebsites.net\";\nIConfidentialClientApplication app = ConfidentialClientApplicationBuilder.Create(clientId)\n    .WithClientSecret(clientSecret)\n    .WithRedirectUri(redirectUri )\n    .Build();\nBuilder modifiers\nIn the code snippets using application builders,\n.With\nmethods can be applied as modifiers (for example,\n.WithAuthority\nand\n.WithRedirectUri\n).\n.WithAuthority\nmodifier: The\n.WithAuthority\nmodifier sets the application default authority to a Microsoft Entra authority, with the possibility of choosing the Azure Cloud, the audience, the tenant (tenant ID or domain name), or providing directly the authority URI.\nIPublicClientApplication app;\napp = PublicClientApplicationBuilder.Create(clientId)\n    .WithAuthority(AzureCloudInstance.AzurePublic, tenantId)\n    .Build();\n.WithRedirectUri\nmodifier: The\n.WithRedirectUri\nmodifier overrides the default redirect URI.\nIPublicClientApplication app;\napp = PublicClientApplicationBuilder.Create(client_id)\n    .WithAuthority(AzureCloudInstance.AzurePublic, tenant_id)\n    .WithRedirectUri(\"http://localhost\")\n    .Build();\nModifiers common to public and confidential client applications\nThe table below lists some of the modifiers you can set on a public, or confidential client.\nModifier\nDescription\n.WithAuthority()\nSets the application default authority to a Microsoft Entra authority, with the possibility of choosing the Azure Cloud, the audience, the tenant (tenant ID or domain name), or providing directly the authority URI.\n.WithTenantId(string tenantId)\nOverrides the tenant ID, or the tenant description.\n.WithClientId(string)\nOverrides the client ID.\n.WithRedirectUri(string redirectUri)\nOverrides the default redirect URI. This is useful for scenarios requiring a broker.\n.WithComponent(string)\nSets the name of the library using MSAL.NET (for telemetry reasons).\n.WithDebugLoggingCallback()\nIf called, the application calls\nDebug.Write\nsimply enabling debugging traces.\n.WithLogging()\nIf called, the application calls a callback with debugging traces.\n.WithTelemetry(TelemetryCallback telemetryCallback)\nSets the delegate used to send telemetry.\nModifiers specific to confidential client applications\nThe modifiers specific to a confidential client application builder can be found in the\nConfidentialClientApplicationBuilder\nclass. The different methods can be found in the\nAzure SDK for .NET documentation\n.\nModifiers such as\n.WithCertificate(X509Certificate2 certificate)\nand\n.WithClientSecret(string clientSecret)\nare mutually exclusive. If you provide both, MSAL throws a meaningful exception.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Initialize client applications"
                },
                {
                  "level": 2,
                  "text": "Initializing public and confidential client applications from code"
                },
                {
                  "level": 2,
                  "text": "Builder modifiers"
                },
                {
                  "level": 2,
                  "text": "Modifiers common to public and confidential client applications"
                },
                {
                  "level": 2,
                  "text": "Modifiers specific to confidential client applications"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [
                "PublicClientApplicationBuilder",
                "ConfidentialClientApplicationBuilder",
                "X509Certificate2",
                "IPublicClientApplication app = PublicClientApplicationBuilder.Create(clientId).Build();",
                "IPublicClientApplication app = PublicClientApplicationBuilder.Create(clientId).Build();",
                "https://myapp.azurewebsites.net",
                "string redirectUri = \"https://myapp.azurewebsites.net\";\nIConfidentialClientApplication app = ConfidentialClientApplicationBuilder.Create(clientId)\n    .WithClientSecret(clientSecret)\n    .WithRedirectUri(redirectUri )\n    .Build();",
                "string redirectUri = \"https://myapp.azurewebsites.net\";\nIConfidentialClientApplication app = ConfidentialClientApplicationBuilder.Create(clientId)\n    .WithClientSecret(clientSecret)\n    .WithRedirectUri(redirectUri )\n    .Build();",
                ".WithAuthority",
                ".WithRedirectUri",
                ".WithAuthority",
                ".WithAuthority",
                "IPublicClientApplication app;\napp = PublicClientApplicationBuilder.Create(clientId)\n    .WithAuthority(AzureCloudInstance.AzurePublic, tenantId)\n    .Build();",
                "IPublicClientApplication app;\napp = PublicClientApplicationBuilder.Create(clientId)\n    .WithAuthority(AzureCloudInstance.AzurePublic, tenantId)\n    .Build();",
                ".WithRedirectUri",
                ".WithRedirectUri",
                "IPublicClientApplication app;\napp = PublicClientApplicationBuilder.Create(client_id)\n    .WithAuthority(AzureCloudInstance.AzurePublic, tenant_id)\n    .WithRedirectUri(\"http://localhost\")\n    .Build();",
                "IPublicClientApplication app;\napp = PublicClientApplicationBuilder.Create(client_id)\n    .WithAuthority(AzureCloudInstance.AzurePublic, tenant_id)\n    .WithRedirectUri(\"http://localhost\")\n    .Build();",
                ".WithAuthority()",
                ".WithTenantId(string tenantId)",
                ".WithClientId(string)",
                ".WithRedirectUri(string redirectUri)",
                ".WithComponent(string)",
                ".WithDebugLoggingCallback()",
                "Debug.Write",
                ".WithLogging()",
                ".WithTelemetry(TelemetryCallback telemetryCallback)",
                "ConfidentialClientApplicationBuilder",
                ".WithCertificate(X509Certificate2 certificate)",
                ".WithClientSecret(string clientSecret)"
              ],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                },
                {
                  "url": "/en-us/dotnet/api/microsoft.identity.client.confidentialclientapplicationbuilder",
                  "text": "Azure SDK for .NET documentation"
                }
              ]
            },
            {
              "number": 4,
              "title": "Exercise - Implement interactive authentication with MSAL.NET",
              "url": "https://learn.microsoft.com/en-us/training/modules/implement-authentication-by-using-microsoft-authentication-library/4-interactive-authentication-msal",
              "href": "4-interactive-authentication-msal",
              "content": "Read in English\nAdd\nAdd to plan\nExercise - Implement interactive authentication with MSAL.NET\nCompleted\n15 minutes\nIn this exercise, you register an application in Microsoft Entra ID, then create a .NET console application that uses MSAL.NET to perform interactive authentication and acquire an access token for Microsoft Graph. You learn how to configure authentication scopes, handle user consent, and see how tokens are cached for subsequent runs.\nTasks performed in this exercise:\nRegister an application with the Microsoft identity platform\nCreate a .NET console app that implements the\nPublicClientApplicationBuilder\nclass to configure authentication.\nAcquire a token interactively using the\nuser.read\nMicrosoft Graph permission.\nThis exercise takes approximately\n15\nminutes to complete.\nBefore you start\nTo complete the exercise, you need:\nAn Azure subscription. If you don't already have one, you can sign up for one\nhttps://azure.microsoft.com/\n.\nVisual Studio Code\non one of the\nsupported platforms\n.\n.NET 8\nor greater.\nC# Dev Kit\nfor Visual Studio Code.\nGet started\nSelect the\nLaunch Exercise\nbutton to open the exercise instructions in a new browser window. When you're finished with the exercise, return here to:\nComplete the module\nEarn a badge for completing this module\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Exercise - Implement interactive authentication with MSAL.NET"
                },
                {
                  "level": 2,
                  "text": "Before you start"
                },
                {
                  "level": 2,
                  "text": "Get started"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [
                {
                  "src": "../../wwl-azure/implement-authentication-by-using-microsoft-authentication-library/media/launch-exercise.png",
                  "absolute_url": "https://learn.microsoft.com/en-us/wwl-azure/implement-authentication-by-using-microsoft-authentication-library/media/launch-exercise.png",
                  "alt_text": "Button to launch exercise.",
                  "title": "",
                  "filename": "launch-exercise.png",
                  "image_type": "icon",
                  "context": {
                    "preceding_heading": "Get started",
                    "following_text": "Was this page helpful?",
                    "figure_caption": "",
                    "parent_section": ""
                  }
                }
              ],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                },
                {
                  "url": "https://azure.microsoft.com/",
                  "text": "https://azure.microsoft.com/"
                },
                {
                  "url": "https://code.visualstudio.com/",
                  "text": "Visual Studio Code"
                },
                {
                  "url": "https://code.visualstudio.com/docs/supporting/requirements#_platforms",
                  "text": "supported platforms"
                },
                {
                  "url": "https://dotnet.microsoft.com/en-us/download/dotnet/8.0",
                  "text": ".NET 8"
                },
                {
                  "url": "https://marketplace.visualstudio.com/items?itemName=ms-dotnettools.csdevkit",
                  "text": "C# Dev Kit"
                }
              ]
            },
            {
              "number": 5,
              "title": "Module assessment",
              "url": "https://learn.microsoft.com/en-us/training/modules/implement-authentication-by-using-microsoft-authentication-library/5-knowledge-check",
              "href": "5-knowledge-check",
              "content": "Read in English\nAdd\nAdd to plan\nModule assessment\nCompleted\n3 minutes\nCheck your knowledge\n1.\nWhich of the following MSAL libraries supports single-page web apps?\nMSAL Node\nMSAL.js\nMSAL.NET\n2.\nWhat is the purpose of using\nPublicClientApplicationBuilder\nclass in MSAL.NET?\nThe class creates a new Azure account.\nTo configure and instantiate a public client application that can acquire tokens and authenticate users against the Microsoft identity platform.\nAdds a new API permission to the registered app.\nYou must answer all questions before checking your work.\nYou must answer all questions before checking your work.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Module assessment"
                },
                {
                  "level": 2,
                  "text": "Check your knowledge"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [
                "PublicClientApplicationBuilder"
              ],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 6,
              "title": "Summary",
              "url": "https://learn.microsoft.com/en-us/training/modules/implement-authentication-by-using-microsoft-authentication-library/6-summary",
              "href": "6-summary",
              "content": "Read in English\nAdd\nAdd to plan\nSummary\nCompleted\n3 minutes\nIn this module, you learned how to:\nExplain the benefits of using MSAL and the application types and scenarios it supports\nInstantiate both public and confidential client apps from code\nRegister an app with the Microsoft identity platform\nCreate an app that retrieves a token with the MSAL.NET SDK\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Summary"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            }
          ]
        },
        {
          "title": "Implement shared access signatures",
          "url": "https://learn.microsoft.com/en-us/training/modules/implement-shared-access-signatures/",
          "description": "",
          "learning_objectives": [],
          "prerequisites": [],
          "units": [
            {
              "number": 1,
              "title": "Introduction",
              "url": "https://learn.microsoft.com/en-us/training/modules/implement-shared-access-signatures/1-introduction",
              "href": "1-introduction",
              "content": "Read in English\nAdd\nAdd to plan\nIntroduction\nCompleted\n3 minutes\nA shared access signature (SAS) is a URI that grants restricted access rights to Azure Storage resources. You can provide a shared access signature to clients that you want to grant delegate access to certain storage account resources.\nAfter completing this module, you'll be able to:\nIdentify the three types of shared access signatures\nExplain when to implement shared access signatures\nCreate a stored access policy\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Introduction"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 2,
              "title": "Discover shared access signatures",
              "url": "https://learn.microsoft.com/en-us/training/modules/implement-shared-access-signatures/2-shared-access-signatures-overview",
              "href": "2-shared-access-signatures-overview",
              "content": "Read in English\nAdd\nAdd to plan\nDiscover shared access signatures\nCompleted\n3 minutes\nA shared access signature (SAS) is a signed URI that points to one or more storage resources and includes a token that contains a special set of query parameters. The token indicates how the resources might be accessed by the client. One of the query parameters, the signature, is constructed from the SAS parameters and signed with the key that was used to create the SAS. This signature is used by Azure Storage to authorize access to the storage resource.\nTypes of shared access signatures\nAzure Storage supports three types of shared access signatures:\nUser delegation SAS\n: A user delegation SAS is secured with Microsoft Entra credentials and also by the permissions specified for the SAS. A user delegation SAS applies to Blob storage only.\nService SAS\n: A service SAS is secured with the storage account key. A service SAS delegates access to a resource in the following Azure Storage services: Blob storage, Queue storage, Table storage, or Azure Files.\nAccount SAS\n: An account SAS is secured with the storage account key. An account SAS delegates access to resources in one or more of the storage services. All of the operations available via a service or user delegation SAS are also available via an account SAS.\nNote\nMicrosoft recommends that you use Microsoft Entra credentials when possible as a security best practice, rather than using the account key, which can be more easily compromised. When your application design requires shared access signatures for access to Blob storage, use Microsoft Entra credentials to create a user delegation SAS when possible for superior security\nHow shared access signatures work\nWhen you use a SAS to access data stored in Azure Storage, you need two components. The first is a URI to the resource you want to access. The second part is a SAS token that you've created to authorize access to that resource.\nIn a single URI, such as\nhttps://medicalrecords.blob.core.windows.net/patient-images/patient-116139-nq8z7f.jpg?sp=r&st=2020-01-20T11:42:32Z&se=2020-01-20T19:42:32Z&spr=https&sv=2019-02-02&sr=b&sig=SrW1HZ5Nb6MbRzTbXCaPm%2BJiSEn15tC91Y4umMPwVZs%3D\n, you can separate the URI from the SAS token as follows:\nURI:\nhttps://medicalrecords.blob.core.windows.net/patient-images/patient-116139-nq8z7f.jpg?\nSAS token:\nsp=r&st=2020-01-20T11:42:32Z&se=2020-01-20T19:42:32Z&spr=https&sv=2019-02-02&sr=b&sig=SrW1HZ5Nb6MbRzTbXCaPm%2BJiSEn15tC91Y4umMPwVZs%3D\nThe SAS token itself is made up of several components.\nComponent\nDescription\nsp=r\nControls the access rights. The values can be\na\nfor add,\nc\nfor create,\nd\nfor delete,\nl\nfor list,\nr\nfor read, or\nw\nfor write. This example is read only. The example\nsp=acdlrw\ngrants all the available rights.\nst=2020-01-20T11:42:32Z\nThe date and time when access starts.\nse=2020-01-20T19:42:32Z\nThe date and time when access ends. This example grants eight hours of access.\nsv=2019-02-02\nThe version of the storage API to use.\nsr=b\nThe kind of storage being accessed. In this example, b is for blob.\nsig=SrW1HZ5Nb6MbRzTbXCaPm%2BJiSEn15tC91Y4umMPwVZs%3D\nThe cryptographic signature.\nBest practices\nTo reduce the potential risks of using a SAS, Microsoft provides some guidance:\nTo securely distribute a SAS and prevent man-in-the-middle attacks, always use HTTPS.\nThe most secure SAS is a user delegation SAS. Use it wherever possible because it removes the need to store your storage account key in code. You must use Microsoft Entra ID to manage credentials. This option might not be possible for your solution.\nTry to set your expiration time to the smallest useful value. If a SAS key becomes compromised, it can be exploited for only a short time.\nApply the rule of minimum-required privileges. Only grant the access that's required. For example, in your app, read-only access is sufficient.\nThere are some situations where a SAS isn't the correct solution. When there's an unacceptable risk of using a SAS, create a middle-tier service to manage users and their access to storage.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Discover shared access signatures"
                },
                {
                  "level": 2,
                  "text": "Types of shared access signatures"
                },
                {
                  "level": 2,
                  "text": "How shared access signatures work"
                },
                {
                  "level": 2,
                  "text": "Best practices"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [
                "https://medicalrecords.blob.core.windows.net/patient-images/patient-116139-nq8z7f.jpg?sp=r&st=2020-01-20T11:42:32Z&se=2020-01-20T19:42:32Z&spr=https&sv=2019-02-02&sr=b&sig=SrW1HZ5Nb6MbRzTbXCaPm%2BJiSEn15tC91Y4umMPwVZs%3D",
                "https://medicalrecords.blob.core.windows.net/patient-images/patient-116139-nq8z7f.jpg?",
                "sp=r&st=2020-01-20T11:42:32Z&se=2020-01-20T19:42:32Z&spr=https&sv=2019-02-02&sr=b&sig=SrW1HZ5Nb6MbRzTbXCaPm%2BJiSEn15tC91Y4umMPwVZs%3D",
                "st=2020-01-20T11:42:32Z",
                "se=2020-01-20T19:42:32Z",
                "sv=2019-02-02",
                "sig=SrW1HZ5Nb6MbRzTbXCaPm%2BJiSEn15tC91Y4umMPwVZs%3D"
              ],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 3,
              "title": "Choose when to use shared access signatures",
              "url": "https://learn.microsoft.com/en-us/training/modules/implement-shared-access-signatures/3-shared-access-signatures",
              "href": "3-shared-access-signatures",
              "content": "Read in English\nAdd\nAdd to plan\nChoose when to use shared access signatures\nCompleted\n3 minutes\nUse a SAS when you want to provide secure access to resources in your storage account to any client who doesn't otherwise have permissions to those resources.\nA common scenario where a SAS is useful is a service where users read and write their own data to your storage account. In a scenario where a storage account stores user data, there are two typical design patterns:\nClients upload and download data via a front-end proxy service, which performs authentication. This front-end proxy service has the advantage of allowing validation of business rules, but for large amounts of data or high-volume transactions, creating a service that can scale to match demand may be expensive or difficult.\nA lightweight service authenticates the client as needed and then generates a SAS. Once the client application receives the SAS, they can access storage account resources directly with the permissions defined by the SAS and for the interval allowed by the SAS. The SAS mitigates the need for routing all data through the front-end proxy service.\nMany real-world services might use a hybrid of these two approaches. For example, some data might be processed and validated via the front-end proxy, while other data is saved and/or read directly using SAS.\nAdditionally, a SAS is required to authorize access to the source object in a copy operation in certain scenarios:\nWhen you copy a blob to another blob that resides in a different storage account, you must use a SAS to authorize access to the source blob. You can optionally use a SAS to authorize access to the destination blob as well.\nWhen you copy a file to another file that resides in a different storage account, you must use a SAS to authorize access to the source file. You can optionally use a SAS to authorize access to the destination file as well.\nWhen you copy a blob to a file, or a file to a blob, you must use a SAS to authorize access to the source object, even if the source and destination objects reside within the same storage account.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Choose when to use shared access signatures"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [
                {
                  "src": "../../wwl-azure/implement-shared-access-signatures/media/storage-proxy-service.png",
                  "absolute_url": "https://learn.microsoft.com/en-us/wwl-azure/implement-shared-access-signatures/media/storage-proxy-service.png",
                  "alt_text": "Scenario diagram: Front-end proxy service",
                  "title": "",
                  "filename": "storage-proxy-service.png",
                  "image_type": "diagram",
                  "context": {
                    "preceding_heading": "Choose when to use shared access signatures",
                    "following_text": "A lightweight service authenticates the client as needed and then generates a SAS. Once the client application receives the SAS, they can access storage account resources directly with the permissions",
                    "figure_caption": "",
                    "parent_section": ""
                  }
                },
                {
                  "src": "../../wwl-azure/implement-shared-access-signatures/media/storage-provider-service.png",
                  "absolute_url": "https://learn.microsoft.com/en-us/wwl-azure/implement-shared-access-signatures/media/storage-provider-service.png",
                  "alt_text": "Scenario diagram: SAS provider service",
                  "title": "",
                  "filename": "storage-provider-service.png",
                  "image_type": "diagram",
                  "context": {
                    "preceding_heading": "Choose when to use shared access signatures",
                    "following_text": "Many real-world services might use a hybrid of these two approaches. For example, some data might be processed and validated via the front-end proxy, while other data is saved and/or read directly usi",
                    "figure_caption": "",
                    "parent_section": ""
                  }
                }
              ],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 4,
              "title": "Explore stored access policies",
              "url": "https://learn.microsoft.com/en-us/training/modules/implement-shared-access-signatures/4-stored-access-policies",
              "href": "4-stored-access-policies",
              "content": "Read in English\nAdd\nAdd to plan\nExplore stored access policies\nCompleted\n3 minutes\nA stored access policy provides an extra level of control over service-level shared access signatures (SAS) on the server side. Establishing a stored access policy groups SAS and provides more restrictions for signatures that bound by the policy. You can use a stored access policy to change the start time, expiry time, or permissions for a signature, or to revoke it after it is issued.\nThe following storage resources support stored access policies:\nBlob containers\nFile shares\nQueues\nTables\nCreating a stored access policy\nThe access policy for a SAS consists of the start time, expiry time, and permissions for the signature. You can specify all of these parameters on the signature URI and none within the stored access policy; all on the stored access policy and none on the URI; or some combination of the two. However, you can't specify a given parameter on both the SAS token and the stored access policy.\nTo create or modify a stored access policy, call the\nSet ACL\noperation for the resource (see\nSet Container ACL\n,\nSet Queue ACL\n,\nSet Table ACL\n, or\nSet Share ACL\n) with a request body that specifies the terms of the access policy. The body of the request includes a unique signed identifier of your choosing, up to 64 characters in length, and the optional parameters of the access policy, as follows:\nNote\nWhen you establish a stored access policy on a container, table, queue, or share, it may take up to 30 seconds to take effect. During this time requests against a SAS associated with the stored access policy may fail with status code 403 (Forbidden), until the access policy becomes active. Table entity range restrictions (\nstartpk\n,\nstartrk\n,\nendpk\n, and\nendrk\n) cannot be specified in a stored access policy.\nFollowing are examples of creating a stored access policy by using C# .NET and the Azure CLI.\nBlobSignedIdentifier identifier = new BlobSignedIdentifier\n{\n    Id = \"stored access policy identifier\",\n    AccessPolicy = new BlobAccessPolicy\n    {\n        ExpiresOn = DateTimeOffset.UtcNow.AddHours(1),\n        Permissions = \"rw\"\n    }\n};\n\nblobContainer.SetAccessPolicy(permissions: new BlobSignedIdentifier[] { identifier });\naz storage container policy create \\\n    --name <stored access policy identifier> \\\n    --container-name <container name> \\\n    --start <start time UTC datetime> \\\n    --expiry <expiry time UTC datetime> \\\n    --permissions <(a)dd, (c)reate, (d)elete, (l)ist, (r)ead, or (w)rite> \\\n    --account-key <storage account key> \\\n    --account-name <storage account name> \\\nModifying or revoking a stored access policy\nTo modify the parameters of the stored access policy you can call the access control list operation for the resource type to replace the existing policy. For example, if your existing policy grants read and write permissions to a resource, you can modify it to grant only read permissions for all future requests.\nTo revoke a stored access policy you can delete it, rename it by changing the signed identifier, or change the expiry time to a value in the past. Changing the signed identifier breaks the associations between any existing signatures and the stored access policy. Changing the expiry time to a value in the past causes any associated signatures to expire. Deleting or modifying the stored access policy immediately affects all of the SAS associated with it.\nTo remove a single access policy, call the resource's\nSet ACL\noperation, passing in the set of signed identifiers that you wish to maintain on the container. To remove all access policies from the resource, call the\nSet ACL\noperation with an empty request body.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Explore stored access policies"
                },
                {
                  "level": 2,
                  "text": "Creating a stored access policy"
                },
                {
                  "level": 2,
                  "text": "Modifying or revoking a stored access policy"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [
                "BlobSignedIdentifier identifier = new BlobSignedIdentifier\n{\n    Id = \"stored access policy identifier\",\n    AccessPolicy = new BlobAccessPolicy\n    {\n        ExpiresOn = DateTimeOffset.UtcNow.AddHours(1),\n        Permissions = \"rw\"\n    }\n};\n\nblobContainer.SetAccessPolicy(permissions: new BlobSignedIdentifier[] { identifier });",
                "BlobSignedIdentifier identifier = new BlobSignedIdentifier\n{\n    Id = \"stored access policy identifier\",\n    AccessPolicy = new BlobAccessPolicy\n    {\n        ExpiresOn = DateTimeOffset.UtcNow.AddHours(1),\n        Permissions = \"rw\"\n    }\n};\n\nblobContainer.SetAccessPolicy(permissions: new BlobSignedIdentifier[] { identifier });",
                "az storage container policy create \\\n    --name <stored access policy identifier> \\\n    --container-name <container name> \\\n    --start <start time UTC datetime> \\\n    --expiry <expiry time UTC datetime> \\\n    --permissions <(a)dd, (c)reate, (d)elete, (l)ist, (r)ead, or (w)rite> \\\n    --account-key <storage account key> \\\n    --account-name <storage account name> \\",
                "az storage container policy create \\\n    --name <stored access policy identifier> \\\n    --container-name <container name> \\\n    --start <start time UTC datetime> \\\n    --expiry <expiry time UTC datetime> \\\n    --permissions <(a)dd, (c)reate, (d)elete, (l)ist, (r)ead, or (w)rite> \\\n    --account-key <storage account key> \\\n    --account-name <storage account name> \\"
              ],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                },
                {
                  "url": "/en-us/rest/api/storageservices/set-container-acl",
                  "text": "Set Container ACL"
                },
                {
                  "url": "/en-us/rest/api/storageservices/set-queue-acl",
                  "text": "Set Queue ACL"
                },
                {
                  "url": "/en-us/rest/api/storageservices/set-table-acl",
                  "text": "Set Table ACL"
                },
                {
                  "url": "/en-us/rest/api/storageservices/set-share-acl",
                  "text": "Set Share ACL"
                }
              ]
            },
            {
              "number": 5,
              "title": "Module assessment",
              "url": "https://learn.microsoft.com/en-us/training/modules/implement-shared-access-signatures/5-knowledge-check",
              "href": "5-knowledge-check",
              "content": "Read in English\nAdd\nAdd to plan\nModule assessment\nCompleted\n3 minutes\nCheck your knowledge\n1.\nWhich of the following types of shared access signatures (SAS) applies to Blob storage only?\nAccount SAS\nService SAS\nUser delegation SAS\n2.\nWhich of the following best practices provides the most flexible and secure way to use a service shared access signature (SAS)?\nAssociate SAS tokens with a stored access policy.\nAlways use HTTPS\nImplement a user delegation SAS\nYou must answer all questions before checking your work.\nYou must answer all questions before checking your work.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Module assessment"
                },
                {
                  "level": 2,
                  "text": "Check your knowledge"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 6,
              "title": "Summary",
              "url": "https://learn.microsoft.com/en-us/training/modules/implement-shared-access-signatures/6-summary",
              "href": "6-summary",
              "content": "Read in English\nAdd\nAdd to plan\nSummary\nCompleted\n3 minutes\nIn this module, you learned how to:\nIdentify the three types of shared access signatures\nExplain when to implement shared access signatures\nCreate a stored access policy\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Summary"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            }
          ]
        },
        {
          "title": "Explore Microsoft Graph",
          "url": "https://learn.microsoft.com/en-us/training/modules/microsoft-graph/",
          "description": "",
          "learning_objectives": [],
          "prerequisites": [],
          "units": [
            {
              "number": 1,
              "title": "Introduction",
              "url": "https://learn.microsoft.com/en-us/training/modules/microsoft-graph/1-introduction",
              "href": "1-introduction",
              "content": "Read in English\nAdd\nAdd to plan\nIntroduction\nCompleted\n3 minutes\nUse the wealth of data in Microsoft Graph to build apps for organizations and consumers that interact with millions of users.\nAfter completing this module, you'll be able to:\nExplain the benefits of using Microsoft Graph\nPerform operations on Microsoft Graph by using REST and SDKs\nApply best practices to help your applications get the most out of Microsoft Graph\nRetrieve user profile information with the Microsoft Graph SDK\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Introduction"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 2,
              "title": "Discover Microsoft Graph",
              "url": "https://learn.microsoft.com/en-us/training/modules/microsoft-graph/2-microsoft-graph-overview",
              "href": "2-microsoft-graph-overview",
              "content": "Read in English\nAdd\nAdd to plan\nDiscover Microsoft Graph\nCompleted\n3 minutes\nMicrosoft Graph is the gateway to data and intelligence in Microsoft 365. It provides a unified programmability model that you can use to access the tremendous amount of data in Microsoft 365, Windows 10, and Enterprise Mobility + Security.\nIn the Microsoft 365 platform, three main components facilitate the access and flow of data:\nThe Microsoft Graph API offers a single endpoint,\nhttps://graph.microsoft.com\n. You can use REST APIs or SDKs to access the endpoint. Microsoft Graph also includes services that manage user and device identity, access, compliance, and security.\nMicrosoft Graph connectors\nwork in the incoming direction,\ndelivering data external to the Microsoft cloud into Microsoft Graph services and applications\n, to enhance Microsoft 365 experiences such as Microsoft Search. Connectors exist for many commonly used data sources such as Box, Google Drive, Jira, and Salesforce.\nMicrosoft Graph Data Connect\nprovides a set of tools to streamline secure and scalable\ndelivery of Microsoft Graph data to popular Azure data stores\n. The cached data serves as data sources for Azure development tools that you can use to build intelligent applications.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Discover Microsoft Graph"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [
                "https://graph.microsoft.com"
              ],
              "images": [
                {
                  "src": "../../wwl-azure/microsoft-graph/media/microsoft-graph-data-connectors.png",
                  "absolute_url": "https://learn.microsoft.com/en-us/wwl-azure/microsoft-graph/media/microsoft-graph-data-connectors.png",
                  "alt_text": "Microsoft Graph, Microsoft Graph data connect, and Microsoft Graph connectors enable extending Microsoft 365 experiences and building intelligent apps.",
                  "title": "",
                  "filename": "microsoft-graph-data-connectors.png",
                  "image_type": "chart",
                  "context": {
                    "preceding_heading": "Discover Microsoft Graph",
                    "following_text": "In the Microsoft 365 platform, three main components facilitate the access and flow of data:",
                    "figure_caption": "",
                    "parent_section": ""
                  }
                }
              ],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                },
                {
                  "url": "/en-us/microsoftsearch/connectors-overview",
                  "text": "Microsoft Graph connectors"
                },
                {
                  "url": "/en-us/graph/overview#access-microsoft-graph-data-at-scale-using-microsoft-graph-data-connect",
                  "text": "Microsoft Graph Data Connect"
                }
              ]
            },
            {
              "number": 3,
              "title": "Query Microsoft Graph by using REST",
              "url": "https://learn.microsoft.com/en-us/training/modules/microsoft-graph/3-microsoft-graph-api",
              "href": "3-microsoft-graph-api",
              "content": "Read in English\nAdd\nAdd to plan\nQuery Microsoft Graph by using REST\nCompleted\n3 minutes\nMicrosoft Graph is a RESTful web API that enables you to access Microsoft Cloud service resources. After you register your app and get authentication tokens for a user or service, you can make requests to the Microsoft Graph API.\nThe Microsoft Graph API defines most of its resources, methods, and enumerations in the OData namespace,\nmicrosoft.graph\n, in the\nMicrosoft Graph metadata\n. A few API sets are defined in their subnamespaces, such as the\ncall records API\nwhich defines resources like\ncallRecord\nin\nmicrosoft.graph.callRecords\n.\nUnless explicitly specified in the corresponding topic, assume types, methods, and enumerations are part of the\nmicrosoft.graph\nnamespace.\nCall a REST API method\nTo read from or write to a resource such as a user or an email message, construct a request that looks like the following sample:\n{HTTP method} https://graph.microsoft.com/{version}/{resource}?{query-parameters}\nThe components of a request include:\n{HTTP method}\n- The HTTP method used on the request to Microsoft Graph.\n{version}\n- The version of the Microsoft Graph API your application is using.\n{resource}\n- The resource in Microsoft Graph that you're referencing.\n{query-parameters}\n- Optional OData query options or REST method parameters that customize the response.\nAfter you make a request, a response is returned that includes:\nStatus code - An HTTP status code that indicates success or failure.\nResponse message - The data that you requested or the result of the operation. The response message can be empty for some operations.\nnextLink\n- If your request returns numerous data, you need to page through it by using the URL returned in\n@odata.nextLink\n.\nHTTP methods\nMicrosoft Graph uses the HTTP method on your request to determine what your request is doing. The API supports the following methods.\nMethod\nDescription\nGET\nRead data from a resource.\nPOST\nCreate a new resource, or perform an action.\nPATCH\nUpdate a resource with new values.\nPUT\nReplace a resource with a new one.\nDELETE\nRemove a resource.\nFor the CRUD methods\nGET\nand\nDELETE\n, no request body is required.\nThe\nPOST\n,\nPATCH\n, and\nPUT\nmethods require a request body specified in JSON format that contains additional information. Such as the values for properties of the resource.\nVersion\nMicrosoft Graph currently supports two versions:\nv1.0\nand\nbeta\n.\nv1.0\nincludes generally available APIs. Use the v1.0 version for all production apps.\nbeta\nincludes APIs that are currently in preview. Because we might introduce breaking changes to our beta APIs, we recommend that you use the beta version only to test apps that are in development; don't use beta APIs in your production apps.\nResource\nA resource can be an entity or complex type, commonly defined with properties. Entities differ from complex types by always including an\nid\nproperty.\nYour URL includes the resource you're interacting with in the request, such as\nme\n,\nuser\n,\ngroup\n,\ndrive\n, and\nsite\n. Often, top-level resources also include\nrelationships\n, which you can use to access other resources, like\nme/messages\nor\nme/drive\n. You can also interact with resources using\nmethods\n; for example, to send an email, use\nme/sendMail\n.\nEach resource might require different permissions to access it. You often need a higher level of permissions to create or update a resource than to read it. For details about required permissions, see the method reference topic.\nQuery parameters\nQuery parameters can be OData system query options, or other strings that a method accepts to customize its response.\nYou can use optional OData system query options to include more or fewer properties than the default response. You can filter the response for items that match a custom query, or provide another parameters for a method.\nFor example, adding the following\nfilter\nparameter restricts the messages returned with the\nemailAddress\nproperty of\njon@contoso.com\n.\nGET https://graph.microsoft.com/v1.0/me/messages?filter=emailAddress eq 'jon@contoso.com'\nOther resources\nFollowing are links to some tools you can use to build and test requests using Microsoft Graph APIs.\nGraph Explorer\nPostman\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Query Microsoft Graph by using REST"
                },
                {
                  "level": 2,
                  "text": "Call a REST API method"
                },
                {
                  "level": 2,
                  "text": "HTTP methods"
                },
                {
                  "level": 2,
                  "text": "Version"
                },
                {
                  "level": 2,
                  "text": "Resource"
                },
                {
                  "level": 2,
                  "text": "Query parameters"
                },
                {
                  "level": 2,
                  "text": "Other resources"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [
                "microsoft.graph",
                "microsoft.graph.callRecords",
                "microsoft.graph",
                "{HTTP method} https://graph.microsoft.com/{version}/{resource}?{query-parameters}",
                "{HTTP method} https://graph.microsoft.com/{version}/{resource}?{query-parameters}",
                "{HTTP method}",
                "{query-parameters}",
                "@odata.nextLink",
                "me/messages",
                "me/sendMail",
                "emailAddress",
                "jon@contoso.com",
                "GET https://graph.microsoft.com/v1.0/me/messages?filter=emailAddress eq 'jon@contoso.com'",
                "GET https://graph.microsoft.com/v1.0/me/messages?filter=emailAddress eq 'jon@contoso.com'"
              ],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                },
                {
                  "url": "/en-us/graph/traverse-the-graph#microsoft-graph-api-metadata",
                  "text": "Microsoft Graph metadata"
                },
                {
                  "url": "/en-us/graph/api/resources/callrecords-api-overview",
                  "text": "call records API"
                },
                {
                  "url": "/en-us/graph/api/resources/callrecords-callrecord",
                  "text": "callRecord"
                },
                {
                  "url": "https://developer.microsoft.com/graph/graph-explorer",
                  "text": "Graph Explorer"
                },
                {
                  "url": "https://www.getpostman.com/",
                  "text": "Postman"
                }
              ]
            },
            {
              "number": 4,
              "title": "Query Microsoft Graph by using SDKs",
              "url": "https://learn.microsoft.com/en-us/training/modules/microsoft-graph/4-microsoft-graph-sdk",
              "href": "4-microsoft-graph-sdk",
              "content": "Read in English\nAdd\nAdd to plan\nQuery Microsoft Graph by using SDKs\nCompleted\n3 minutes\nThe Microsoft Graph SDKs are designed to simplify building high-quality, efficient, and resilient applications that access Microsoft Graph. The SDKs include two components: a service library and a core library.\nThe service library contains models and request builders that are generated from Microsoft Graph metadata to provide a rich and discoverable experience.\nThe core library provides a set of features that enhance working with all the Microsoft Graph services. Embedded support for retry handling, secure redirects, transparent authentication, and payload compression, improve the quality of your application's interactions with Microsoft Graph, with no added complexity, while leaving you completely in control. The core library also provides support for common tasks such as paging through collections and creating batch requests.\nIn this unit, you learn about the available SDKs and see some code examples of some of the most common operations.\nNote\nThe code samples in this unit are based on version 5.65 of the Microsoft Graph .NET SDK.\nInstall the Microsoft Graph .NET SDK\nThe Microsoft Graph .NET SDK is included in the following NuGet packages:\nMicrosoft.Graph\n- Contains the models and request builders for accessing the\nv1.0\nendpoint with the fluent API.\nMicrosoft.Graph\nhas a dependency on\nMicrosoft.Graph.Core\n.\nMicrosoft.Graph.Beta\n- Contains the models and request builders for accessing the\nbeta\nendpoint with the fluent API.\nMicrosoft.Graph.Beta\nhas a dependency on\nMicrosoft.Graph.Core\n.\nMicrosoft.Graph.Core\n- The core library for making calls to Microsoft Graph.\nCreate a Microsoft Graph client\nThe Microsoft Graph client is designed to make it simple to make calls to Microsoft Graph. You can use a single client instance for the lifetime of the application. The following code examples show how to create an instance of a Microsoft Graph client. The authentication provider handles acquiring access tokens for the application. The different application providers support different client scenarios. For details about which provider and options are appropriate for your scenario, see\nChoose an Authentication Provider\n.\nvar scopes = new[] { \"User.Read\" };\n\n// Multi-tenant apps can use \"common\",\n// single-tenant apps must use the tenant ID from the Azure portal\nvar tenantId = \"common\";\n\n// Value from app registration\nvar clientId = \"YOUR_CLIENT_ID\";\n\n// using Azure.Identity;\nvar options = new TokenCredentialOptions\n{\n    AuthorityHost = AzureAuthorityHosts.AzurePublicCloud\n};\n\n// Callback function that receives the user prompt\n// Prompt contains the generated device code that you must\n// enter during the auth process in the browser\nFunc<DeviceCodeInfo, CancellationToken, Task> callback = (code, cancellation) => {\n    Console.WriteLine(code.Message);\n    return Task.FromResult(0);\n};\n\n// /dotnet/api/azure.identity.devicecodecredential\nvar deviceCodeCredential = new DeviceCodeCredential(\n    callback, tenantId, clientId, options);\n\nvar graphClient = new GraphServiceClient(deviceCodeCredential, scopes);\nRead information from Microsoft Graph\nTo read information from Microsoft Graph, you first need to create a request object, and then run the\nGET\nmethod on the request.\n// GET https://graph.microsoft.com/v1.0/me\n\nvar user = await graphClient.Me\n    .GetAsync();\nRetrieve a list of entities\nRetrieving a list of entities is similar to retrieving a single entity except there are other options for configuring the request. The\n$filter\nquery parameter can be used to reduce the result set to only those rows that match the provided condition. The\n$orderBy\nquery parameter requests that the server provides the list of entities sorted by the specified properties.\n// GET https://graph.microsoft.com/v1.0/me/messages?\n// $select=subject,sender&$filter=subject eq 'Hello world'\nvar messages = await graphClient.Me.Messages\n    .GetAsync(requestConfig =>\n    {\n        requestConfig.QueryParameters.Select =\n            [\"subject\", \"sender\"];\n        requestConfig.QueryParameters.Filter =\n            \"subject eq 'Hello world'\";\n    });\nDelete an entity\nDelete requests are constructed in the same way as requests to retrieve an entity, but use a\nDELETE\nrequest instead of a\nGET\n.\n// DELETE https://graph.microsoft.com/v1.0/me/messages/{message-id}\n// messageId is a string containing the id property of the message\nawait graphClient.Me.Messages[messageId]\n    .DeleteAsync();\nCreate a new entity\nFor fluent style and template-based SDKs, new items can be added to collections with a\nPOST\nmethod.\n// POST https://graph.microsoft.com/v1.0/me/calendars\nvar calendar = new Calendar\n{\n    Name = \"Volunteer\",\n};\n\nvar newCalendar = await graphClient.Me.Calendars\n    .PostAsync(calendar);\nOther resources\nMicrosoft Graph REST API v1.0 reference\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Query Microsoft Graph by using SDKs"
                },
                {
                  "level": 2,
                  "text": "Install the Microsoft Graph .NET SDK"
                },
                {
                  "level": 2,
                  "text": "Create a Microsoft Graph client"
                },
                {
                  "level": 2,
                  "text": "Read information from Microsoft Graph"
                },
                {
                  "level": 2,
                  "text": "Retrieve a list of entities"
                },
                {
                  "level": 2,
                  "text": "Delete an entity"
                },
                {
                  "level": 2,
                  "text": "Create a new entity"
                },
                {
                  "level": 2,
                  "text": "Other resources"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [
                "Microsoft.Graph",
                "Microsoft.Graph.Core",
                "Microsoft.Graph.Beta",
                "Microsoft.Graph.Core",
                "var scopes = new[] { \"User.Read\" };\n\n// Multi-tenant apps can use \"common\",\n// single-tenant apps must use the tenant ID from the Azure portal\nvar tenantId = \"common\";\n\n// Value from app registration\nvar clientId = \"YOUR_CLIENT_ID\";\n\n// using Azure.Identity;\nvar options = new TokenCredentialOptions\n{\n    AuthorityHost = AzureAuthorityHosts.AzurePublicCloud\n};\n\n// Callback function that receives the user prompt\n// Prompt contains the generated device code that you must\n// enter during the auth process in the browser\nFunc<DeviceCodeInfo, CancellationToken, Task> callback = (code, cancellation) => {\n    Console.WriteLine(code.Message);\n    return Task.FromResult(0);\n};\n\n// /dotnet/api/azure.identity.devicecodecredential\nvar deviceCodeCredential = new DeviceCodeCredential(\n    callback, tenantId, clientId, options);\n\nvar graphClient = new GraphServiceClient(deviceCodeCredential, scopes);",
                "var scopes = new[] { \"User.Read\" };\n\n// Multi-tenant apps can use \"common\",\n// single-tenant apps must use the tenant ID from the Azure portal\nvar tenantId = \"common\";\n\n// Value from app registration\nvar clientId = \"YOUR_CLIENT_ID\";\n\n// using Azure.Identity;\nvar options = new TokenCredentialOptions\n{\n    AuthorityHost = AzureAuthorityHosts.AzurePublicCloud\n};\n\n// Callback function that receives the user prompt\n// Prompt contains the generated device code that you must\n// enter during the auth process in the browser\nFunc<DeviceCodeInfo, CancellationToken, Task> callback = (code, cancellation) => {\n    Console.WriteLine(code.Message);\n    return Task.FromResult(0);\n};\n\n// /dotnet/api/azure.identity.devicecodecredential\nvar deviceCodeCredential = new DeviceCodeCredential(\n    callback, tenantId, clientId, options);\n\nvar graphClient = new GraphServiceClient(deviceCodeCredential, scopes);",
                "// GET https://graph.microsoft.com/v1.0/me\n\nvar user = await graphClient.Me\n    .GetAsync();",
                "// GET https://graph.microsoft.com/v1.0/me\n\nvar user = await graphClient.Me\n    .GetAsync();",
                "// GET https://graph.microsoft.com/v1.0/me/messages?\n// $select=subject,sender&$filter=subject eq 'Hello world'\nvar messages = await graphClient.Me.Messages\n    .GetAsync(requestConfig =>\n    {\n        requestConfig.QueryParameters.Select =\n            [\"subject\", \"sender\"];\n        requestConfig.QueryParameters.Filter =\n            \"subject eq 'Hello world'\";\n    });",
                "// GET https://graph.microsoft.com/v1.0/me/messages?\n// $select=subject,sender&$filter=subject eq 'Hello world'\nvar messages = await graphClient.Me.Messages\n    .GetAsync(requestConfig =>\n    {\n        requestConfig.QueryParameters.Select =\n            [\"subject\", \"sender\"];\n        requestConfig.QueryParameters.Filter =\n            \"subject eq 'Hello world'\";\n    });",
                "// DELETE https://graph.microsoft.com/v1.0/me/messages/{message-id}\n// messageId is a string containing the id property of the message\nawait graphClient.Me.Messages[messageId]\n    .DeleteAsync();",
                "// DELETE https://graph.microsoft.com/v1.0/me/messages/{message-id}\n// messageId is a string containing the id property of the message\nawait graphClient.Me.Messages[messageId]\n    .DeleteAsync();",
                "// POST https://graph.microsoft.com/v1.0/me/calendars\nvar calendar = new Calendar\n{\n    Name = \"Volunteer\",\n};\n\nvar newCalendar = await graphClient.Me.Calendars\n    .PostAsync(calendar);",
                "// POST https://graph.microsoft.com/v1.0/me/calendars\nvar calendar = new Calendar\n{\n    Name = \"Volunteer\",\n};\n\nvar newCalendar = await graphClient.Me.Calendars\n    .PostAsync(calendar);"
              ],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                },
                {
                  "url": "https://github.com/microsoftgraph/msgraph-sdk-dotnet",
                  "text": "Microsoft.Graph"
                },
                {
                  "url": "https://github.com/microsoftgraph/msgraph-beta-sdk-dotnet",
                  "text": "Microsoft.Graph.Beta"
                },
                {
                  "url": "https://github.com/microsoftgraph/msgraph-sdk-dotnet",
                  "text": "Microsoft.Graph.Core"
                },
                {
                  "url": "/en-us/graph/sdks/choose-authentication-providers",
                  "text": "Choose an Authentication Provider"
                },
                {
                  "url": "/en-us/graph/api/overview",
                  "text": "Microsoft Graph REST API v1.0 reference"
                }
              ]
            },
            {
              "number": 5,
              "title": "Apply best practices to Microsoft Graph",
              "url": "https://learn.microsoft.com/en-us/training/modules/microsoft-graph/5-microsoft-graph-best-practices",
              "href": "5-microsoft-graph-best-practices",
              "content": "Read in English\nAdd\nAdd to plan\nApply best practices to Microsoft Graph\nCompleted\n3 minutes\nThis unit describes best practices that you can apply to get the most out of Microsoft Graph and make your application more reliable for end users.\nAuthentication\nTo access the data in Microsoft Graph, your application needs to acquire an OAuth 2.0 access token, and presents it to Microsoft Graph in either of the following methods:\nThe HTTP\nAuthorization\nrequest header, as a\nBearer\ntoken\nThe graph client constructor, when using a Microsoft Graph client library\nUse the Microsoft Authentication Library API,\nMSAL\nto acquire the access token to Microsoft Graph.\nConsent and authorization\nApply the following best practices for consent and authorization in your app:\nUse least privilege\n. Only request permissions that are necessary, and only when you need them. For the APIs, your application calls check the permissions section in the method topics. For example, see\ncreating a user\nand choose the least privileged permissions.\nUse the correct permission type based on scenarios\n. If you're building an interactive application where a signed in user is present, your application should use\ndelegated\npermissions. If, however, your application runs without a signed-in user, such as a background service or daemon, your application should use application permissions.\nCaution\nUsing application permissions for interactive scenarios can put your application at compliance and security risk. Be sure to check user's privileges to ensure they don't have undesired access to information, or are circumnavigating policies configured by an administrator.\nConsider the end user and admin experience\n. Directly affects end user and admin experiences. For example:\nConsider who is consenting to your application, either end users or administrators, and configure your application to\nrequest permissions appropriately\n.\nEnsure that you understand the difference between\nstatic, dynamic, and incremental consent\n.\nConsider multi-tenant applications\n. Expect customers to have various application and consent controls in different states. For example:\nTenant administrators can disable the ability for end users to consent to applications. In this case, an administrator would need to consent on behalf of their users.\nTenant administrators can set custom authorization policies such as blocking users from reading other user's profiles, or limiting self-service group creation to a limited set of users. In this case, your application should expect to handle 403 error response when acting on behalf of a user.\nHandle responses effectively\nDepending on the requests you make to Microsoft Graph, your applications should be prepared to handle different types of responses. The following are some of the most important practices to follow to ensure that your application behaves reliably and predictably for your end users. For example:\nPagination\n: When querying resource collections, you should expect that Microsoft Graph returns the result set in multiple pages, due to server-side page size limits. Your application should\nalways\nhandle the possibility that the responses are paged in nature, and use the\n@odata.nextLink\nproperty to obtain the next paged set of results, until all pages of the result set are read. The final page doesn't include an\n@odata.nextLink\nproperty. For more information, visit\npaging\n.\nEvolvable enumerations\n: Adding members to existing enumerations can break applications already using these enums. Evolvable enums are a mechanism that Microsoft Graph API uses to add new members to existing enumerations without causing a breaking change for applications. By default, a GET operation returns only known members for properties of evolvable enum types and your application needs to handle only the known members. If you design your application to handle unknown members as well, you can opt in to receive those members by using an HTTP\nPrefer\nrequest header.\nStoring data locally\nYour application should ideally make calls to Microsoft Graph to retrieve data in real time as necessary. You should only cache or store data locally necessary for a specific scenario. If that use case is covered by your terms of use and privacy policy, and doesn't violate the\nMicrosoft APIs Terms of Use\n, your application should also implement proper retention and deletion policies.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Apply best practices to Microsoft Graph"
                },
                {
                  "level": 2,
                  "text": "Authentication"
                },
                {
                  "level": 2,
                  "text": "Consent and authorization"
                },
                {
                  "level": 2,
                  "text": "Handle responses effectively"
                },
                {
                  "level": 2,
                  "text": "Storing data locally"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [
                "@odata.nextLink",
                "@odata.nextLink"
              ],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                },
                {
                  "url": "/en-us/azure/active-directory/develop/active-directory-v2-libraries",
                  "text": "MSAL"
                },
                {
                  "url": "/en-us/graph/api/user-post-users",
                  "text": "creating a user"
                },
                {
                  "url": "/en-us/azure/active-directory/develop/v2-permissions-and-consent",
                  "text": "request permissions appropriately"
                },
                {
                  "url": "/en-us/azure/active-directory/develop/v2-permissions-and-consent#consent-types",
                  "text": "static, dynamic, and incremental consent"
                },
                {
                  "url": "/en-us/graph/paging",
                  "text": "paging"
                },
                {
                  "url": "/en-us/legal/microsoft-apis/terms-of-use?context=/graph/context",
                  "text": "Microsoft APIs Terms of Use"
                }
              ]
            },
            {
              "number": 6,
              "title": "Module assessment",
              "url": "https://learn.microsoft.com/en-us/training/modules/microsoft-graph/6-knowledge-check",
              "href": "6-knowledge-check",
              "content": "Read in English\nAdd\nAdd to plan\nModule assessment\nCompleted\n3 minutes\nCheck your knowledge\n1.\nWhich HTTP method is used to partially update a resource with new values?\nPOST\nPATCH\nPUT\n2.\nWhich of the components of the Microsoft 365 platform is used to deliver data external to Azure into Microsoft Graph services and applications?\nMicrosoft Graph API\nMicrosoft Graph connectors\nMicrosoft Graph Data Connect\nYou must answer all questions before checking your work.\nYou must answer all questions before checking your work.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Module assessment"
                },
                {
                  "level": 2,
                  "text": "Check your knowledge"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 7,
              "title": "Summary",
              "url": "https://learn.microsoft.com/en-us/training/modules/microsoft-graph/7-summary",
              "href": "7-summary",
              "content": "Read in English\nAdd\nAdd to plan\nSummary\nCompleted\n3 minutes\nIn this module, you learned how to:\nExplain the benefits of using Microsoft Graph\nPerform operations on Microsoft Graph by using REST and SDKs\nApply best practices to help your applications get the most out of Microsoft Graph\nRetrieve user profile information with the Microsoft Graph SDK\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Summary"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 900,
              "title": "Exercise - Retrieve user profile information with the Microsoft Graph SDK",
              "url": "https://learn.microsoft.com/en-us/training/modules/microsoft-graph/5a-exercise-microsoft-graph-user-profile",
              "href": "5a-exercise-microsoft-graph-user-profile",
              "content": "Read in English\nAdd\nAdd to plan\nExercise - Retrieve user profile information with the Microsoft Graph SDK\nCompleted\n15 minutes\nIn this exercise, you create a .NET app to authenticate with Microsoft Entra ID and request an access token, then call the Microsoft Graph API to retrieve and display your user profile information. You learn how to configure permissions and interact with Microsoft Graph from your application.\nTasks performed in this exercise:\nRegister an application with the Microsoft identity platform\nCreate a .NET console application that implements interactive authentication, and uses the\nGraphServiceClient\nclass to retrieve user profile information.\nThis exercise takes approximately\n15\nminutes to complete.\nBefore you start\nTo complete the exercise, you need:\nAn Azure subscription. If you don't already have one, you can sign up for one\nhttps://azure.microsoft.com/\n.\nVisual Studio Code\non one of the\nsupported platforms\n.\n.NET 8\nor greater.\nC# Dev Kit\nfor Visual Studio Code.\nGet started\nSelect the\nLaunch Exercise\nbutton to open the exercise instructions in a new browser window. When you're finished with the exercise, return here to:\nComplete the module\nEarn a badge for completing this module\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Exercise - Retrieve user profile information with the Microsoft Graph SDK"
                },
                {
                  "level": 2,
                  "text": "Before you start"
                },
                {
                  "level": 2,
                  "text": "Get started"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [
                {
                  "src": "../../wwl-azure/microsoft-graph/media/launch-exercise.png",
                  "absolute_url": "https://learn.microsoft.com/en-us/wwl-azure/microsoft-graph/media/launch-exercise.png",
                  "alt_text": "Button to launch exercise.",
                  "title": "",
                  "filename": "launch-exercise.png",
                  "image_type": "chart",
                  "context": {
                    "preceding_heading": "Get started",
                    "following_text": "Was this page helpful?",
                    "figure_caption": "",
                    "parent_section": ""
                  }
                }
              ],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                },
                {
                  "url": "https://azure.microsoft.com/",
                  "text": "https://azure.microsoft.com/"
                },
                {
                  "url": "https://code.visualstudio.com/",
                  "text": "Visual Studio Code"
                },
                {
                  "url": "https://code.visualstudio.com/docs/supporting/requirements#_platforms",
                  "text": "supported platforms"
                },
                {
                  "url": "https://dotnet.microsoft.com/en-us/download/dotnet/8.0",
                  "text": ".NET 8"
                },
                {
                  "url": "https://marketplace.visualstudio.com/items?itemName=ms-dotnettools.csdevkit",
                  "text": "C# Dev Kit"
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "title": "Implement secure Azure solutions",
      "url": "https://learn.microsoft.com/en-us/training/paths/az-204-implement-secure-cloud-solutions/",
      "learn_uid": "learn.wwl.az-204-implement-secure-cloud-solutions",
      "modules": [
        {
          "title": "Implement Azure Key Vault",
          "url": "https://learn.microsoft.com/en-us/training/modules/implement-azure-key-vault/",
          "description": "",
          "learning_objectives": [],
          "prerequisites": [],
          "units": [
            {
              "number": 1,
              "title": "Introduction",
              "url": "https://learn.microsoft.com/en-us/training/modules/implement-azure-key-vault/1-introduction",
              "href": "1-introduction",
              "content": "Read in English\nAdd\nAdd to plan\nIntroduction\nCompleted\n3 minutes\nAzure Key Vault is a cloud service for securely storing and accessing secrets. A secret is anything that you want to tightly control access to, such as API keys, passwords, certificates, or cryptographic keys.\nAfter completing this module, you'll be able to:\nDescribe the benefits of using Azure Key Vault\nExplain how to authenticate to Azure Key Vault\nCreate and retrieve secrets from Azure Key Vault\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Introduction"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 2,
              "title": "Explore Azure Key Vault",
              "url": "https://learn.microsoft.com/en-us/training/modules/implement-azure-key-vault/2-key-vault-overview",
              "href": "2-key-vault-overview",
              "content": "Read in English\nAdd\nAdd to plan\nExplore Azure Key Vault\nCompleted\n3 minutes\nThe Azure Key Vault service supports two types of containers: vaults and managed hardware security module(HSM) pools. Vaults support storing software and HSM-backed keys, secrets, and certificates. Managed HSM pools only support HSM-backed keys.\nAzure Key Vault helps solve the following problems:\nSecrets Management:\nAzure Key Vault can be used to Securely store and tightly control access to tokens, passwords, certificates, API keys, and other secrets\nKey Management:\nAzure Key Vault can also be used as a Key Management solution. Azure Key Vault makes it easy to create and control the encryption keys used to encrypt your data.\nCertificate Management:\nAzure Key Vault is also a service that lets you easily provision, manage, and deploy public and private Secure Sockets Layer/Transport Layer Security (SSL/TLS) certificates for use with Azure and your internal connected resources.\nAzure Key Vault has two service tiers: Standard, which encrypts with a software key, and a Premium tier, which includes hardware security module(HSM)-protected keys. To see a comparison between the Standard and Premium tiers, see the\nAzure Key Vault pricing page\n.\nKey benefits of using Azure Key Vault\nCentralized application secrets:\nCentralizing storage of application secrets in Azure Key Vault allows you to control their distribution. For example, instead of storing the connection string in the app's code you can store it securely in Key Vault. Your applications can securely access the information they need by using URIs. These URIs allow the applications to retrieve specific versions of a secret.\nSecurely store secrets and keys:\nAccess to a key vault requires proper authentication and authorization before a caller (user or application) can get access. Authentication is done via Microsoft Entra ID. Authorization might be done via Azure role-based access control (Azure RBAC) or Key Vault access policy. Azure RBAC can be used for both management of the vaults, and to access data stored in a vault. A key vault access policy can only be used when attempting to access data stored in a vault. Azure Key Vaults might be either software-protected or, with the Azure Key Vault Premium tier, hardware-protected by hardware security modules (HSMs).\nMonitor access and use:\nYou can monitor activity by enabling logging for your vaults. You have control over your logs and you might secure them by restricting access and you might also delete logs that you no longer need. Azure Key Vault can be configured to:\nArchive to a storage account.\nStream to an event hub.\nSend the logs to Azure Monitor logs.\nSimplified administration of application secrets:\nSecurity information must be secured, it must follow a life cycle, and it must be highly available. Azure Key Vault simplifies the process of meeting these requirements by:\nRemoving the need for in-house knowledge of Hardware Security Modules\nScaling up on short notice to meet your organizationâs usage spikes.\nReplicating the contents of your Key Vault within a region and to a secondary region. Data replication ensures high availability and takes away the need of any action from the administrator to trigger the failover.\nProviding standard Azure administration options via the portal, Azure CLI and PowerShell.\nAutomating certain tasks on certificates that you purchase from Public CAs, such as enrollment and renewal.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Explore Azure Key Vault"
                },
                {
                  "level": 2,
                  "text": "Key benefits of using Azure Key Vault"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                },
                {
                  "url": "https://azure.microsoft.com/pricing/details/key-vault/",
                  "text": "Azure Key Vault pricing page"
                }
              ]
            },
            {
              "number": 3,
              "title": "Discover Azure Key Vault best practices",
              "url": "https://learn.microsoft.com/en-us/training/modules/implement-azure-key-vault/3-key-vault-concepts",
              "href": "3-key-vault-concepts",
              "content": "Read in English\nAdd\nAdd to plan\nDiscover Azure Key Vault best practices\nCompleted\n3 minutes\nAzure Key Vault is a tool for securely storing and accessing secrets. A secret is anything that you want to tightly control access to, such as API keys, passwords, or certificates. A vault is logical group of secrets.\nAuthentication\nTo do any operations with Key Vault, you first need to authenticate to it. There are three ways to authenticate to Key Vault:\nManaged identities for Azure resources\n: When you deploy an app on a virtual machine in Azure, you can assign an identity to your virtual machine that has access to Key Vault. You can also assign identities to other Azure resources. The benefit of this approach is that the app or service isn't managing the rotation of the first secret. Azure automatically rotates the service principal client secret associated with the identity. We recommend this approach as a best practice.\nService principal and certificate\n: You can use a service principal and an associated certificate that has access to Key Vault. We don't recommend this approach because the application owner or developer must rotate the certificate.\nService principal and secret\n: Although you can use a service principal and a secret to authenticate to Key Vault, we don't recommend it. It's hard to automatically rotate the bootstrap secret that's used to authenticate to Key Vault.\nEncryption of data in transit\nAzure Key Vault enforces Transport Layer Security (TLS) protocol to protect data when itâs traveling between Azure Key Vault and clients. Clients negotiate a TLS connection with Azure Key Vault. TLS provides strong authentication, message privacy, and integrity (enabling detection of message tampering, interception, and forgery), interoperability, algorithm flexibility, and ease of deployment and use.\nPerfect Forward Secrecy (PFS) protects connections between customersâ client systems and Microsoft cloud services by unique keys. Connections also use RSA-based 2,048-bit encryption key lengths. This combination makes it difficult for someone to intercept and access data that is in transit.\nAzure Key Vault best practices\nUse separate key vaults:\nRecommended using a vault per application per environment (Development, Pre-Production and Production). This pattern helps you not share secrets across environments and also reduces the threat if there is a breach.\nControl access to your vault:\nKey Vault data is sensitive and business critical, you need to secure access to your key vaults by allowing only authorized applications and users.\nBackup:\nCreate regular back ups of your vault on update/delete/create of objects within a Vault.\nLogging:\nBe sure to turn on logging and alerts.\nRecovery options:\nTurn on\nsoft-delete\nand purge protection if you want to guard against force deletion of the secret.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Discover Azure Key Vault best practices"
                },
                {
                  "level": 2,
                  "text": "Authentication"
                },
                {
                  "level": 2,
                  "text": "Encryption of data in transit"
                },
                {
                  "level": 2,
                  "text": "Azure Key Vault best practices"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                },
                {
                  "url": "/en-us/azure/key-vault/general/soft-delete-overview",
                  "text": "soft-delete"
                }
              ]
            },
            {
              "number": 4,
              "title": "Authenticate to Azure Key Vault",
              "url": "https://learn.microsoft.com/en-us/training/modules/implement-azure-key-vault/4-key-vault-authentication",
              "href": "4-key-vault-authentication",
              "content": "Read in English\nAdd\nAdd to plan\nAuthenticate to Azure Key Vault\nCompleted\n3 minutes\nAuthentication with Key Vault works with Microsoft Entra ID, which is responsible for authenticating the identity of any given security principal. A security principal is anything that can request access to Azure resources. This includes:\nUsers â Real people with accounts in Microsoft Entra ID.\nGroups â Collections of users. Permissions given to the group apply to all its members.\nService Principals â Represent apps or services (not people). Think of it like a user account for an app.\nFor applications, there are two main ways to obtain a service principal:\nUse a managed identity (recommended): Azure creates and manages the service principal for you. The app can securely access other Azure services without storing credentials. Works with services like App Service, Azure Functions, and Virtual Machines.\nRegister the app manually: You register the app in Microsoft Entra ID. This creates a service principal and an app object that identifies the app across all tenants.\nNote\nIt's recommended to use a system-assigned managed identity.\nAuthentication to Key Vault in application code\nKey Vault SDK is using Azure Identity client library, which allows seamless authentication to Key Vault across environments with same code. The following table provides information on the Azure Identity client libraries:\n.NET\nPython\nJava\nJavaScript\nAzure Identity SDK .NET\nAzure Identity SDK Python\nAzure Identity SDK Java\nAzure Identity SDK JavaScript\nAuthentication to Key Vault with REST\nAccess tokens must be sent to the service using the HTTP Authorization header:\nPUT /keys/MYKEY?api-version=<api_version>  HTTP/1.1  \nAuthorization: Bearer <access_token>\nWhen an access token isn't supplied, or when the service rejects a token, an\nHTTP 401\nerror is returned to the client and includes the\nWWW-Authenticate\nheader, for example:\n401 Not Authorized  \nWWW-Authenticate: Bearer authorization=\"â¦\", resource=\"â¦\"\nThe parameters on the\nWWW-Authenticate\nheader are:\nauthorization: The address of the OAuth2 authorization service that might be used to obtain an access token for the request.\nresource: The name of the resource (\nhttps://vault.azure.net\n) to use in the authorization request.\nOther resources\nAzure Key Vault developer's guide\nAzure Key Vault availability and redundancy\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Authenticate to Azure Key Vault"
                },
                {
                  "level": 2,
                  "text": "Authentication to Key Vault in application code"
                },
                {
                  "level": 2,
                  "text": "Authentication to Key Vault with REST"
                },
                {
                  "level": 2,
                  "text": "Other resources"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [
                "PUT /keys/MYKEY?api-version=<api_version>  HTTP/1.1  \nAuthorization: Bearer <access_token>",
                "PUT /keys/MYKEY?api-version=<api_version>  HTTP/1.1  \nAuthorization: Bearer <access_token>",
                "WWW-Authenticate",
                "401 Not Authorized  \nWWW-Authenticate: Bearer authorization=\"â¦\", resource=\"â¦\"",
                "401 Not Authorized  \nWWW-Authenticate: Bearer authorization=\"â¦\", resource=\"â¦\"",
                "WWW-Authenticate",
                "https://vault.azure.net"
              ],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                },
                {
                  "url": "/en-us/dotnet/api/overview/azure/identity-readme",
                  "text": "Azure Identity SDK .NET"
                },
                {
                  "url": "/en-us/python/api/overview/azure/identity-readme",
                  "text": "Azure Identity SDK Python"
                },
                {
                  "url": "/en-us/java/api/overview/azure/identity-readme",
                  "text": "Azure Identity SDK Java"
                },
                {
                  "url": "/en-us/javascript/api/overview/azure/identity-readme",
                  "text": "Azure Identity SDK JavaScript"
                },
                {
                  "url": "/en-us/azure/key-vault/general/developers-guide",
                  "text": "Azure Key Vault developer's guide"
                },
                {
                  "url": "/en-us/azure/key-vault/general/disaster-recovery-guidance",
                  "text": "Azure Key Vault availability and redundancy"
                }
              ]
            },
            {
              "number": 5,
              "title": "Exercise - Create and retrieve secrets from Azure Key Vault",
              "url": "https://learn.microsoft.com/en-us/training/modules/implement-azure-key-vault/5-set-retrieve-secret-azure-key-vault",
              "href": "5-set-retrieve-secret-azure-key-vault",
              "content": "Read in English\nAdd\nAdd to plan\nExercise - Create and retrieve secrets from Azure Key Vault\nCompleted\n30 minutes\nIn this exercise, you create an Azure Key Vault, store secrets using the Azure CLI, and build a .NET console application that can create and retrieve secrets from the key vault. You learn how to configure authentication, manage secrets programmatically, and clean up resources when finished.\nTasks performed in this exercise:\nCreate Azure Key Vault resources\nStore a secret in a key vault using Azure CLI\nCreate a .NET console app to create and retrieve secrets\nClean up resources\nThis exercise takes approximately\n30\nminutes to complete.\nBefore you start\nTo complete the exercise, you need:\nAn Azure subscription. If you don't already have one, you can sign up for one\nhttps://azure.microsoft.com/\n.\nGet started\nSelect the\nLaunch Exercise\nbutton to open the exercise instructions in a new browser window. When you're finished with the exercise, return here to:\nComplete the module\nEarn a badge for completing this module\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Exercise - Create and retrieve secrets from Azure Key Vault"
                },
                {
                  "level": 2,
                  "text": "Before you start"
                },
                {
                  "level": 2,
                  "text": "Get started"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [
                {
                  "src": "../../wwl-azure/implement-azure-key-vault/media/launch-exercise.png",
                  "absolute_url": "https://learn.microsoft.com/en-us/wwl-azure/implement-azure-key-vault/media/launch-exercise.png",
                  "alt_text": "Button to launch exercise.",
                  "title": "",
                  "filename": "launch-exercise.png",
                  "image_type": "icon",
                  "context": {
                    "preceding_heading": "Get started",
                    "following_text": "Was this page helpful?",
                    "figure_caption": "",
                    "parent_section": ""
                  }
                }
              ],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                },
                {
                  "url": "https://azure.microsoft.com/",
                  "text": "https://azure.microsoft.com/"
                }
              ]
            },
            {
              "number": 6,
              "title": "Module assessment",
              "url": "https://learn.microsoft.com/en-us/training/modules/implement-azure-key-vault/6-knowledge-check",
              "href": "6-knowledge-check",
              "content": "Read in English\nAdd\nAdd to plan\nModule assessment\nCompleted\n3 minutes\nCheck your knowledge\n1.\nWhich of the below methods of authenticating to Azure Key Vault is recommended for most scenarios?\nService principal and certificate\nService principal and secret\nManaged identities\n2.\nAzure Key Vault protects data when it's traveling between Azure Key Vault and clients. What protocol does it use for encryption?\nSecure Sockets Layer\nTransport Layer Security\nPresentation Layer\nYou must answer all questions before checking your work.\nYou must answer all questions before checking your work.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Module assessment"
                },
                {
                  "level": 2,
                  "text": "Check your knowledge"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 7,
              "title": "Summary",
              "url": "https://learn.microsoft.com/en-us/training/modules/implement-azure-key-vault/7-summary",
              "href": "7-summary",
              "content": "Read in English\nAdd\nAdd to plan\nSummary\nCompleted\n3 minutes\nIn this module, you learned how to:\nDescribe the benefits of using Azure Key Vault\nExplain how to authenticate to Azure Key Vault\nCreate and retrieve secrets from Azure Key Vault\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Summary"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            }
          ]
        },
        {
          "title": "Implement managed identities",
          "url": "https://learn.microsoft.com/en-us/training/modules/implement-managed-identities/",
          "description": "",
          "learning_objectives": [],
          "prerequisites": [],
          "units": [
            {
              "number": 1,
              "title": "Introduction",
              "url": "https://learn.microsoft.com/en-us/training/modules/implement-managed-identities/1-introduction",
              "href": "1-introduction",
              "content": "Read in English\nAdd\nAdd to plan\nIntroduction\nCompleted\n3 minutes\nA common challenge for developers is the management of secrets and credentials used to secure communication between different components making up a solution. Managed identities eliminate the need for developers to manage credentials.\nAfter completing this module, you'll be able to:\nExplain the differences between the two types of managed identities\nDescribe the flows for user- and system-assigned managed identities\nConfigure managed identities\nAcquire access tokens by using REST and code\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Introduction"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 2,
              "title": "Explore managed identities",
              "url": "https://learn.microsoft.com/en-us/training/modules/implement-managed-identities/2-managed-identities-overview",
              "href": "2-managed-identities-overview",
              "content": "Read in English\nAdd\nAdd to plan\nExplore managed identities\nCompleted\n3 minutes\nA common challenge for developers is the management of secrets, credentials, certificates, and keys used to secure communication between services. Managed identities eliminate the need for developers to manage these credentials.\nWhile developers can securely store the secrets in Azure Key Vault, services need a way to access Azure Key Vault. Managed identities provide an automatically managed identity in Microsoft Entra ID for applications to use when connecting to resources that support Microsoft Entra authentication. Applications can use managed identities to obtain Microsoft Entra tokens without having to manage any credentials.\nTypes of managed identities\nThere are two types of managed identities:\nA\nsystem-assigned managed identity\nis enabled directly on an Azure service instance. When the identity is enabled, Azure creates an identity for the instance in the Microsoft Entra tenant trusted by the subscription of the instance. After the identity is created, the credentials are provisioned onto the instance. The lifecycle of a system-assigned identity is directly tied to the Azure service instance that it's enabled on. If the instance is deleted, Azure automatically cleans up the credentials and the identity in Microsoft Entra ID.\nA\nuser-assigned managed identity\nis created as a standalone Azure resource. Through a create process, Azure creates an identity in the Microsoft Entra tenant that's trusted by the subscription in use. After the identity is created, the identity can be assigned to one or more Azure service instances. The lifecycle of a user-assigned identity is managed separately from the lifecycle of the Azure service instances to which it's assigned.\nInternally, managed identities are service principals of a special type, which are locked to only be used with Azure resources. When the managed identity is deleted, the corresponding service principal is automatically removed.\nCharacteristics of managed identities\nThe following table highlights some of the key differences between the two types of managed identities.\nProperty\nSystem-assigned managed identity\nUser-assigned managed identity\nCreation\nCreated as part of an Azure resource (for example, an Azure virtual machine or Azure App Service)\nCreated as a stand-alone Azure resource\nLifecycle\nShared lifecycle with the Azure resource that the managed identity is created with. When the parent resource is deleted, the managed identity is deleted as well.\nIndependent life-cycle. Must be explicitly deleted.\nSharing across Azure resources\nCan't be shared, it can only be associated with a single Azure resource.\nCan be shared. The same user-assigned managed identity can be associated with more than one Azure resource.\nFollowing are common use cases for managed identities:\nSystem-assigned managed identity\nWorkloads contained within a single Azure resource.\nWorkloads needing independent identities.\nFor example, an application that runs on a single virtual machine.\nUser-assigned managed identity\nWorkloads that run on multiple resources and can share a single identity.\nWorkloads needing preauthorization to a secure resource, as part of a provisioning flow.\nWorkloads where resources are recycled frequently, but permissions should stay consistent.\nFor example, a workload where multiple virtual machines need to access the same resource.\nWhat Azure services support managed identities?\nManaged identities for Azure resources can be used to authenticate to services that support Microsoft Entra authentication. For a list of Azure services that support the managed identities for Azure resources feature, visit\nServices that support managed identities for Azure resources\n.\nThe rest of this module uses Azure virtual machines in the examples, but the same concepts and similar actions can be applied to any resource in Azure that supports Microsoft Entra authentication.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Explore managed identities"
                },
                {
                  "level": 2,
                  "text": "Types of managed identities"
                },
                {
                  "level": 2,
                  "text": "Characteristics of managed identities"
                },
                {
                  "level": 2,
                  "text": "What Azure services support managed identities?"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                },
                {
                  "url": "/en-us/azure/active-directory/managed-identities-azure-resources/services-support-msi",
                  "text": "Services that support managed identities for Azure resources"
                }
              ]
            },
            {
              "number": 3,
              "title": "Discover the managed identities authentication flow",
              "url": "https://learn.microsoft.com/en-us/training/modules/implement-managed-identities/3-managed-identities-auzre-virtual-machines",
              "href": "3-managed-identities-auzre-virtual-machines",
              "content": "Read in English\nAdd\nAdd to plan\nDiscover the managed identities authentication flow\nCompleted\n3 minutes\nIn this unit, you learn how managed identities work with Azure virtual machines. Below are the flows detailing how the two types of managed identities work with an Azure virtual machine.\nHow a system-assigned managed identity works with an Azure virtual machine\nAzure Resource Manager receives a request to enable the system-assigned managed identity on a virtual machine.\nAzure Resource Manager creates a service principal in Microsoft Entra ID for the identity of the virtual machine. The service principal is created in the Microsoft Entra tenant that's trusted by the subscription.\nAzure Resource Manager configures the identity on the virtual machine by updating the Azure Instance Metadata Service identity endpoint with the service principal client ID and certificate.\nAfter the virtual machine has an identity, use the service principal information to grant the virtual machine access to Azure resources. To call Azure Resource Manager, use role-based access control in Microsoft Entra ID to assign the appropriate role to the virtual machine service principal. To call Key Vault, grant your code access to the specific secret or key in Key Vault.\nYour code that's running on the virtual machine can request a token from the Azure Instance Metadata service endpoint, accessible only from within the virtual machine:\nhttp://169.254.169.254/metadata/identity/oauth2/token\nA call is made to Microsoft Entra ID to request an access token (as specified in step 5) by using the client ID and certificate configured in step 3. Microsoft Entra ID returns a JSON Web Token (JWT) access token.\nYour code sends the access token on a call to a service that supports Microsoft Entra authentication.\nHow a user-assigned managed identity works with an Azure virtual machine\nAzure Resource Manager receives a request to create a user-assigned managed identity.\nAzure Resource Manager creates a service principal in Microsoft Entra ID for the user-assigned managed identity. The service principal is created in the Microsoft Entra tenant that's trusted by the subscription.\nAzure Resource Manager receives a request to configure the user-assigned managed identity on a virtual machine and updates the Azure Instance Metadata Service identity endpoint with the user-assigned managed identity service principal client ID and certificate.\nAfter the user-assigned managed identity is created, use the service principal information to grant the identity access to Azure resources. To call Azure Resource Manager, use role-based access control in Microsoft Entra ID to assign the appropriate role to the service principal of the user-assigned identity. To call Key Vault, grant your code access to the specific secret or key in Key Vault.\nNote\nYou can also do this step before step 3.\nYour code that's running on the virtual machine can request a token from the Azure Instance Metadata Service identity endpoint, accessible only from within the virtual machine:\nhttp://169.254.169.254/metadata/identity/oauth2/token\nA call is made to Microsoft Entra ID to request an access token (as specified in step 5) by using the client ID and certificate configured in step 3. Microsoft Entra ID returns a JSON Web Token (JWT) access token.\nYour code sends the access token on a call to a service that supports Microsoft Entra authentication.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Discover the managed identities authentication flow"
                },
                {
                  "level": 2,
                  "text": "How a system-assigned managed identity works with an Azure virtual machine"
                },
                {
                  "level": 2,
                  "text": "How a user-assigned managed identity works with an Azure virtual machine"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [
                "http://169.254.169.254/metadata/identity/oauth2/token",
                "http://169.254.169.254/metadata/identity/oauth2/token"
              ],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 4,
              "title": "Configure managed identities",
              "url": "https://learn.microsoft.com/en-us/training/modules/implement-managed-identities/4-configure-managed-identities",
              "href": "4-configure-managed-identities",
              "content": "Read in English\nAdd\nAdd to plan\nConfigure managed identities\nCompleted\n3 minutes\nYou can configure an Azure virtual machine with a managed identity during, or after, the creation of the virtual machine. CLI examples showing the commands for both system- and user-assigned identities are used in this unit.\nSystem-assigned managed identity\nTo create, or enable, an Azure virtual machine with the system-assigned managed identity your account needs the\nVirtual Machine Contributor\nrole assignment. No other Microsoft Entra directory role assignments are required.\nEnable system-assigned managed identity during creation of an Azure virtual machine\nThe following example creates a virtual machine named\nmyVM\nwith a system-assigned managed identity, as requested by the\n--assign-identity\nparameter, with the specified\n--role\nand\n--scope\n. The\n--admin-username\nand\n--admin-password\nparameters specify the administrative user name and password account for virtual machine sign-in. Update these values as appropriate for your environment:\naz vm create --resource-group myResourceGroup \\ \n    --name myVM --image win2016datacenter \\ \n    --generate-ssh-keys \\ \n    --assign-identity \\ \n    --role contributor \\\n    --scope mySubscription \\\n    --admin-username azureuser \\ \n    --admin-password myPassword12\nEnable system-assigned managed identity on an existing Azure virtual machine\nUse the\naz vm identity assign\ncommand to assign the system-assigned identity to an existing virtual machine:\naz vm identity assign -g myResourceGroup -n myVm\nUser-assigned managed identity\nTo assign a user-assigned identity to a virtual machine during its creation, your account needs the\nVirtual Machine Contributor\nand\nManaged Identity Operator\nrole assignments. No other Microsoft Entra directory role assignments are required.\nEnabling user-assigned managed identities is a two-step process:\nCreate the user-assigned identity\nAssign the identity to a virtual machine\nCreate a user-assigned identity\nCreate a user-assigned managed identity using\naz identity create\n.  The\n-g\nparameter specifies the resource group where the user-assigned managed identity is created, and the\n-n\nparameter specifies its name.\naz identity create -g myResourceGroup -n myUserAssignedIdentity\nAssign a user-assigned managed identity during the creation of an Azure virtual machine\nThe following example creates a virtual machine associated with the new user-assigned identity, as specified by the\n--assign-identity\nparameter, with the given\n--role\nand\n--scope\n.\naz vm create \\\n--resource-group <RESOURCE GROUP> \\\n--name <VM NAME> \\\n--image Ubuntu2204 \\\n--admin-username <USER NAME> \\\n--admin-password <PASSWORD> \\\n--assign-identity <USER ASSIGNED IDENTITY NAME> \\\n--role <ROLE> \\\n--scope <SUBSCRIPTION>\nAssign a user-assigned managed identity to an existing Azure virtual machine\nAssign the user-assigned identity to your virtual machine using\naz vm identity assign\n.\naz vm identity assign \\\n    -g <RESOURCE GROUP> \\\n    -n <VM NAME> \\\n    --identities <USER ASSIGNED IDENTITY>\nAzure SDKs with managed identities for Azure resources support\nAzure supports multiple programming platforms through a series of\nAzure SDKs\n. Several of them have been updated to support managed identities for Azure resources, and provide corresponding samples to demonstrate usage.\nSDK\nSample\n.NET\nManage resource from a virtual machine enabled with managed identities for Azure resources enabled\nJava\nManage storage from a virtual machine enabled with managed identities for Azure resources\nNode.js\nCreate a virtual machine with system-assigned managed identity enabled\nPython\nCreate a virtual machine with system-assigned managed identity enabled\nRuby\nCreate Azure virtual machine with an system-assigned identity enabled\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Configure managed identities"
                },
                {
                  "level": 2,
                  "text": "System-assigned managed identity"
                },
                {
                  "level": 3,
                  "text": "Enable system-assigned managed identity during creation of an Azure virtual machine"
                },
                {
                  "level": 3,
                  "text": "Enable system-assigned managed identity on an existing Azure virtual machine"
                },
                {
                  "level": 2,
                  "text": "User-assigned managed identity"
                },
                {
                  "level": 3,
                  "text": "Create a user-assigned identity"
                },
                {
                  "level": 3,
                  "text": "Assign a user-assigned managed identity during the creation of an Azure virtual machine"
                },
                {
                  "level": 3,
                  "text": "Assign a user-assigned managed identity to an existing Azure virtual machine"
                },
                {
                  "level": 2,
                  "text": "Azure SDKs with managed identities for Azure resources support"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [
                "--assign-identity",
                "--admin-username",
                "--admin-password",
                "az vm create --resource-group myResourceGroup \\ \n    --name myVM --image win2016datacenter \\ \n    --generate-ssh-keys \\ \n    --assign-identity \\ \n    --role contributor \\\n    --scope mySubscription \\\n    --admin-username azureuser \\ \n    --admin-password myPassword12",
                "az vm create --resource-group myResourceGroup \\ \n    --name myVM --image win2016datacenter \\ \n    --generate-ssh-keys \\ \n    --assign-identity \\ \n    --role contributor \\\n    --scope mySubscription \\\n    --admin-username azureuser \\ \n    --admin-password myPassword12",
                "az vm identity assign",
                "az vm identity assign -g myResourceGroup -n myVm",
                "az vm identity assign -g myResourceGroup -n myVm",
                "az identity create",
                "az identity create -g myResourceGroup -n myUserAssignedIdentity",
                "az identity create -g myResourceGroup -n myUserAssignedIdentity",
                "--assign-identity",
                "az vm create \\\n--resource-group <RESOURCE GROUP> \\\n--name <VM NAME> \\\n--image Ubuntu2204 \\\n--admin-username <USER NAME> \\\n--admin-password <PASSWORD> \\\n--assign-identity <USER ASSIGNED IDENTITY NAME> \\\n--role <ROLE> \\\n--scope <SUBSCRIPTION>",
                "az vm create \\\n--resource-group <RESOURCE GROUP> \\\n--name <VM NAME> \\\n--image Ubuntu2204 \\\n--admin-username <USER NAME> \\\n--admin-password <PASSWORD> \\\n--assign-identity <USER ASSIGNED IDENTITY NAME> \\\n--role <ROLE> \\\n--scope <SUBSCRIPTION>",
                "az vm identity assign",
                "az vm identity assign \\\n    -g <RESOURCE GROUP> \\\n    -n <VM NAME> \\\n    --identities <USER ASSIGNED IDENTITY>",
                "az vm identity assign \\\n    -g <RESOURCE GROUP> \\\n    -n <VM NAME> \\\n    --identities <USER ASSIGNED IDENTITY>"
              ],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                },
                {
                  "url": "https://azure.microsoft.com/downloads",
                  "text": "Azure SDKs"
                },
                {
                  "url": "https://github.com/Azure-Samples/aad-dotnet-manage-resources-from-vm-with-msi",
                  "text": "Manage resource from a virtual machine enabled with managed identities for Azure resources enabled"
                },
                {
                  "url": "https://github.com/Azure-Samples/compute-java-manage-resources-from-vm-with-msi-in-aad-group",
                  "text": "Manage storage from a virtual machine enabled with managed identities for Azure resources"
                },
                {
                  "url": "https://github.com/Azure-Samples/compute-node-msi-vm",
                  "text": "Create a virtual machine with system-assigned managed identity enabled"
                },
                {
                  "url": "/en-us/azure/developer/python/sdk/examples/azure-sdk-example-virtual-machines",
                  "text": "Create a virtual machine with system-assigned managed identity enabled"
                },
                {
                  "url": "https://github.com/Azure-Samples/compute-ruby-msi-vm/",
                  "text": "Create Azure virtual machine with an system-assigned identity enabled"
                }
              ]
            },
            {
              "number": 5,
              "title": "Acquire an access token",
              "url": "https://learn.microsoft.com/en-us/training/modules/implement-managed-identities/5-acquire-access-token",
              "href": "5-acquire-access-token",
              "content": "Read in English\nAdd\nAdd to plan\nAcquire an access token\nCompleted\n5 minutes\nA client application can request managed identities for Azure resources app-only access token for accessing a given resource. The token is based on the managed identities for Azure resources service principal. The recommended method is to use the\nDefaultAzureCredential\n.\nThe Azure Identity library supports a\nDefaultAzureCredential\ntype.\nDefaultAzureCredential\nautomatically attempts to authenticate via multiple mechanisms, including environment variables or an interactive sign-in. The credential type can be used in your development environment using your own credentials. It can also be used in your production Azure environment using a managed identity. No code changes are required when you deploy your application.\nNote\nDefaultAzureCredential\nis intended to simplify getting started with the SDK by handling common scenarios with reasonable default behaviors. Developers who want more control or whose scenario isn't served by the default settings should use other credential types.\nThe\nDefaultAzureCredential\nattempts to authenticate via the following mechanisms, in this order, stopping when one succeeds:\nEnvironment\n- The\nDefaultAzureCredential\nreads account information specified via environment variables and use it to authenticate.\nManaged Identity\n- If the application is deployed to an Azure host with Managed Identity enabled, the\nDefaultAzureCredential\nauthenticates with that account.\nVisual Studio\n- If the developer authenticated via Visual Studio, the\nDefaultAzureCredential\nauthenticates with that account.\nAzure CLI\n- If the developer authenticated an account via the Azure CLI\naz login\ncommand, the\nDefaultAzureCredential\nauthenticates with that account. Visual Studio Code users can authenticate their development environment using the Azure CLI.\nAzure PowerShell\n- If the developer authenticated an account via the Azure PowerShell\nConnect-AzAccount\ncommand, the\nDefaultAzureCredential\nauthenticates with that account.\nInteractive browser\n- If enabled, the\nDefaultAzureCredential\ninteractively authenticates the developer via the current system's default browser. By default, this credential type is disabled.\nExamples\nThe following examples use the Azure Identity SDK that can be added to a project with this command:\ndotnet add package Azure.Identity\nAuthenticate with\nDefaultAzureCredential\nThis example demonstrates authenticating the\nSecretClient\nfrom the\nAzure.Security.KeyVault.Secrets\nclient library using the\nDefaultAzureCredential\n.\n// Create a secret client using the DefaultAzureCredential\nvar client = new SecretClient(new Uri(\"https://myvault.vault.azure.net/\"), new DefaultAzureCredential());\nSpecify a user-assigned managed identity with\nDefaultAzureCredential\nThis example demonstrates configuring the\nDefaultAzureCredential\nto authenticate a user-assigned identity when deployed to an Azure host. It then authenticates a\nBlobClient\nfrom the\nAzure.Storage.Blobs\nclient library with credential.\n// When deployed to an azure host, the default azure credential will authenticate the specified user assigned managed identity.\n\nstring userAssignedClientId = \"<your managed identity client Id>\";\nvar credential = new DefaultAzureCredential(new DefaultAzureCredentialOptions { ManagedIdentityClientId = userAssignedClientId });\n\nvar blobClient = new BlobClient(new Uri(\"https://myaccount.blob.core.windows.net/mycontainer/myblob\"), credential);\nDefine a custom authentication flow with\nChainedTokenCredential\nWhile the\nDefaultAzureCredential\nis generally the quickest way to get started developing applications for Azure, more advanced users may want to customize the credentials considered when authenticating. The\nChainedTokenCredential\nenables users to combine multiple credential instances to define a customized chain of credentials. This example demonstrates creating a\nChainedTokenCredential\nwhich attempts to authenticate using managed identity, and fall back to authenticating via the Azure CLI if managed identity is unavailable in the current environment. The credential is then used to authenticate an\nEventHubProducerClient\nfrom the\nAzure.Messaging.EventHubs\nclient library.\n// Authenticate using managed identity if it is available; otherwise use the Azure CLI to authenticate.\n\nvar credential = new ChainedTokenCredential(new ManagedIdentityCredential(), new AzureCliCredential());\n\nvar eventHubProducerClient = new EventHubProducerClient(\"myeventhub.eventhubs.windows.net\", \"myhubpath\", credential);\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Acquire an access token"
                },
                {
                  "level": 2,
                  "text": "Examples"
                },
                {
                  "level": 3,
                  "text": "Authenticate withDefaultAzureCredential"
                },
                {
                  "level": 3,
                  "text": "Specify a user-assigned managed identity withDefaultAzureCredential"
                },
                {
                  "level": 3,
                  "text": "Define a custom authentication flow withChainedTokenCredential"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [
                "DefaultAzureCredential",
                "DefaultAzureCredential",
                "DefaultAzureCredential",
                "DefaultAzureCredential",
                "DefaultAzureCredential",
                "DefaultAzureCredential",
                "DefaultAzureCredential",
                "DefaultAzureCredential",
                "DefaultAzureCredential",
                "Connect-AzAccount",
                "DefaultAzureCredential",
                "DefaultAzureCredential",
                "dotnet add package Azure.Identity",
                "dotnet add package Azure.Identity",
                "DefaultAzureCredential",
                "SecretClient",
                "DefaultAzureCredential",
                "// Create a secret client using the DefaultAzureCredential\nvar client = new SecretClient(new Uri(\"https://myvault.vault.azure.net/\"), new DefaultAzureCredential());",
                "// Create a secret client using the DefaultAzureCredential\nvar client = new SecretClient(new Uri(\"https://myvault.vault.azure.net/\"), new DefaultAzureCredential());",
                "DefaultAzureCredential",
                "DefaultAzureCredential",
                "// When deployed to an azure host, the default azure credential will authenticate the specified user assigned managed identity.\n\nstring userAssignedClientId = \"<your managed identity client Id>\";\nvar credential = new DefaultAzureCredential(new DefaultAzureCredentialOptions { ManagedIdentityClientId = userAssignedClientId });\n\nvar blobClient = new BlobClient(new Uri(\"https://myaccount.blob.core.windows.net/mycontainer/myblob\"), credential);",
                "// When deployed to an azure host, the default azure credential will authenticate the specified user assigned managed identity.\n\nstring userAssignedClientId = \"<your managed identity client Id>\";\nvar credential = new DefaultAzureCredential(new DefaultAzureCredentialOptions { ManagedIdentityClientId = userAssignedClientId });\n\nvar blobClient = new BlobClient(new Uri(\"https://myaccount.blob.core.windows.net/mycontainer/myblob\"), credential);",
                "ChainedTokenCredential",
                "DefaultAzureCredential",
                "ChainedTokenCredential",
                "ChainedTokenCredential",
                "EventHubProducerClient",
                "// Authenticate using managed identity if it is available; otherwise use the Azure CLI to authenticate.\n\nvar credential = new ChainedTokenCredential(new ManagedIdentityCredential(), new AzureCliCredential());\n\nvar eventHubProducerClient = new EventHubProducerClient(\"myeventhub.eventhubs.windows.net\", \"myhubpath\", credential);",
                "// Authenticate using managed identity if it is available; otherwise use the Azure CLI to authenticate.\n\nvar credential = new ChainedTokenCredential(new ManagedIdentityCredential(), new AzureCliCredential());\n\nvar eventHubProducerClient = new EventHubProducerClient(\"myeventhub.eventhubs.windows.net\", \"myhubpath\", credential);"
              ],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                },
                {
                  "url": "https://github.com/Azure/azure-sdk-for-net/tree/Azure.Identity_1.8.2/sdk/keyvault/Azure.Security.KeyVault.Secrets",
                  "text": "Azure.Security.KeyVault.Secrets"
                },
                {
                  "url": "https://github.com/Azure/azure-sdk-for-net/tree/Azure.Identity_1.8.2/sdk/storage/Azure.Storage.Blobs",
                  "text": "Azure.Storage.Blobs"
                },
                {
                  "url": "https://github.com/Azure/azure-sdk-for-net/tree/Azure.Identity_1.8.2/sdk/eventhub/Azure.Messaging.EventHubs",
                  "text": "Azure.Messaging.EventHubs"
                }
              ]
            },
            {
              "number": 6,
              "title": "Module assessment",
              "url": "https://learn.microsoft.com/en-us/training/modules/implement-managed-identities/6-knowledge-check",
              "href": "6-knowledge-check",
              "content": "Read in English\nAdd\nAdd to plan\nModule assessment\nCompleted\n3 minutes\nCheck your knowledge\n1.\nWhich of the following managed identity characteristics is indicative of user-assigned identities?\nShared lifecycle with an Azure resource\nIndependent life-cycle\nCan only be associated with a single Azure resource\n2.\nA client app requests managed identities for an access token for a given resource. Which of the following options is the basis for the token?\nOauth 2.0\nService principal\nVirtual machine\nYou must answer all questions before checking your work.\nYou must answer all questions before checking your work.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Module assessment"
                },
                {
                  "level": 2,
                  "text": "Check your knowledge"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 7,
              "title": "Summary",
              "url": "https://learn.microsoft.com/en-us/training/modules/implement-managed-identities/7-summary",
              "href": "7-summary",
              "content": "Read in English\nAdd\nAdd to plan\nSummary\nCompleted\n3 minutes\nIn this module, you learned how to:\nExplain the differences between the two types of managed identities\nDescribe the flows for user- and system-assigned managed identities\nConfigure managed identities\nAcquire access tokens by using REST and code\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Summary"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            }
          ]
        },
        {
          "title": "Implement Azure App Configuration",
          "url": "https://learn.microsoft.com/en-us/training/modules/implement-azure-app-configuration/",
          "description": "",
          "learning_objectives": [],
          "prerequisites": [],
          "units": [
            {
              "number": 1,
              "title": "Introduction",
              "url": "https://learn.microsoft.com/en-us/training/modules/implement-azure-app-configuration/1-introduction",
              "href": "1-introduction",
              "content": "Read in English\nAdd\nAdd to plan\nIntroduction\nCompleted\n3 minutes\nAzure App Configuration provides a service to centrally manage application settings and feature flags.\nAfter completing this module, you'll be able to:\nExplain the benefits of using Azure App Configuration\nDescribe how Azure App Configuration stores information\nImplement feature management\nSecurely access your app configuration information\nRetrieve configuration settings from Azure App Configuration\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Introduction"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 2,
              "title": "Explore the Azure App Configuration service",
              "url": "https://learn.microsoft.com/en-us/training/modules/implement-azure-app-configuration/2-app-configuration-overview",
              "href": "2-app-configuration-overview",
              "content": "Read in English\nAdd\nAdd to plan\nExplore the Azure App Configuration service\nCompleted\n3 minutes\nAzure App Configuration provides a service to centrally manage application settings and feature flags. Modern programs, especially programs running in a cloud, generally have many components that are distributed in nature. Spreading configuration settings across these components can lead to hard-to-troubleshoot errors during an application deployment. Use App Configuration to store all the settings for your application and secure their accesses in one place.\nApp Configuration offers the following benefits:\nA fully managed service that can be set up in minutes\nFlexible key representations and mappings\nTagging with labels\nPoint-in-time replay of settings\nDedicated UI for feature flag management\nComparison of two sets of configurations on custom-defined dimensions\nEnhanced security through Azure-managed identities\nEncryption of sensitive information at rest and in transit\nNative integration with popular frameworks\nApp Configuration complements Azure Key Vault, which is used to store application secrets. App Configuration makes it easier to implement the following scenarios:\nCentralize management and distribution of hierarchical configuration data for different environments and geographies\nDynamically change application settings without the need to redeploy or restart an application\nControl feature availability in real-time\nUse App Configuration\nThe easiest way to add an App Configuration store to your application is through a client library that Microsoft provides. Based on the programming language and framework, the following best methods are available to you.\nProgramming language and framework\nHow to connect\n.NET\nApp Configuration\nprovider\nfor .NET\nASP.NET Core\nApp Configuration\nprovider\nfor .NET\n.NET Framework and ASP.NET\nApp Configuration\nbuilder\nfor .NET\nJava Spring\nApp Configuration\nprovider\nfor Spring Cloud\nJavaScript/Node.js\nApp Configuration\nprovider\nfor JavaScript\nPython\nApp Configuration\nprovider\nfor Python\nOthers\nApp Configuration\nREST API\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Explore the Azure App Configuration service"
                },
                {
                  "level": 2,
                  "text": "Use App Configuration"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                },
                {
                  "url": "/en-us/dotnet/api/Microsoft.Extensions.Configuration.AzureAppConfiguration",
                  "text": "provider"
                },
                {
                  "url": "/en-us/dotnet/api/Microsoft.Extensions.Configuration.AzureAppConfiguration",
                  "text": "provider"
                },
                {
                  "url": "https://github.com/aspnet/MicrosoftConfigurationBuilders/blob/main/README.md#azureappconfigurationbuilder",
                  "text": "builder"
                },
                {
                  "url": "https://microsoft.github.io/spring-cloud-azure/docs/azure-app-configuration/2.9.0/reference/html/index.html",
                  "text": "provider"
                },
                {
                  "url": "https://github.com/Azure/azure-sdk-for-js/tree/main/sdk/appconfiguration/app-configuration",
                  "text": "provider"
                },
                {
                  "url": "https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/appconfiguration/azure-appconfiguration",
                  "text": "provider"
                },
                {
                  "url": "/en-us/rest/api/appconfiguration/",
                  "text": "REST API"
                }
              ]
            },
            {
              "number": 3,
              "title": "Create paired keys and values",
              "url": "https://learn.microsoft.com/en-us/training/modules/implement-azure-app-configuration/3-keys-values",
              "href": "3-keys-values",
              "content": "Read in English\nAdd\nAdd to plan\nCreate paired keys and values\nCompleted\n3 minutes\nAzure App Configuration stores configuration data as key-value pairs.\nKeys\nKeys serve as the name for key-value pairs and are used to store and retrieve corresponding values. It's a common practice to organize keys into a hierarchical namespace by using a character delimiter, such as\n/\nor\n:\n. Use a convention that's best suited for your application. App Configuration treats keys as a whole. It doesn't parse keys to figure out how their names are structured or enforce any rule on them.\nHere's an example of key names structured into a hierarchy based on component services:\nAppName:Service1:ApiEndpoint\nAppName:Service2:ApiEndpoint\nThe use of configuration data within application frameworks might dictate specific naming schemes for key-values. For example, Java's Spring Cloud framework defines\nEnvironment\nresources that supply settings to a Spring application. These resources are parameterized by variables that include\napplication name\nand\nprofile\n. Keys for Spring Cloud-related configuration data typically start with these two elements separated by a delimiter.\nKeys stored in App Configuration are case-sensitive, unicode-based strings. The keys\napp1\nand\nApp1\nare distinct in an App Configuration store. Keep this in mind when you use configuration settings within an application because some frameworks handle configuration keys case-insensitively.\nYou can use any unicode character in key names entered into App Configuration except for\n*\n,\n,\n, and\n\\\n. These characters are reserved. If you need to include a reserved character, you must escape it by using\n\\{Reserved Character}\n. There's a combined size limit of 10,000 characters on a key-value pair. This limit includes all characters in the key, its value, and all associated optional attributes. Within this limit, you can have many hierarchical levels for keys.\nDesign key namespaces\nThere are two general approaches to naming keys used for configuration data: flat or hierarchical. These methods are similar from an application usage standpoint, but hierarchical naming offers many advantages:\nEasier to read. Instead of one long sequence of characters, delimiters in a hierarchical key name function as spaces in a sentence.\nEasier to manage. A key name hierarchy represents logical groups of configuration data.\nEasier to use. It's simpler to write a query that pattern-matches keys in a hierarchical structure and retrieves only a portion of configuration data.\nLabel keys\nKey-values in App Configuration can optionally have a label attribute. Labels are used to differentiate key-values with the same key. A key\napp1\nwith labels\nA\nand\nB\nforms two separate keys in an App Configuration store. By default, a key-value has no label. To explicitly reference a key-value without a label, use\n\\0\n(URL encoded as\n%00\n).\nLabel provides a convenient way to create variants of a key. A common use of labels is to specify multiple environments for the same key:\nKey = AppName:DbEndpoint & Label = Test\nKey = AppName:DbEndpoint & Label = Staging\nKey = AppName:DbEndpoint & Label = Production\nVersion key values\nApp Configuration doesn't version key values automatically as they're modified. Use labels as a way to create multiple versions of a key value. For example, you can input an application version number or a Git commit ID in labels to identify key values associated with a particular software build.\nQuery key values\nEach key-value is uniquely identified by its key plus a label that can be\n\\0\n. You query an App Configuration store for key-values by specifying a pattern. The App Configuration store returns all key-values that match the pattern including their corresponding values and attributes.\nValues\nValues assigned to keys are also unicode strings. You can use all unicode characters for values. There's an optional user-defined content type associated with each value. Use this attribute to store information, for example, an encoding scheme, about a value that helps your application to process it properly.\nConfiguration data stored in an App Configuration store, which includes all keys and values, is encrypted at rest and in transit. App Configuration isn't a replacement solution for Azure Key Vault. Don't store application secrets in it.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Create paired keys and values"
                },
                {
                  "level": 2,
                  "text": "Keys"
                },
                {
                  "level": 3,
                  "text": "Design key namespaces"
                },
                {
                  "level": 3,
                  "text": "Label keys"
                },
                {
                  "level": 3,
                  "text": "Version key values"
                },
                {
                  "level": 3,
                  "text": "Query key values"
                },
                {
                  "level": 2,
                  "text": "Values"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [
                "AppName:Service1:ApiEndpoint\nAppName:Service2:ApiEndpoint",
                "AppName:Service1:ApiEndpoint\nAppName:Service2:ApiEndpoint",
                "Environment",
                "\\{Reserved Character}",
                "Key = AppName:DbEndpoint & Label = Test\nKey = AppName:DbEndpoint & Label = Staging\nKey = AppName:DbEndpoint & Label = Production",
                "Key = AppName:DbEndpoint & Label = Test\nKey = AppName:DbEndpoint & Label = Staging\nKey = AppName:DbEndpoint & Label = Production"
              ],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 4,
              "title": "Manage application features",
              "url": "https://learn.microsoft.com/en-us/training/modules/implement-azure-app-configuration/4-app-configuration-feature-management",
              "href": "4-app-configuration-feature-management",
              "content": "Read in English\nAdd\nAdd to plan\nManage application features\nCompleted\n3 minutes\nFeature management is a modern software-development practice that decouples feature release from code deployment and enables quick changes to feature availability on demand. It uses a technique called feature flags (also known as feature toggles, feature switches, and so on) to dynamically administer a feature's lifecycle.\nBasic concepts\nHere are several new terms related to feature management:\nFeature flag\n: A feature flag is a variable with a binary state of\non\nor\noff\n. The feature flag also has an associated code block. The state of the feature flag triggers whether the code block runs or not.\nFeature manager\n: A feature manager is an application package that handles the lifecycle of all the feature flags in an application. The feature manager typically provides extra functionality, such as caching feature flags and updating their states.\nFilter\n: A filter is a rule for evaluating the state of a feature flag. A user group, a device or browser type, a geographic location, and a time window are all examples of what a filter can represent.\nAn effective implementation of feature management consists of at least two components working in concert:\nAn application that makes use of feature flags.\nA separate repository that stores the feature flags and their current states.\nHow these components interact is illustrated in the following examples.\nFeature flag usage in code\nThe basic pattern for implementing feature flags in an application is simple. You can think of a feature flag as a Boolean state variable used with an\nif\nconditional statement in your code:\nif (featureFlag) {\n    // Run the following code\n}\nIn this case, if\nfeatureFlag\nis set to\nTrue\n, the enclosed code block is executed; otherwise, it's skipped. You can set the value of\nfeatureFlag\nstatically, as in the following code example:\nbool featureFlag = true;\nYou can also evaluate the flag's state based on certain rules:\nbool featureFlag = isBetaUser();\nYou can extend the conditional to set application behavior for either state:\nif (featureFlag) {\n    // This following code will run if the featureFlag value is true\n} else {\n    // This following code will run if the featureFlag value is false\n}\nFeature flag declaration\nEach feature flag has two parts: a name and a list of one or more filters that are used to evaluate if a feature's state is\non\n(that is, when its value is\nTrue\n). A filter defines a use case for when a feature should be turned on.\nWhen a feature flag has multiple filters, the filter list is traversed in order until one of the filters determines the feature should be enabled. At that point, the feature flag is\non\n, and any remaining filter results are skipped. If no filter indicates the feature should be enabled, the feature flag is\noff\n.\nThe feature manager supports\nappsettings.json\nas a configuration source for feature flags. The following example shows how to set up feature flags in a JSON file:\n\"FeatureManagement\": {\n    \"FeatureA\": true, // Feature flag set to on\n    \"FeatureB\": false, // Feature flag set to off\n    \"FeatureC\": {\n        \"EnabledFor\": [\n            {\n                \"Name\": \"Percentage\",\n                \"Parameters\": {\n                    \"Value\": 50\n                }\n            }\n        ]\n    }\n}\nFeature flag repository\nTo use feature flags effectively, you need to externalize all the feature flags used in an application. This approach allows you to change feature flag states without modifying and redeploying the application itself.\nAzure App Configuration is designed to be a centralized repository for feature flags. You can use it to define different kinds of feature flags and manipulate their states quickly and confidently. You can then use the App Configuration libraries for various programming language frameworks to easily access these feature flags from your application.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Manage application features"
                },
                {
                  "level": 2,
                  "text": "Basic concepts"
                },
                {
                  "level": 2,
                  "text": "Feature flag usage in code"
                },
                {
                  "level": 2,
                  "text": "Feature flag declaration"
                },
                {
                  "level": 2,
                  "text": "Feature flag repository"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [
                "if (featureFlag) {\n    // Run the following code\n}",
                "if (featureFlag) {\n    // Run the following code\n}",
                "featureFlag",
                "featureFlag",
                "bool featureFlag = true;",
                "bool featureFlag = true;",
                "bool featureFlag = isBetaUser();",
                "bool featureFlag = isBetaUser();",
                "if (featureFlag) {\n    // This following code will run if the featureFlag value is true\n} else {\n    // This following code will run if the featureFlag value is false\n}",
                "if (featureFlag) {\n    // This following code will run if the featureFlag value is true\n} else {\n    // This following code will run if the featureFlag value is false\n}",
                "\"FeatureManagement\": {\n    \"FeatureA\": true, // Feature flag set to on\n    \"FeatureB\": false, // Feature flag set to off\n    \"FeatureC\": {\n        \"EnabledFor\": [\n            {\n                \"Name\": \"Percentage\",\n                \"Parameters\": {\n                    \"Value\": 50\n                }\n            }\n        ]\n    }\n}",
                "\"FeatureManagement\": {\n    \"FeatureA\": true, // Feature flag set to on\n    \"FeatureB\": false, // Feature flag set to off\n    \"FeatureC\": {\n        \"EnabledFor\": [\n            {\n                \"Name\": \"Percentage\",\n                \"Parameters\": {\n                    \"Value\": 50\n                }\n            }\n        ]\n    }\n}"
              ],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 5,
              "title": "Secure app configuration data",
              "url": "https://learn.microsoft.com/en-us/training/modules/implement-azure-app-configuration/5-secure-app-configuration-data",
              "href": "5-secure-app-configuration-data",
              "content": "Read in English\nAdd\nAdd to plan\nSecure app configuration data\nCompleted\n3 minutes\nIn this unit you learn how to secure your apps configuration data by using:\nCustomer-managed keys\nPrivate endpoints\nManaged identities\nEncrypt configuration data by using customer-managed keys\nAzure App Configuration encrypts sensitive information at rest using a 256-bit AES encryption key provided by Microsoft. Every App Configuration instance has its own encryption key managed by the service and used to encrypt sensitive information. Sensitive information includes the values found in key-value pairs. When customer-managed key capability is enabled, App Configuration uses a managed identity assigned to the App Configuration instance to authenticate with Microsoft Entra ID. The managed identity then calls Azure Key Vault and wraps the App Configuration instance's encryption key. The wrapped encryption key is then stored and the unwrapped encryption key is cached within App Configuration for one hour. App Configuration refreshes the unwrapped version of the App Configuration instance's encryption key hourly. This ensures availability under normal operating conditions.\nEnable customer-managed key capability\nThe following components are required to successfully enable the customer-managed key capability for Azure App Configuration:\nStandard tier Azure App Configuration instance\nAzure Key Vault with soft-delete and purge-protection features enabled\nAn RSA or RSA-HSM key within the Key Vault: The key must not be expired, it must be enabled, and it must have both wrap and unwrap capabilities enabled\nOnce these resources are configured, two steps remain to allow Azure App Configuration to use the Key Vault key:\nAssign a managed identity to the Azure App Configuration instance\nGrant the identity\nGET\n,\nWRAP\n, and\nUNWRAP\npermissions in the target Key Vault's access policy.\nUse private endpoints for Azure App Configuration\nYou can use private endpoints for Azure App Configuration to allow clients on a virtual network to securely access data over a private link. The private endpoint uses an IP address from the virtual network address space for your App Configuration store. Network traffic between the clients on the virtual network and the App Configuration store traverses over the virtual network using a private link on the Microsoft backbone network, eliminating exposure to the public internet.\nUsing private endpoints for your App Configuration store enables you to:\nSecure your application configuration details by configuring the firewall to block all connections to App Configuration on the public endpoint.\nIncrease security for the virtual network ensuring data doesn't escape.\nSecurely connect to the App Configuration store from on-premises networks that connect to the virtual network using VPN or ExpressRoutes with private-peering.\nManaged identities\nA managed identity from Microsoft Entra ID allows Azure App Configuration to easily access other Microsoft Entra ID-protected resources, such as Azure Key Vault. The identity is managed by the Azure platform. It doesn't require you to provision or rotate any secrets.\nYour application can be granted two types of identities:\nA\nsystem-assigned identity\nis tied to your configuration store. It's deleted if your configuration store is deleted. A configuration store can only have one system-assigned identity.\nA\nuser-assigned identity\nis a standalone Azure resource that can be assigned to your configuration store. A configuration store can have multiple user-assigned identities.\nAdd a system-assigned identity\nTo set up a managed identity using the Azure CLI, use the\naz appconfig identity assign\ncommand against an existing configuration store. The following Azure CLI example creates a system-assigned identity for an Azure App Configuration store named\nmyTestAppConfigStore\n.\naz appconfig identity assign \\ \n    --name myTestAppConfigStore \\ \n    --resource-group myResourceGroup\nAdd a user-assigned identity\nCreating an App Configuration store with a user-assigned identity requires that you create the identity and then assign its resource identifier to your store. The following Azure CLI examples create  a user-assigned identity called\nmyUserAssignedIdentity\nand assign it to an Azure App Configuration store named\nmyTestAppConfigStore\n.\nCreate an identity using the\naz identity create\ncommand:\naz identity create --resource-group myResourceGroup --name myUserAssignedIdentity\nAssign the new user-assigned identity to the\nmyTestAppConfigStore\nconfiguration store:\naz appconfig identity assign --name myTestAppConfigStore \\ \n    --resource-group myResourceGroup \\ \n    --identities /subscriptions/[subscription id]/resourcegroups/myResourceGroup/providers/Microsoft.ManagedIdentity/userAssignedIdentities/myUserAssignedIdentity\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Secure app configuration data"
                },
                {
                  "level": 2,
                  "text": "Encrypt configuration data by using customer-managed keys"
                },
                {
                  "level": 3,
                  "text": "Enable customer-managed key capability"
                },
                {
                  "level": 2,
                  "text": "Use private endpoints for Azure App Configuration"
                },
                {
                  "level": 2,
                  "text": "Managed identities"
                },
                {
                  "level": 3,
                  "text": "Add a system-assigned identity"
                },
                {
                  "level": 3,
                  "text": "Add a user-assigned identity"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [
                "az appconfig identity assign",
                "myTestAppConfigStore",
                "az appconfig identity assign \\ \n    --name myTestAppConfigStore \\ \n    --resource-group myResourceGroup",
                "az appconfig identity assign \\ \n    --name myTestAppConfigStore \\ \n    --resource-group myResourceGroup",
                "myUserAssignedIdentity",
                "myTestAppConfigStore",
                "az identity create",
                "az identity create --resource-group myResourceGroup --name myUserAssignedIdentity",
                "az identity create --resource-group myResourceGroup --name myUserAssignedIdentity",
                "myTestAppConfigStore",
                "az appconfig identity assign --name myTestAppConfigStore \\ \n    --resource-group myResourceGroup \\ \n    --identities /subscriptions/[subscription id]/resourcegroups/myResourceGroup/providers/Microsoft.ManagedIdentity/userAssignedIdentities/myUserAssignedIdentity",
                "az appconfig identity assign --name myTestAppConfigStore \\ \n    --resource-group myResourceGroup \\ \n    --identities /subscriptions/[subscription id]/resourcegroups/myResourceGroup/providers/Microsoft.ManagedIdentity/userAssignedIdentities/myUserAssignedIdentity"
              ],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 6,
              "title": "Module assessment",
              "url": "https://learn.microsoft.com/en-us/training/modules/implement-azure-app-configuration/6-knowledge-check",
              "href": "6-knowledge-check",
              "content": "Read in English\nAdd\nAdd to plan\nModule assessment\nCompleted\n3 minutes\nCheck your knowledge\n1.\nWhat is the purpose of labels in Azure App Configuration?\nLabels are used to differentiate key-values with the same key in App Configuration.\nLabels are used to encrypt key-values in App Configuration.\nLabels are used to limit the size of key-values in App Configuration.\n2.\nWhat is the role of a feature manager in managing application features?\nA feature manager is a rule for evaluating the state of a feature flag.\nA feature manager is a variable with a binary state of on or off.\nA feature manager is an application package that handles the lifecycle of all the feature flags in an application.\n3.\nWhat is the purpose of using customer-managed keys in Azure App Configuration?\nTo enable authentication with Microsoft Entra ID\nTo permanently store the unwrapped encryption key\nTo encrypt sensitive information at rest\nYou must answer all questions before checking your work.\nYou must answer all questions before checking your work.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Module assessment"
                },
                {
                  "level": 2,
                  "text": "Check your knowledge"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 7,
              "title": "Summary",
              "url": "https://learn.microsoft.com/en-us/training/modules/implement-azure-app-configuration/7-summary",
              "href": "7-summary",
              "content": "Read in English\nAdd\nAdd to plan\nSummary\nCompleted\n3 minutes\nIn this module, you learned how to:\nExplain the benefits of using Azure App Configuration\nDescribe how Azure App Configuration stores information\nImplement feature management\nSecurely access your app configuration information\nRetrieve configuration settings from Azure App Configuration\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Summary"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "title": "Implement API Management",
      "url": "https://learn.microsoft.com/en-us/training/paths/az-204-implement-api-management/",
      "learn_uid": "learn.wwl.az-204-implement-api-management",
      "modules": [
        {
          "title": "Explore API Management",
          "url": "https://learn.microsoft.com/en-us/training/modules/explore-api-management/",
          "description": "",
          "learning_objectives": [],
          "prerequisites": [],
          "units": [
            {
              "number": 1,
              "title": "Introduction",
              "url": "https://learn.microsoft.com/en-us/training/modules/explore-api-management/1-introduction",
              "href": "1-introduction",
              "content": "Read in English\nAdd\nAdd to plan\nIntroduction\nCompleted\n3 minutes\nAPI Management helps organizations publish APIs to external, partner, and internal developers to unlock the potential of their data and services.\nAfter completing this module, you'll be able to:\nDescribe the components, and their function, of the API Management service.\nExplain how API gateways can help manage calls to your APIs.\nSecure access to APIs by using subscriptions and certificates.\nImport and configure an API.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Introduction"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 2,
              "title": "Discover the API Management service",
              "url": "https://learn.microsoft.com/en-us/training/modules/explore-api-management/2-api-management-overview",
              "href": "2-api-management-overview",
              "content": "Read in English\nAdd\nAdd to plan\nDiscover the API Management service\nCompleted\n3 minutes\nAPI Management provides the core functionality to ensure a successful API program through developer engagement, business insights, analytics, security, and protection. Each API consists of one or more operations, and each API can be added to one or more products. To use an API, developers subscribe to a product that contains that API, and then they can call the API's operation, subject to any usage policies.\nAPI Management components\nAzure API Management is made up of an\nAPI gateway\n, a\nmanagement plane\n, and a\ndeveloper portal\n. These components are Azure-hosted and fully managed by default. API Management is available in various\ntiers\ndiffering in capacity and features.\nThe\nAPI gateway\nis the endpoint that:\nAccepts API calls and routes them to appropriate backends\nVerifies API keys and other credentials presented with requests\nEnforces usage quotas and rate limits\nTransforms requests and responses specified in policy statements\nCaches responses to improve response latency and minimize the load on backend services\nEmits logs, metrics, and traces for monitoring, reporting, and troubleshooting\nThe\nmanagement plane\nis the administrative interface where you set up your API program. Use it to:\nProvision and configure API Management service settings\nDefine or import API schema\nPackage APIs into products\nSet up policies like quotas or transformations on the APIs\nGet insights from analytics\nManage users\nThe\nDeveloper portal\nis an automatically generated, fully customizable website with the documentation of your APIs. Using the developer portal, developers can:\nRead API documentation\nCall an API via the interactive console\nCreate an account and subscribe to get API keys\nAccess analytics on their own usage\nDownload API definitions\nManage API keys\nProducts\nProducts are how APIs are surfaced to developers. Products in API Management have one or more APIs, and are configured with a title, description, and terms of use. Products can be\nOpen\nor\nProtected\n. Protected products must be subscribed to before they can be used, while open products can be used without a subscription. Subscription approval is configured at the product level and can either require administrator approval, or be autoapproved.\nGroups\nGroups are used to manage the visibility of products to developers. API Management has the following immutable system groups:\nAdministrators\n- Manage API Management service instances and create the APIs, operations, and products that are used by developers. Azure subscription administrators are members of this group.\nDevelopers\n- Authenticated developer portal users that build applications using your APIs. Developers are granted access to the developer portal and build applications that call the operations of an API.\nGuests\n- Unauthenticated developer portal users. They can be granted certain read-only access, like the ability to view APIs but not call them.\nIn addition to these system groups, administrators can create custom groups or use external groups in associated Microsoft Entra tenants.\nDevelopers\nDevelopers represent the user accounts in an API Management service instance. Developers can be created or invited to join by administrators, or they can sign up from the Developer portal. Each developer is a member of one or more groups, and can subscribe to the products that grant visibility to those groups.\nPolicies\nPolicies are a collection of statements that are executed sequentially on the request or response of an API. Popular statements include format conversion from XML to JSON and call rate limiting to restrict the number of incoming calls from a developer, and many other policies are available.\nPolicy expressions can be used as attribute values or text values in any of the API Management policies, unless the policy specifies otherwise. Some policies such as the Control flow and Set variable policies are based on policy expressions.\nPolicies can be applied at different scopes, depending on your needs: global (all APIs), a product, a specific API, or an API operation.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Discover the API Management service"
                },
                {
                  "level": 2,
                  "text": "API Management components"
                },
                {
                  "level": 2,
                  "text": "Products"
                },
                {
                  "level": 2,
                  "text": "Groups"
                },
                {
                  "level": 2,
                  "text": "Developers"
                },
                {
                  "level": 2,
                  "text": "Policies"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                },
                {
                  "url": "/en-us/azure/api-management/api-management-features",
                  "text": "tiers"
                }
              ]
            },
            {
              "number": 3,
              "title": "Explore API gateways",
              "url": "https://learn.microsoft.com/en-us/training/modules/explore-api-management/3-api-gateways",
              "href": "3-api-gateways",
              "content": "Read in English\nAdd\nAdd to plan\nExplore API gateways\nCompleted\n3 minutes\nYour solution might contain several front- and back-end services. In this scenario, how does a client know what endpoints to call? What happens when new services are introduced, or existing services are refactored? How do services handle SSL termination, authentication, and other concerns?\nThe API Management gateway (also called data plane or runtime) is the service component that's responsible for proxying API requests, applying policies, and collecting telemetry.\nAn API gateway sits between clients and services. It acts as a reverse proxy, routing requests from clients to services. It might also perform various cross-cutting tasks such as authentication, SSL termination, and rate limiting. If you don't deploy a gateway, clients must send requests directly to back-end services. However, there are some potential problems with exposing services directly to clients:\nIt can result in complex client code. The client must keep track of multiple endpoints, and handle failures in a resilient way.\nIt creates coupling between the client and the backend. The client needs to know how the individual services are decomposed. That makes it harder to maintain the client and also harder to refactor services.\nA single operation might require calls to multiple services.\nEach public-facing service must handle concerns such as authentication, SSL, and client rate limiting.\nServices must expose a client-friendly protocol such as HTTP or WebSocket. This limits the choice of communication protocols.\nServices with public endpoints are a potential attack surface, and must be hardened.\nA gateway helps to address these issues by decoupling clients from services.\nManaged and self-hosted\nAPI Management offers both managed and self-hosted gateways:\nManaged\n- The managed gateway is the default gateway component that is deployed in Azure for every API Management instance in every service tier. With the managed gateway, all API traffic flows through Azure regardless of where backends implementing the APIs are hosted.\nSelf-hosted\n- The self-hosted gateway is an optional, containerized version of the default managed gateway. It's useful for hybrid and multicloud scenarios where there's a requirement to run the gateways off of Azure in the same environments where API backends are hosted. The self-hosted gateway enables customers with hybrid IT infrastructure to manage APIs hosted on-premises and across clouds from a single API Management service in Azure.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Explore API gateways"
                },
                {
                  "level": 2,
                  "text": "Managed and self-hosted"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 4,
              "title": "Explore API Management policies",
              "url": "https://learn.microsoft.com/en-us/training/modules/explore-api-management/4-api-management-policies",
              "href": "4-api-management-policies",
              "content": "Read in English\nAdd\nAdd to plan\nExplore API Management policies\nCompleted\n3 minutes\nIn Azure API Management, policies allow the publisher to change the behavior of the API through configuration. Policies are a collection of Statements that are executed sequentially on the request or response of an API.\nPolicies are applied inside the gateway that sits between the API consumer and the managed API. The gateway receives all requests and usually forwards them unaltered to the underlying API. However a policy can apply changes to both the inbound request and outbound response. Policy expressions can be used as attribute values or text values in any of the API Management policies, unless the policy specifies otherwise.\nUnderstanding policy configuration\nThe policy definition is a simple XML document that describes a sequence of inbound and outbound statements. The XML can be edited directly in the definition window.\nThe configuration is divided into\ninbound\n,\nbackend\n,\noutbound\n, and\non-error\n. The series of specified policy statements is executed in order for a request and a response.\n<policies>\n  <inbound>\n    <!-- statements to be applied to the request go here -->\n  </inbound>\n  <backend>\n    <!-- statements to be applied before the request is forwarded to \n         the backend service go here -->\n  </backend>\n  <outbound>\n    <!-- statements to be applied to the response go here -->\n  </outbound>\n  <on-error>\n    <!-- statements to be applied if there is an error condition go here -->\n  </on-error>\n</policies>\nIf there's an error during the processing of a request, any remaining steps in the\ninbound\n,\nbackend\n, or\noutbound\nsections are skipped and execution jumps to the statements in the\non-error\nsection. By placing policy statements in the\non-error\nsection you can review the error by using the\ncontext.LastError\nproperty, inspect and customize the error response using the\nset-body\npolicy, and configure what happens if an error occurs.\nPolicy expressions\nUnless the policy specifies otherwise, policy expressions can be used as attribute values or text values in any of the API Management policies. A policy expression is either:\na single C# statement enclosed in\n@(expression)\n, or\na multi-statement C# code block, enclosed in\n@{expression}\n, that returns a value\nEach expression has access to the implicitly provided\ncontext\nvariable and an allowed subset of .NET Framework types.\nPolicy expressions\nprovide a sophisticated means to control traffic and modify API behavior without requiring you to write specialized code or modify backend services.\nThe following example uses policy expressions and the set-header policy to add user data to the incoming request. The added header includes the user ID associated with the subscription key in the request, and the region where the gateway processing the request is hosted.\n<policies>\n    <inbound>\n        <base />\n        <set-header name=\"x-request-context-data\" exists-action=\"override\">\n            <value>@(context.User.Id)</value>\n            <value>@(context.Deployment.Region)</value>\n      </set-header>\n    </inbound>\n</policies>\nApply policies specified at different scopes\nIf you have a policy at the global level and a policy configured for an API, then whenever that particular API is used both policies are applied. API Management allows for deterministic ordering of combined policy statements via the base element.\n<policies>\n    <inbound>\n        <cross-domain />\n        <base />\n        <find-and-replace from=\"xyz\" to=\"abc\" />\n    </inbound>\n</policies>\nIn the previous example policy definition, The\ncross-domain\nstatement would execute first. The\nfind-and-replace\npolicy would execute after any policies at a broader scope.\nFilter response content\nThe policy defined in following example demonstrates how to filter data elements from the response payload based on the product associated with the request.\nThe snippet assumes that response content is formatted as JSON and contains root-level properties named\nminutely\n,\nhourly\n,\ndaily\n, and\nflags\n.\n<policies>\n  <inbound>\n    <base />\n  </inbound>\n  <backend>\n    <base />\n  </backend>\n  <outbound>\n    <base />\n    <choose>\n      <when condition=\"@(context.Response.StatusCode == 200 && context.Product.Name.Equals(\"Starter\"))\">\n        <!-- NOTE that we are not using preserveContent=true when deserializing response body stream into a JSON object since we don't intend to access it again. See details on /azure/api-management/api-management-transformation-policies#SetBody -->\n        <set-body>\n          @{\n            var response = context.Response.Body.As<JObject>();\n            foreach (var key in new [] {\"minutely\", \"hourly\", \"daily\", \"flags\"}) {\n            response.Property (key).Remove ();\n           }\n          return response.ToString();\n          }\n    </set-body>\n      </when>\n    </choose>    \n  </outbound>\n  <on-error>\n    <base />\n  </on-error>\n</policies>\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Explore API Management policies"
                },
                {
                  "level": 2,
                  "text": "Understanding policy configuration"
                },
                {
                  "level": 2,
                  "text": "Policy expressions"
                },
                {
                  "level": 2,
                  "text": "Apply policies specified at different scopes"
                },
                {
                  "level": 2,
                  "text": "Filter response content"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [
                "<policies>\n  <inbound>\n    <!-- statements to be applied to the request go here -->\n  </inbound>\n  <backend>\n    <!-- statements to be applied before the request is forwarded to \n         the backend service go here -->\n  </backend>\n  <outbound>\n    <!-- statements to be applied to the response go here -->\n  </outbound>\n  <on-error>\n    <!-- statements to be applied if there is an error condition go here -->\n  </on-error>\n</policies>",
                "<policies>\n  <inbound>\n    <!-- statements to be applied to the request go here -->\n  </inbound>\n  <backend>\n    <!-- statements to be applied before the request is forwarded to \n         the backend service go here -->\n  </backend>\n  <outbound>\n    <!-- statements to be applied to the response go here -->\n  </outbound>\n  <on-error>\n    <!-- statements to be applied if there is an error condition go here -->\n  </on-error>\n</policies>",
                "context.LastError",
                "@(expression)",
                "@{expression}",
                "<policies>\n    <inbound>\n        <base />\n        <set-header name=\"x-request-context-data\" exists-action=\"override\">\n            <value>@(context.User.Id)</value>\n            <value>@(context.Deployment.Region)</value>\n      </set-header>\n    </inbound>\n</policies>",
                "<policies>\n    <inbound>\n        <base />\n        <set-header name=\"x-request-context-data\" exists-action=\"override\">\n            <value>@(context.User.Id)</value>\n            <value>@(context.Deployment.Region)</value>\n      </set-header>\n    </inbound>\n</policies>",
                "<policies>\n    <inbound>\n        <cross-domain />\n        <base />\n        <find-and-replace from=\"xyz\" to=\"abc\" />\n    </inbound>\n</policies>",
                "<policies>\n    <inbound>\n        <cross-domain />\n        <base />\n        <find-and-replace from=\"xyz\" to=\"abc\" />\n    </inbound>\n</policies>",
                "cross-domain",
                "find-and-replace",
                "<policies>\n  <inbound>\n    <base />\n  </inbound>\n  <backend>\n    <base />\n  </backend>\n  <outbound>\n    <base />\n    <choose>\n      <when condition=\"@(context.Response.StatusCode == 200 && context.Product.Name.Equals(\"Starter\"))\">\n        <!-- NOTE that we are not using preserveContent=true when deserializing response body stream into a JSON object since we don't intend to access it again. See details on /azure/api-management/api-management-transformation-policies#SetBody -->\n        <set-body>\n          @{\n            var response = context.Response.Body.As<JObject>();\n            foreach (var key in new [] {\"minutely\", \"hourly\", \"daily\", \"flags\"}) {\n            response.Property (key).Remove ();\n           }\n          return response.ToString();\n          }\n    </set-body>\n      </when>\n    </choose>    \n  </outbound>\n  <on-error>\n    <base />\n  </on-error>\n</policies>",
                "<policies>\n  <inbound>\n    <base />\n  </inbound>\n  <backend>\n    <base />\n  </backend>\n  <outbound>\n    <base />\n    <choose>\n      <when condition=\"@(context.Response.StatusCode == 200 && context.Product.Name.Equals(\"Starter\"))\">\n        <!-- NOTE that we are not using preserveContent=true when deserializing response body stream into a JSON object since we don't intend to access it again. See details on /azure/api-management/api-management-transformation-policies#SetBody -->\n        <set-body>\n          @{\n            var response = context.Response.Body.As<JObject>();\n            foreach (var key in new [] {\"minutely\", \"hourly\", \"daily\", \"flags\"}) {\n            response.Property (key).Remove ();\n           }\n          return response.ToString();\n          }\n    </set-body>\n      </when>\n    </choose>    \n  </outbound>\n  <on-error>\n    <base />\n  </on-error>\n</policies>"
              ],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                },
                {
                  "url": "/en-us/azure/api-management/api-management-policy-expressions",
                  "text": "Policy expressions"
                }
              ]
            },
            {
              "number": 5,
              "title": "Create advanced policies",
              "url": "https://learn.microsoft.com/en-us/training/modules/explore-api-management/5-create-advanced-policies",
              "href": "5-create-advanced-policies",
              "content": "Read in English\nAdd\nAdd to plan\nCreate advanced policies\nCompleted\n3 minutes\nThis unit provides a reference for the following API Management policies:\nControl flow - Conditionally applies policy statements based on the results of the evaluation of Boolean expressions.\nForward request - Forwards the request to the backend service.\nLimit concurrency - Prevents enclosed policies from executing by more than the specified number of requests at a time.\nLog to Event Hubs - Sends messages in the specified format to an event hub defined by a Logger entity.\nMock response - Aborts pipeline execution and returns a mocked response directly to the caller.\nRetry - Retries execution of the enclosed policy statements, if and until the condition is met. Execution repeats at the specified time intervals and up to the specified retry count.\nControl flow\nThe\nchoose\npolicy applies enclosed policy statements based on the outcome of evaluation of boolean expressions, similar to an if-then-else or a switch construct in a programming language.\n<choose>\n    <when condition=\"Boolean expression | Boolean constant\">\n        <!â one or more policy statements to be applied if the above condition is true  -->\n    </when>\n    <when condition=\"Boolean expression | Boolean constant\">\n        <!â one or more policy statements to be applied if the above condition is true  -->\n    </when>\n    <otherwise>\n        <!â one or more policy statements to be applied if none of the above conditions are true  -->\n</otherwise>\n</choose>\nThe control flow policy must contain at least one\n<when/>\nelement. The\n<otherwise/>\nelement is optional. Conditions in\n<when/>\nelements are evaluated in order of their appearance within the policy. Policy statements enclosed within the first\n<when/>\nelement are applied when the condition attribute is true. Policies enclosed within the\n<otherwise/>\nelement, if present, are applied if all of the\n<when/>\nelement condition attributes are false.\nForward request\nThe\nforward-request\npolicy forwards the incoming request to the backend service specified in the request context. The backend service URL is specified in the API settings and can be changed using the set backend service policy.\nRemoving this policy results in the request not being forwarded to the backend service. The policies in the outbound section are evaluated immediately upon the successful completion of the policies in the inbound section.\n<forward-request timeout=\"time in seconds\" follow-redirects=\"true | false\"/>\nLimit concurrency\nThe\nlimit-concurrency\npolicy prevents enclosed policies from executing by more than the specified number of requests at any time. When requests exceed that number, new requests fail immediately with a\n429 Too Many Requests\nstatus code.\n<limit-concurrency key=\"expression\" max-count=\"number\">\n        <!â nested policy statements -->\n</limit-concurrency>\nLog to Event Hubs\nThe\nlog-to-eventhub\npolicy sends messages in the specified format to an event hub defined by a Logger entity. As its name implies, the policy is used for saving selected request or response context information for online or offline analysis.\n<log-to-eventhub logger-id=\"id of the logger entity\" partition-id=\"index of the partition where messages are sent\" partition-key=\"value used for partition assignment\">\n  Expression returning a string to be logged\n</log-to-eventhub>\nMock response\nThe\nmock-response\n, as the name implies, is used to mock APIs and operations. It aborts normal pipeline execution and returns a mocked response to the caller. The policy always tries to return responses of highest fidelity. It prefers response content examples, whenever available. It generates sample responses from schemas, when schemas are provided and examples aren't. If examples or schemas aren't found, responses with no content are returned.\n<mock-response status-code=\"code\" content-type=\"media type\"/>\nRetry\nThe\nretry\npolicy executes its child policies once and then retries their execution until the retry\ncondition\nbecomes\nfalse\nor retry\ncount\nis exhausted.\n<retry\n    condition=\"boolean expression or literal\"\n    count=\"number of retry attempts\"\n    interval=\"retry interval in seconds\"\n    max-interval=\"maximum retry interval in seconds\"\n    delta=\"retry interval delta in seconds\"\n    first-fast-retry=\"boolean expression or literal\">\n        <!-- One or more child policies. No restrictions -->\n</retry>\nReturn response\nThe\nreturn-response\npolicy aborts pipeline execution and returns either a default or custom response to the caller. Default response is\n200 OK\nwith no body. Custom response can be specified via a context variable or policy statements. When both are provided, the policy statement modifies the context variable before being returned to the caller.\n<return-response response-variable-name=\"existing context variable\">\n  <set-header/>\n  <set-body/>\n  <set-status/>\n</return-response>\nOther resources\nVisit\nAPI Management policies\nfor more policy examples.\nError handling in API Management policies\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Create advanced policies"
                },
                {
                  "level": 2,
                  "text": "Control flow"
                },
                {
                  "level": 2,
                  "text": "Forward request"
                },
                {
                  "level": 2,
                  "text": "Limit concurrency"
                },
                {
                  "level": 2,
                  "text": "Log to Event Hubs"
                },
                {
                  "level": 2,
                  "text": "Mock response"
                },
                {
                  "level": 2,
                  "text": "Retry"
                },
                {
                  "level": 2,
                  "text": "Return response"
                },
                {
                  "level": 2,
                  "text": "Other resources"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [
                "<choose>\n    <when condition=\"Boolean expression | Boolean constant\">\n        <!â one or more policy statements to be applied if the above condition is true  -->\n    </when>\n    <when condition=\"Boolean expression | Boolean constant\">\n        <!â one or more policy statements to be applied if the above condition is true  -->\n    </when>\n    <otherwise>\n        <!â one or more policy statements to be applied if none of the above conditions are true  -->\n</otherwise>\n</choose>",
                "<choose>\n    <when condition=\"Boolean expression | Boolean constant\">\n        <!â one or more policy statements to be applied if the above condition is true  -->\n    </when>\n    <when condition=\"Boolean expression | Boolean constant\">\n        <!â one or more policy statements to be applied if the above condition is true  -->\n    </when>\n    <otherwise>\n        <!â one or more policy statements to be applied if none of the above conditions are true  -->\n</otherwise>\n</choose>",
                "<otherwise/>",
                "<otherwise/>",
                "forward-request",
                "<forward-request timeout=\"time in seconds\" follow-redirects=\"true | false\"/>",
                "<forward-request timeout=\"time in seconds\" follow-redirects=\"true | false\"/>",
                "limit-concurrency",
                "<limit-concurrency key=\"expression\" max-count=\"number\">\n        <!â nested policy statements -->\n</limit-concurrency>",
                "<limit-concurrency key=\"expression\" max-count=\"number\">\n        <!â nested policy statements -->\n</limit-concurrency>",
                "log-to-eventhub",
                "<log-to-eventhub logger-id=\"id of the logger entity\" partition-id=\"index of the partition where messages are sent\" partition-key=\"value used for partition assignment\">\n  Expression returning a string to be logged\n</log-to-eventhub>",
                "<log-to-eventhub logger-id=\"id of the logger entity\" partition-id=\"index of the partition where messages are sent\" partition-key=\"value used for partition assignment\">\n  Expression returning a string to be logged\n</log-to-eventhub>",
                "mock-response",
                "<mock-response status-code=\"code\" content-type=\"media type\"/>",
                "<mock-response status-code=\"code\" content-type=\"media type\"/>",
                "<retry\n    condition=\"boolean expression or literal\"\n    count=\"number of retry attempts\"\n    interval=\"retry interval in seconds\"\n    max-interval=\"maximum retry interval in seconds\"\n    delta=\"retry interval delta in seconds\"\n    first-fast-retry=\"boolean expression or literal\">\n        <!-- One or more child policies. No restrictions -->\n</retry>",
                "<retry\n    condition=\"boolean expression or literal\"\n    count=\"number of retry attempts\"\n    interval=\"retry interval in seconds\"\n    max-interval=\"maximum retry interval in seconds\"\n    delta=\"retry interval delta in seconds\"\n    first-fast-retry=\"boolean expression or literal\">\n        <!-- One or more child policies. No restrictions -->\n</retry>",
                "return-response",
                "<return-response response-variable-name=\"existing context variable\">\n  <set-header/>\n  <set-body/>\n  <set-status/>\n</return-response>",
                "<return-response response-variable-name=\"existing context variable\">\n  <set-header/>\n  <set-body/>\n  <set-status/>\n</return-response>"
              ],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                },
                {
                  "url": "/en-us/azure/api-management/api-management-policies",
                  "text": "API Management policies"
                },
                {
                  "url": "/en-us/azure/api-management/api-management-error-handling-policies",
                  "text": "Error handling in API Management policies"
                }
              ]
            },
            {
              "number": 6,
              "title": "Secure APIs by using subscriptions",
              "url": "https://learn.microsoft.com/en-us/training/modules/explore-api-management/6-secure-access-api-subscriptions",
              "href": "6-secure-access-api-subscriptions",
              "content": "Read in English\nAdd\nAdd to plan\nSecure APIs by using subscriptions\nCompleted\n3 minutes\nWhen you publish APIs through API Management, it's easy and common to secure access to those APIs by using subscription keys. Developers who need to consume the published APIs must include a valid subscription key in HTTP requests when they make calls to those APIs. The API Management gateway rejects calls without a subscription key and the calls aren't forwarded to the back-end services.\nTo get a subscription key for accessing APIs, a subscription is required. A subscription is essentially a named container for a pair of subscription keys. Developers who need to consume the published APIs can get subscriptions. And they don't need approval from API publishers. API publishers can also create subscriptions directly for API consumers.\nNote\nAPI Management also supports other mechanisms for securing access to APIs, including: OAuth2.0, Client certificates, and IP allow listing.\nSubscriptions and Keys\nA subscription key is a unique autogenerated key that can be passed through in the headers of the client request or as a query string parameter. The key is directly related to a subscription, which can be scoped to different areas. Subscriptions give you granular control over permissions and policies.\nThe three main subscription scopes are:\nScope\nDetails\nAll APIs\nApplies to every API accessible from the gateway\nSingle API\nThis scope applies to a single imported API and all of its endpoints\nProduct\nA product is a collection of one or more APIs that you configure in API Management. You can assign APIs to more than one product. Products can have different access rules, usage quotas, and terms of use.\nApplications that call a protected API must include the key in every request.\nYou can regenerate these subscription keys at any time, for example, if you suspect that a key has been shared with unauthorized users.\nEvery subscription has two keys, a primary and a secondary. Having two keys makes it easier when you do need to regenerate a key. For example, if you want to change the primary key and avoid downtime, use the secondary key in your apps.\nFor products where subscriptions are enabled, clients must supply a key when making calls to APIs in that product. Developers can obtain a key by submitting a subscription request. If you approve the request, you must send them the subscription key securely, for example, in an encrypted message. This step is a core part of the API Management workflow.\nCall an API with the subscription key\nApplications must include a valid key in all HTTP requests when they make calls to API endpoints that are protected by a subscription. Keys can be passed in the request header, or as a query string in the URL.\nThe default header name is\nOcp-Apim-Subscription-Key\n, and the default query string is\nsubscription-key\n.\nTo test out your API calls, you can use the developer portal, or command-line tools, such as\ncurl\n. Here's an example of a\nGET\nrequest using the developer portal, which shows the subscription key header:\nHere's how you can pass a key in the request header using\ncurl\n:\ncurl --header \"Ocp-Apim-Subscription-Key: <key string>\" https://<apim gateway>.azure-api.net/api/path\nHere's an example\ncurl\ncommand that passes a key in the URL as a query string:\ncurl https://<apim gateway>.azure-api.net/api/path?subscription-key=<key string>\nIf the key isn't passed in the header, or as a query string in the URL, you get a\n401 Access Denied\nresponse from the API gateway.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Secure APIs by using subscriptions"
                },
                {
                  "level": 2,
                  "text": "Subscriptions and Keys"
                },
                {
                  "level": 2,
                  "text": "Call an API with the subscription key"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [
                "curl --header \"Ocp-Apim-Subscription-Key: <key string>\" https://<apim gateway>.azure-api.net/api/path",
                "curl --header \"Ocp-Apim-Subscription-Key: <key string>\" https://<apim gateway>.azure-api.net/api/path",
                "curl https://<apim gateway>.azure-api.net/api/path?subscription-key=<key string>",
                "curl https://<apim gateway>.azure-api.net/api/path?subscription-key=<key string>"
              ],
              "images": [
                {
                  "src": "../../wwl-azure/explore-api-management/media/subscription-keys.png",
                  "absolute_url": "https://learn.microsoft.com/en-us/wwl-azure/explore-api-management/media/subscription-keys.png",
                  "alt_text": "Image showing the Subscriptions screen.",
                  "title": "",
                  "filename": "subscription-keys.png",
                  "image_type": "screenshot",
                  "context": {
                    "preceding_heading": "Subscriptions and Keys",
                    "following_text": "Every subscription has two keys, a primary and a secondary. Having two keys makes it easier when you do need to regenerate a key. For example, if you want to change the primary key and avoid downtime,",
                    "figure_caption": "",
                    "parent_section": ""
                  }
                },
                {
                  "src": "../../wwl-azure/explore-api-management/media/key-header-portal.png",
                  "absolute_url": "https://learn.microsoft.com/en-us/wwl-azure/explore-api-management/media/key-header-portal.png",
                  "alt_text": "Call API from developer portal",
                  "title": "",
                  "filename": "key-header-portal.png",
                  "image_type": "screenshot",
                  "context": {
                    "preceding_heading": "Call an API with the subscription key",
                    "following_text": "Here's how you can pass a key in the request header usingcurl:",
                    "figure_caption": "",
                    "parent_section": ""
                  }
                }
              ],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 7,
              "title": "Secure APIs by using certificates",
              "url": "https://learn.microsoft.com/en-us/training/modules/explore-api-management/7-secure-access-api-certificates",
              "href": "7-secure-access-api-certificates",
              "content": "Read in English\nAdd\nAdd to plan\nSecure APIs by using certificates\nCompleted\n3 minutes\nCertificates can be used to provide Transport Layer Security (TLS) mutual authentication between the client and the API gateway. You can configure the API Management gateway to allow only requests with certificates containing a specific thumbprint. The authorization at the gateway level is handled through inbound policies.\nTransport Layer Security client authentication\nWith TLS client authentication, the API Management gateway can inspect the certificate contained within the client request and check for properties like:\nProperty\nDescription\nCertificate Authority (CA)\nOnly allow certificates signed by a particular CA\nThumbprint\nAllow certificates containing a specified thumbprint\nSubject\nOnly allow certificates with a specified subject\nExpiration Date\nDon't allow expired certificates\nThese properties aren't mutually exclusive and they can be mixed together to form your own policy requirements. For instance, you can specify that the certificate passed in the request is signed and isn't expired.\nClient certificates are signed to ensure that they aren't tampered with. When a partner sends you a certificate, verify that it comes from them and not an imposter. There are two common ways to verify a certificate:\nCheck who issued the certificate. If the issuer was a certificate authority that you trust, you can use the certificate. You can configure the trusted certificate authorities in the Azure portal to automate this process.\nEnsure you trust the source of any self-signed certificates.\nAccepting client certificates in the Consumption tier\nThe Consumption tier in API Management is designed to conform with serverless design principles. If you build your APIs from serverless technologies, such as Azure Functions, this tier is a good fit. In the Consumption tier, you must explicitly enable the use of client certificates, which you can do on the\nCustom domains\npage. This step isn't necessary in other tiers.\nCertificate Authorization Policies\nCreate these policies in the inbound processing policy file within the API Management gateway:\nCheck the thumbprint of a client certificate\nEvery client certificate includes a thumbprint, which is a hash, calculated from other certificate properties. The thumbprint ensures that the values in the certificate haven't been altered since the certificate was issued by the certificate authority. You can check the thumbprint in your policy. The following example checks the thumbprint of the certificate passed in the request:\n<choose>\n    <when condition=\"@(context.Request.Certificate == null || context.Request.Certificate.Thumbprint != \"desired-thumbprint\")\" >\n        <return-response>\n            <set-status code=\"403\" reason=\"Invalid client certificate\" />\n        </return-response>\n    </when>\n</choose>\nCheck the thumbprint against certificates uploaded to API Management\nIn the previous example, only one thumbprint would work so only one certificate would be validated. Usually, each customer or partner company would pass a different certificate with a different thumbprint. To support this scenario, obtain the certificates from your partners and use the\nClient certificates\npage in the Azure portal to upload them to the API Management resource. Then add this code to your policy:\n<choose>\n    <when condition=\"@(context.Request.Certificate == null || !context.Request.Certificate.Verify()  || !context.Deployment.Certificates.Any(c => c.Value.Thumbprint == context.Request.Certificate.Thumbprint))\" >\n        <return-response>\n            <set-status code=\"403\" reason=\"Invalid client certificate\" />\n        </return-response>\n    </when>\n</choose>\nCheck the issuer and subject of a client certificate\nThis example checks the issuer and subject of the certificate passed in the request:\n<choose>\n    <when condition=\"@(context.Request.Certificate == null || context.Request.Certificate.Issuer != \"trusted-issuer\" || context.Request.Certificate.SubjectName.Name != \"expected-subject-name\")\" >\n        <return-response>\n            <set-status code=\"403\" reason=\"Invalid client certificate\" />\n        </return-response>\n    </when>\n</choose>\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Secure APIs by using certificates"
                },
                {
                  "level": 2,
                  "text": "Transport Layer Security client authentication"
                },
                {
                  "level": 2,
                  "text": "Accepting client certificates in the Consumption tier"
                },
                {
                  "level": 2,
                  "text": "Certificate Authorization Policies"
                },
                {
                  "level": 3,
                  "text": "Check the thumbprint of a client certificate"
                },
                {
                  "level": 3,
                  "text": "Check the thumbprint against certificates uploaded to API Management"
                },
                {
                  "level": 3,
                  "text": "Check the issuer and subject of a client certificate"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [
                "<choose>\n    <when condition=\"@(context.Request.Certificate == null || context.Request.Certificate.Thumbprint != \"desired-thumbprint\")\" >\n        <return-response>\n            <set-status code=\"403\" reason=\"Invalid client certificate\" />\n        </return-response>\n    </when>\n</choose>",
                "<choose>\n    <when condition=\"@(context.Request.Certificate == null || context.Request.Certificate.Thumbprint != \"desired-thumbprint\")\" >\n        <return-response>\n            <set-status code=\"403\" reason=\"Invalid client certificate\" />\n        </return-response>\n    </when>\n</choose>",
                "<choose>\n    <when condition=\"@(context.Request.Certificate == null || !context.Request.Certificate.Verify()  || !context.Deployment.Certificates.Any(c => c.Value.Thumbprint == context.Request.Certificate.Thumbprint))\" >\n        <return-response>\n            <set-status code=\"403\" reason=\"Invalid client certificate\" />\n        </return-response>\n    </when>\n</choose>",
                "<choose>\n    <when condition=\"@(context.Request.Certificate == null || !context.Request.Certificate.Verify()  || !context.Deployment.Certificates.Any(c => c.Value.Thumbprint == context.Request.Certificate.Thumbprint))\" >\n        <return-response>\n            <set-status code=\"403\" reason=\"Invalid client certificate\" />\n        </return-response>\n    </when>\n</choose>",
                "<choose>\n    <when condition=\"@(context.Request.Certificate == null || context.Request.Certificate.Issuer != \"trusted-issuer\" || context.Request.Certificate.SubjectName.Name != \"expected-subject-name\")\" >\n        <return-response>\n            <set-status code=\"403\" reason=\"Invalid client certificate\" />\n        </return-response>\n    </when>\n</choose>",
                "<choose>\n    <when condition=\"@(context.Request.Certificate == null || context.Request.Certificate.Issuer != \"trusted-issuer\" || context.Request.Certificate.SubjectName.Name != \"expected-subject-name\")\" >\n        <return-response>\n            <set-status code=\"403\" reason=\"Invalid client certificate\" />\n        </return-response>\n    </when>\n</choose>"
              ],
              "images": [
                {
                  "src": "../../wwl-azure/explore-api-management/media/configure-request-certificates.png",
                  "absolute_url": "https://learn.microsoft.com/en-us/wwl-azure/explore-api-management/media/configure-request-certificates.png",
                  "alt_text": "Configure the gateway to request certificates",
                  "title": "",
                  "filename": "configure-request-certificates.png",
                  "image_type": "illustration",
                  "context": {
                    "preceding_heading": "Accepting client certificates in the Consumption tier",
                    "following_text": "Create these policies in the inbound processing policy file within the API Management gateway:",
                    "figure_caption": "",
                    "parent_section": ""
                  }
                },
                {
                  "src": "../../wwl-azure/explore-api-management/media/inbound-policy.png",
                  "absolute_url": "https://learn.microsoft.com/en-us/wwl-azure/explore-api-management/media/inbound-policy.png",
                  "alt_text": "Inbound processing policy button",
                  "title": "",
                  "filename": "inbound-policy.png",
                  "image_type": "icon",
                  "context": {
                    "preceding_heading": "Certificate Authorization Policies",
                    "following_text": "Every client certificate includes a thumbprint, which is a hash, calculated from other certificate properties. The thumbprint ensures that the values in the certificate haven't been altered since the ",
                    "figure_caption": "",
                    "parent_section": ""
                  }
                }
              ],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 8,
              "title": "Exercise - Import and configure an API with Azure API Management",
              "url": "https://learn.microsoft.com/en-us/training/modules/explore-api-management/8-exercise-import-api",
              "href": "8-exercise-import-api",
              "content": "Read in English\nAdd\nAdd to plan\nExercise - Import and configure an API with Azure API Management\nCompleted\n20 minutes\nIn this exercise, you create an Azure API Management instance, import an OpenAPI specification backend API, configure the API settings including the web service URL and subscription requirements, and test the API operations to verify they work correctly.\nTasks performed in this exercise:\nCreate an Azure API Management (APIM) instance\nImport an API\nConfigure the backend settings\nTest the API\nThis exercise takes approximately\n20\nminutes to complete.\nBefore you start\nTo complete the exercise, you need:\nAn Azure subscription. If you don't already have one, you can sign up for one\nhttps://azure.microsoft.com/\n.\nGet started\nSelect the\nLaunch Exercise\nbutton to open the exercise instructions in a new browser window. When you're finished with the exercise, return here to:\nComplete the module\nEarn a badge for completing this module\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Exercise - Import and configure an API with Azure API Management"
                },
                {
                  "level": 2,
                  "text": "Before you start"
                },
                {
                  "level": 2,
                  "text": "Get started"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [
                {
                  "src": "../../wwl-azure/explore-api-management/media/launch-exercise.png",
                  "absolute_url": "https://learn.microsoft.com/en-us/wwl-azure/explore-api-management/media/launch-exercise.png",
                  "alt_text": "Button to launch exercise.",
                  "title": "",
                  "filename": "launch-exercise.png",
                  "image_type": "icon",
                  "context": {
                    "preceding_heading": "Get started",
                    "following_text": "Was this page helpful?",
                    "figure_caption": "",
                    "parent_section": ""
                  }
                }
              ],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                },
                {
                  "url": "https://azure.microsoft.com/",
                  "text": "https://azure.microsoft.com/"
                }
              ]
            },
            {
              "number": 9,
              "title": "Module assessment",
              "url": "https://learn.microsoft.com/en-us/training/modules/explore-api-management/9-knowledge-check",
              "href": "9-knowledge-check",
              "content": "Read in English\nAdd\nAdd to plan\nModule assessment\nCompleted\n3 minutes\nCheck your knowledge\n1.\nWhich of the following components of the API Management service would a developer use if they need to create an account and subscribe to get API keys?\nAPI gateway\nAzure portal\nDeveloper portal\n2.\nWhich of the following API Management policies applies a policy based on a condition?\nforward-request\nchoose\nreturn-response\nYou must answer all questions before checking your work.\nYou must answer all questions before checking your work.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Module assessment"
                },
                {
                  "level": 2,
                  "text": "Check your knowledge"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 10,
              "title": "Summary",
              "url": "https://learn.microsoft.com/en-us/training/modules/explore-api-management/10-summary",
              "href": "10-summary",
              "content": "Read in English\nAdd\nAdd to plan\nSummary\nCompleted\n3 minutes\nIn this module, you learned how to:\nDescribe the components, and their function, of the API Management service.\nExplain how API gateways can help manage calls to your APIs.\nSecure access to APIs by using subscriptions and certificates.\nImport and configure an API.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Summary"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "title": "Develop event-based solutions",
      "url": "https://learn.microsoft.com/en-us/training/paths/az-204-develop-event-based-solutions/",
      "learn_uid": "learn.wwl.az-204-develop-event-based-solutions",
      "modules": [
        {
          "title": "Explore Azure Event Grid",
          "url": "https://learn.microsoft.com/en-us/training/modules/azure-event-grid/",
          "description": "",
          "learning_objectives": [],
          "prerequisites": [],
          "units": [
            {
              "number": 1,
              "title": "Introduction",
              "url": "https://learn.microsoft.com/en-us/training/modules/azure-event-grid/1-introduction",
              "href": "1-introduction",
              "content": "Read in English\nAdd\nAdd to plan\nIntroduction\nCompleted\n3 minutes\nAzure Event Grid is deeply integrated with Azure services and can be integrated with third-party services. It simplifies event consumption and lowers costs by eliminating the need for constant polling. Event Grid efficiently and reliably routes events from Azure and non-Azure resources, and distributes the events to registered subscriber endpoints.\nAfter completing this module, you'll be able to:\nDescribe how Event Grid operates and how it connects to services and event handlers.\nExplain how Event Grid delivers events and how it handles errors.\nImplement authentication and authorization.\nRoute events to a custom endpoint.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Introduction"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 2,
              "title": "Explore Azure Event Grid",
              "url": "https://learn.microsoft.com/en-us/training/modules/azure-event-grid/2-event-grid-overview",
              "href": "2-event-grid-overview",
              "content": "Read in English\nAdd\nAdd to plan\nExplore Azure Event Grid\nCompleted\n5 minutes\nAzure Event Grid is a highly scalable, fully managed Pub Sub message distribution service that offers flexible message consumption patterns using the Hypertext Transfer Protocol (HTTP) and Message Queuing Telemetry Transport (MQTT) protocols. With Azure Event Grid, you can build data pipelines with device data, integrate applications, and build event-driven serverless architectures. Event Grid enables clients to publish and subscribe to messages over the MQTT v3.1.1 and v5.0 protocols to support Internet of Things (IoT) solutions. Through HTTP, Event Grid enables you to build event-driven solutions where a publisher service announces its system state changes (events) to subscriber applications. Event Grid can be configured to send events to subscribers (push delivery) or subscribers can connect to Event Grid to read events (pull delivery). Event Grid supports CloudEvents 1.0 specification to provide interoperability across systems.\nConcepts in Azure Event Grid\nThere are several concepts in Azure Event Grid you need to understand to help you get started.\nPublishers\nA publisher is the application that sends events to Event Grid. It can be the same application where the events originated, the event source. Azure services publish events to Event Grid to announce an occurrence in their service. You can publish events from your own application. Organizations that host services outside of Azure can publish events through Event Grid too.\nA\npartner\nis a kind of publisher that sends events from its system to make them available to Azure customers. Partners not only can publish events to Azure Event Grid, but they can also receive events from it. These capabilities are enabled through the Partner Events feature.\nEvents and CloudEvents\nAn event is the smallest amount of information that fully describes something that happened in a system. Every event has common information like\nsource\nof the event, the\ntime\nthe event took place, and a unique identifier. Every event also has specific information that is only relevant to the specific type of event.\nEvent Grid conforms to Cloud Native Computing Foundationâs open standard\nCloudEvents 1.0\nspecification using the\nHTTP protocol binding\nwith\nJSON format\n. It means that your solutions publish and consume event messages using a format like the following example:\n{\n    \"specversion\" : \"1.0\",\n    \"type\" : \"com.yourcompany.order.created\",\n    \"source\" : \"https://yourcompany.com/orders/\",\n    \"subject\" : \"O-28964\",\n    \"id\" : \"A234-1234-1234\",\n    \"time\" : \"2018-04-05T17:31:00Z\",\n    \"comexampleextension1\" : \"value\",\n    \"comexampleothervalue\" : 5,\n    \"datacontenttype\" : \"application/json\",\n    \"data\" : {\n       \"orderId\" : \"O-28964\",\n       \"URL\" : \"https://com.yourcompany/orders/O-28964\"\n    }\n}\nThe maximum allowed size for an event is 1 MB. Events over 64 KB are charged in 64-KB increments.\nEvent sources\nAn event source is where the event happens. Each event source is related to one or more event types. For example, Azure Storage is the event source for blob created events. IoT Hub is the event source for device created events. Your application is the event source for custom events that you define. Event sources are responsible for sending events to Event Grid.\nTopics\nA topic holds events that have been published to Event Grid. You typically use a topic resource for a collection of related events. To respond to certain types of events, subscribers (an Azure service or other applications) decide which topics to subscribe to. There are several kinds of topics: custom topics, system topics, and partner topics.\nSystem topics\nare built-in topics provided by Azure services. You don't see system topics in your Azure subscription because the publisher owns the topics, but you can subscribe to them. To subscribe, you provide information about the resource you want to receive events from. As long as you have access to the resource, you can subscribe to its events.\nCustom topics\nare application and third-party topics. When you create or are assigned access to a custom topic, you see that custom topic in your subscription.\nPartner topics\nare a kind of topic used to subscribe to events published by a partner. The feature that enables this type of integration is called Partner Events. Through that integration, you get a partner topic where events from a partner system are made available. Once you have a partner topic, you create an event subscription as you would do for any other type of topic.\nEvent subscriptions\nA subscription tells Event Grid which events on a topic you're interested in receiving. When creating the subscription, you provide an endpoint for handling the event. You can filter the events that are sent to the endpoint. You can filter by event type, or subject pattern. Set an expiration for event subscriptions that are only needed for a limited time and you don't want to worry about cleaning up those subscriptions.\nEvent handlers\nFrom an Event Grid perspective, an event handler is the place where the event is sent. The handler takes some further action to process the event. Event Grid supports several handler types. You can use a supported Azure service or your own webhook as the handler. Depending on the type of handler, Event Grid follows different mechanisms to guarantee the delivery of the event. For HTTP webhook event handlers, the event is retried until the handler returns a status code of\n200 â OK\n. For Azure Storage Queue, the events are retried until the Queue service successfully processes the message push into the queue.\nSecurity\nEvent Grid provides security for subscribing to topics and when publishing events to topics. When subscribing, you must have adequate permissions on the Event Grid topic. If using push delivery, the event handler is an Azure service, and a managed identity is used to authenticate Event Grid, the managed identity should have an appropriate RBAC role. For example, if sending events to Event Hubs, the managed identity used in the event subscription should be a member of the Event Hubs Data Sender role.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Explore Azure Event Grid"
                },
                {
                  "level": 2,
                  "text": "Concepts in Azure Event Grid"
                },
                {
                  "level": 3,
                  "text": "Publishers"
                },
                {
                  "level": 3,
                  "text": "Events and CloudEvents"
                },
                {
                  "level": 3,
                  "text": "Event sources"
                },
                {
                  "level": 3,
                  "text": "Topics"
                },
                {
                  "level": 3,
                  "text": "Event subscriptions"
                },
                {
                  "level": 3,
                  "text": "Event handlers"
                },
                {
                  "level": 3,
                  "text": "Security"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [
                "{\n    \"specversion\" : \"1.0\",\n    \"type\" : \"com.yourcompany.order.created\",\n    \"source\" : \"https://yourcompany.com/orders/\",\n    \"subject\" : \"O-28964\",\n    \"id\" : \"A234-1234-1234\",\n    \"time\" : \"2018-04-05T17:31:00Z\",\n    \"comexampleextension1\" : \"value\",\n    \"comexampleothervalue\" : 5,\n    \"datacontenttype\" : \"application/json\",\n    \"data\" : {\n       \"orderId\" : \"O-28964\",\n       \"URL\" : \"https://com.yourcompany/orders/O-28964\"\n    }\n}",
                "{\n    \"specversion\" : \"1.0\",\n    \"type\" : \"com.yourcompany.order.created\",\n    \"source\" : \"https://yourcompany.com/orders/\",\n    \"subject\" : \"O-28964\",\n    \"id\" : \"A234-1234-1234\",\n    \"time\" : \"2018-04-05T17:31:00Z\",\n    \"comexampleextension1\" : \"value\",\n    \"comexampleothervalue\" : 5,\n    \"datacontenttype\" : \"application/json\",\n    \"data\" : {\n       \"orderId\" : \"O-28964\",\n       \"URL\" : \"https://com.yourcompany/orders/O-28964\"\n    }\n}"
              ],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                },
                {
                  "url": "https://github.com/cloudevents/spec",
                  "text": "CloudEvents 1.0"
                },
                {
                  "url": "https://github.com/cloudevents/spec/blob/v1.0.2/cloudevents/bindings/http-protocol-binding.md",
                  "text": "HTTP protocol binding"
                },
                {
                  "url": "https://github.com/cloudevents/spec/blob/v1.0.2/cloudevents/formats/json-format.md",
                  "text": "JSON format"
                }
              ]
            },
            {
              "number": 3,
              "title": "Discover event schemas",
              "url": "https://learn.microsoft.com/en-us/training/modules/azure-event-grid/3-event-grid-schema",
              "href": "3-event-grid-schema",
              "content": "Read in English\nAdd\nAdd to plan\nDiscover event schemas\nCompleted\n3 minutes\nAzure Event Grid supports two types of event schemas: Event Grid event schema and Cloud event schema. Events consist of a set of four required string properties. The properties are common to all events from any publisher.\nThe data object has properties that are specific to each publisher. For system topics, these properties are specific to the resource provider, such as Azure Storage or Azure Event Hubs.\nEvent sources send events to Azure Event Grid in an array, which can have several event objects. When posting events to an Event Grid topic, the array can have a total size of up to 1 MB. Each event in the array is limited to 1 MB. If an event or the array is greater than the size limits, you receive the response\n413 Payload Too Large\n. Operations are charged in 64 KB increments though. So, events over 64 KB incur operations charges as though they were multiple events. For example, an event that is 130 KB would incur charges as though it were three separate events.\nEvent Grid sends the events to subscribers in an array that has a single event. You can find the JSON schema for the Event Grid event and each Azure publisher's data payload in the\nEvent Schema store\n.\nEvent schema\nThe following example shows the properties that are used by all event publishers:\n[\n  {\n    \"topic\": string,\n    \"subject\": string,\n    \"id\": string,\n    \"eventType\": string,\n    \"eventTime\": string,\n    \"data\":{\n      object-unique-to-each-publisher\n    },\n    \"dataVersion\": string,\n    \"metadataVersion\": string\n  }\n]\nEvent properties\nAll events have the same following top-level data:\nProperty\nType\nRequired\nDescription\ntopic\nstring\nNo. If not included, Event Grid stamps onto the event. If included, it must match the Event Grid topic Azure Resource Manager ID exactly.\nFull resource path to the event source. This field isn't writeable. Event Grid provides this value.\nsubject\nstring\nYes\nPublisher-defined path to the event subject.\neventType\nstring\nYes\nOne of the registered event types for this event source.\neventTime\nstring\nYes\nThe time the event is generated based on the provider's UTC time.\nid\nstring\nYes\nUnique identifier for the event.\ndata\nobject\nNo\nEvent data specific to the resource provider.\ndataVersion\nstring\nNo. If not included, it's stamped with an empty value.\nThe schema version of the data object. The publisher defines the schema version.\nmetadataVersion\nstring\nNo. If not included, Event Grid stamps onto the event. If included, must match the Event Grid Schema\nmetadataVersion\nexactly (currently, only\n1\n).\nThe schema version of the event metadata. Event Grid defines the schema of the top-level properties. Event Grid provides this value.\nFor custom topics, the event publisher determines the data object. The top-level data should have the same fields as standard resource-defined events.\nWhen publishing events to custom topics, create subjects for your events that make it easy for subscribers to know whether they're interested in the event. Subscribers use the subject to filter and route events. Consider providing the path for where the event happened, so subscribers can filter by segments of that path. The path enables subscribers to narrowly or broadly filter events. For example, if you provide a three segment path like\n/A/B/C\nin the subject, subscribers can filter by the first segment\n/A\nto get a broad set of events. Those subscribers get events with subjects like\n/A/B/C\nor\n/A/D/E\n. Other subscribers can filter by\n/A/B\nto get a narrower set of events.\nSometimes your subject needs more detail about what happened. For example, the\nStorage Accounts\npublisher provides the subject\n/blobServices/default/containers/<container-name>/blobs/<file>\nwhen a file is added to a container. A subscriber could filter by the path\n/blobServices/default/containers/testcontainer\nto get all events for that container but not other containers in the storage account. A subscriber could also filter or route by the suffix\n.txt\nto only work with text files.\nCloud events schema\nIn addition to its default event schema, Azure Event Grid natively supports events in the JSON implementation of CloudEvents v1.0 and HTTP protocol binding. CloudEvents is an open specification for describing event data.\nCloudEvents simplifies interoperability by providing a common event schema for publishing, and consuming cloud based events. This schema allows for uniform tooling, standard ways of routing & handling events, and universal ways of deserializing the outer event schema. With a common schema, you can more easily integrate work across platforms.\nHere's an example of an Azure Blob Storage event in CloudEvents format:\n{\n    \"specversion\": \"1.0\",\n    \"type\": \"Microsoft.Storage.BlobCreated\",  \n    \"source\": \"/subscriptions/{subscription-id}/resourceGroups/{resource-group}/providers/Microsoft.Storage/storageAccounts/{storage-account}\",\n    \"id\": \"9aeb0fdf-c01e-0131-0922-9eb54906e209\",\n    \"time\": \"2019-11-18T15:13:39.4589254Z\",\n    \"subject\": \"blobServices/default/containers/{storage-container}/blobs/{new-file}\",\n    \"dataschema\": \"#\",\n    \"data\": {\n        \"api\": \"PutBlockList\",\n        \"clientRequestId\": \"4c5dd7fb-2c48-4a27-bb30-5361b5de920a\",\n        \"requestId\": \"9aeb0fdf-c01e-0131-0922-9eb549000000\",\n        \"eTag\": \"0x8D76C39E4407333\",\n        \"contentType\": \"image/png\",\n        \"contentLength\": 30699,\n        \"blobType\": \"BlockBlob\",\n        \"url\": \"https://gridtesting.blob.core.windows.net/testcontainer/{new-file}\",\n        \"sequencer\": \"000000000000000000000000000099240000000000c41c18\",\n        \"storageDiagnostics\": {\n            \"batchId\": \"681fe319-3006-00a8-0022-9e7cde000000\"\n        }\n    }\n}\nA detailed description of the available fields, their types, and definitions in CloudEvents v1.0 is\navailable here\n.\nThe headers values for events delivered in the CloudEvents schema and the Event Grid schema are the same except for\ncontent-type\n. For CloudEvents schema, that header value is\n\"content-type\":\"application/cloudevents+json; charset=utf-8\"\n. For Event Grid schema, that header value is\n\"content-type\":\"application/json; charset=utf-8\"\n.\nYou can use Event Grid for both input and output of events in CloudEvents schema. You can use CloudEvents for system events, like Blob Storage events and IoT Hub events, and custom events. It can also transform those events on the wire back and forth.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Discover event schemas"
                },
                {
                  "level": 2,
                  "text": "Event schema"
                },
                {
                  "level": 2,
                  "text": "Event properties"
                },
                {
                  "level": 2,
                  "text": "Cloud events schema"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [
                "413 Payload Too Large",
                "[\n  {\n    \"topic\": string,\n    \"subject\": string,\n    \"id\": string,\n    \"eventType\": string,\n    \"eventTime\": string,\n    \"data\":{\n      object-unique-to-each-publisher\n    },\n    \"dataVersion\": string,\n    \"metadataVersion\": string\n  }\n]",
                "[\n  {\n    \"topic\": string,\n    \"subject\": string,\n    \"id\": string,\n    \"eventType\": string,\n    \"eventTime\": string,\n    \"data\":{\n      object-unique-to-each-publisher\n    },\n    \"dataVersion\": string,\n    \"metadataVersion\": string\n  }\n]",
                "metadataVersion",
                "/blobServices/default/containers/<container-name>/blobs/<file>",
                "/blobServices/default/containers/testcontainer",
                "{\n    \"specversion\": \"1.0\",\n    \"type\": \"Microsoft.Storage.BlobCreated\",  \n    \"source\": \"/subscriptions/{subscription-id}/resourceGroups/{resource-group}/providers/Microsoft.Storage/storageAccounts/{storage-account}\",\n    \"id\": \"9aeb0fdf-c01e-0131-0922-9eb54906e209\",\n    \"time\": \"2019-11-18T15:13:39.4589254Z\",\n    \"subject\": \"blobServices/default/containers/{storage-container}/blobs/{new-file}\",\n    \"dataschema\": \"#\",\n    \"data\": {\n        \"api\": \"PutBlockList\",\n        \"clientRequestId\": \"4c5dd7fb-2c48-4a27-bb30-5361b5de920a\",\n        \"requestId\": \"9aeb0fdf-c01e-0131-0922-9eb549000000\",\n        \"eTag\": \"0x8D76C39E4407333\",\n        \"contentType\": \"image/png\",\n        \"contentLength\": 30699,\n        \"blobType\": \"BlockBlob\",\n        \"url\": \"https://gridtesting.blob.core.windows.net/testcontainer/{new-file}\",\n        \"sequencer\": \"000000000000000000000000000099240000000000c41c18\",\n        \"storageDiagnostics\": {\n            \"batchId\": \"681fe319-3006-00a8-0022-9e7cde000000\"\n        }\n    }\n}",
                "{\n    \"specversion\": \"1.0\",\n    \"type\": \"Microsoft.Storage.BlobCreated\",  \n    \"source\": \"/subscriptions/{subscription-id}/resourceGroups/{resource-group}/providers/Microsoft.Storage/storageAccounts/{storage-account}\",\n    \"id\": \"9aeb0fdf-c01e-0131-0922-9eb54906e209\",\n    \"time\": \"2019-11-18T15:13:39.4589254Z\",\n    \"subject\": \"blobServices/default/containers/{storage-container}/blobs/{new-file}\",\n    \"dataschema\": \"#\",\n    \"data\": {\n        \"api\": \"PutBlockList\",\n        \"clientRequestId\": \"4c5dd7fb-2c48-4a27-bb30-5361b5de920a\",\n        \"requestId\": \"9aeb0fdf-c01e-0131-0922-9eb549000000\",\n        \"eTag\": \"0x8D76C39E4407333\",\n        \"contentType\": \"image/png\",\n        \"contentLength\": 30699,\n        \"blobType\": \"BlockBlob\",\n        \"url\": \"https://gridtesting.blob.core.windows.net/testcontainer/{new-file}\",\n        \"sequencer\": \"000000000000000000000000000099240000000000c41c18\",\n        \"storageDiagnostics\": {\n            \"batchId\": \"681fe319-3006-00a8-0022-9e7cde000000\"\n        }\n    }\n}",
                "content-type",
                "\"content-type\":\"application/cloudevents+json; charset=utf-8\"",
                "\"content-type\":\"application/json; charset=utf-8\""
              ],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                },
                {
                  "url": "https://github.com/Azure/azure-rest-api-specs/tree/master/specification/eventgrid/data-plane",
                  "text": "Event Schema store"
                },
                {
                  "url": "https://github.com/cloudevents/spec/blob/v1.0/spec.md#required-attributes",
                  "text": "available here"
                }
              ]
            },
            {
              "number": 4,
              "title": "Explore event delivery durability",
              "url": "https://learn.microsoft.com/en-us/training/modules/azure-event-grid/4-event-grid-delivery-retry",
              "href": "4-event-grid-delivery-retry",
              "content": "Read in English\nAdd\nAdd to plan\nExplore event delivery durability\nCompleted\n3 minutes\nEvent Grid provides durable delivery. It tries to deliver each event at least once for each matching subscription immediately. If a subscriber's endpoint doesn't acknowledge receipt of an event or if there's a failure, Event Grid retries delivery based on a fixed retry schedule and retry policy. By default, Event Grid delivers one event at a time to the subscriber, and the payload is an array with a single event.\nNote\nEvent Grid doesn't guarantee order for event delivery, so subscribers might receive them out of order.\nRetry schedule\nWhen Event Grid receives an error for an event delivery attempt, Event Grid decides whether it should: retry the delivery, dead-letter the event, or drop the event based on the type of the error.\nIf the error returned by the subscribed endpoint is a configuration-related error that can't be fixed with retries, Event Grid will either: perform dead-lettering on the event, or drop the event if dead-letter isn't configured.\nThe following table describes the types of endpoints and errors for which retry doesn't happen:\nEndpoint Type\nError codes\nAzure Resources\n400 (Bad request), 413 (Request entity is too large)\nWebhook\n400 (Bad request), 413 (Request entity is too large), 401 (Unauthorized)\nImportant\nIf Dead-Letter isn't configured for an endpoint, events are dropped when the above errors happen. Consider configuring Dead-Letter if you don't want these kinds of events to be dropped.\nIf the error returned by the subscribed endpoint isn't among the previous list, Event Grid waits 30 seconds for a response after delivering a message. After 30 seconds, if the endpoint fails to respond, the message is queued for retry. Event Grid uses an exponential backoff retry policy for event delivery.\nIf the endpoint responds within 3 minutes, Event Grid attempts to remove the event from the retry queue on a best effort basis but duplicates might still be received. Event Grid adds a small randomization to all retry steps and might opportunistically skip certain retries if an endpoint is consistently unhealthy, down for a long period, or appears to be overwhelmed.\nRetry policy\nYou can customize the retry policy when creating an event subscription by using the following two configurations. An event is dropped if either of the limits of the retry policy is reached.\nMaximum number of attempts\n- The value must be an integer between 1 and 30. The default value is 30.\nEvent time-to-live (TTL)\n-  The value must be an integer between 1 and 1440. The default value is 1440 minutes\nThe following example shows setting the maximum number of attempts to 18 by using the Azure CLI.\naz eventgrid event-subscription create \\\n  -g gridResourceGroup \\\n  --topic-name <topic_name> \\\n  --name <event_subscription_name> \\\n  --endpoint <endpoint_URL> \\\n  --max-delivery-attempts 18\nOutput batching\nYou can configure Event Grid to batch events for delivery for improved HTTP performance in high-throughput scenarios. Batching is turned off by default and can be turned on by subscription via the portal, CLI, PowerShell, or SDKs.\nBatched delivery has two settings:\nMax events per batch\n- Maximum number of events Event Grid delivers per batch. This number won't be exceeded, however fewer events might be delivered if no other events are available at the time of publish. Event Grid doesn't delay events to create a batch if fewer events are available. Must be between 1 and 5,000.\nPreferred batch size in kilobytes\n- Target ceiling for batch size in kilobytes. Similar to max events, the batch size might be smaller if more events aren't available at the time of publish. It's possible that a batch is larger than the preferred batch size\nif\na single event is larger than the preferred size. For example, if the preferred size is 4 KB and a 10-KB event is pushed to Event Grid, the 10-KB event is delivered in its own batch rather than being dropped.\nDelayed delivery\nAs an endpoint experiences delivery failures, Event Grid begins to delay the delivery and retry of events to that endpoint. For example, if the first 10 events published to an endpoint fail, Event Grid assumes that the endpoint is experiencing issues and delays all subsequent retries, and new deliveries, for some time - in some cases up to several hours.\nThe functional purpose of delayed delivery is to protect unhealthy endpoints and the Event Grid system. Without back-off and delay of delivery to unhealthy endpoints, Event Grid's retry policy and volume capabilities can easily overwhelm a system.\nDead-letter events\nWhen Event Grid can't deliver an event within a certain time period or after trying to deliver the event a specific number of times, it can send the undelivered event to a storage account. This process is known as\ndead-lettering\n. Event Grid dead-letters an event when\none of the following\nconditions is met.\nEvent isn't delivered within the\ntime-to-live\nperiod.\nThe\nnumber of tries\nto deliver the event exceeds the limit.\nIf either of the conditions is met, the event is dropped or dead-lettered. By default, Event Grid doesn't turn on dead-lettering. To enable it, you must specify a storage account to hold undelivered events when creating the event subscription. You pull events from this storage account to resolve deliveries.\nIf Event Grid receives a 400 (Bad Request) or 413 (Request Entity Too Large) response code, it immediately schedules the event for dead-lettering. These response codes indicate delivery of the event failed.\nThere's a five-minute delay between the last attempt to deliver an event and delivery to the dead-letter location. This delay is intended to reduce the number of Blob storage operations. If the dead-letter location is unavailable for four hours, the event is dropped.\nCustom delivery properties\nEvent subscriptions allow you to set up HTTP headers that are included in delivered events. This capability allows you to set custom headers that are required by a destination. You can set up to 10 headers when creating an event subscription. Each header value shouldn't be greater than 4,096 bytes. You can set custom headers on the events that are delivered to the following destinations:\nWebhooks\nAzure Service Bus topics and queues\nAzure Event Hubs\nRelay Hybrid Connections\nBefore setting the dead-letter location, you must have a storage account with a container. You provide the endpoint for this container when creating the event subscription.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Explore event delivery durability"
                },
                {
                  "level": 2,
                  "text": "Retry schedule"
                },
                {
                  "level": 2,
                  "text": "Retry policy"
                },
                {
                  "level": 2,
                  "text": "Output batching"
                },
                {
                  "level": 2,
                  "text": "Delayed delivery"
                },
                {
                  "level": 2,
                  "text": "Dead-letter events"
                },
                {
                  "level": 2,
                  "text": "Custom delivery properties"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [
                "az eventgrid event-subscription create \\\n  -g gridResourceGroup \\\n  --topic-name <topic_name> \\\n  --name <event_subscription_name> \\\n  --endpoint <endpoint_URL> \\\n  --max-delivery-attempts 18",
                "az eventgrid event-subscription create \\\n  -g gridResourceGroup \\\n  --topic-name <topic_name> \\\n  --name <event_subscription_name> \\\n  --endpoint <endpoint_URL> \\\n  --max-delivery-attempts 18"
              ],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 5,
              "title": "Control access to events",
              "url": "https://learn.microsoft.com/en-us/training/modules/azure-event-grid/5-authorize-access-event-grid",
              "href": "5-authorize-access-event-grid",
              "content": "Read in English\nAdd\nAdd to plan\nControl access to events\nCompleted\n3 minutes\nAzure Event Grid allows you to control the level of access given to different users to do various management operations such as list event subscriptions, create new ones, and generate keys. Event Grid uses Azure role-based access control (Azure RBAC).\nBuilt-in roles\nEvent Grid provides the following built-in roles:\nRole\nDescription\nEvent Grid Subscription Reader\nLets you read Event Grid event subscriptions.\nEvent Grid Subscription Contributor\nLets you manage Event Grid event subscription operations.\nEvent Grid Contributor\nLets you create and manage Event Grid resources.\nEvent Grid Data Sender\nLets you send events to Event Grid topics.\nThe Event Grid Subscription Reader and Event Grid Subscription Contributor roles are for managing event subscriptions. They're important when implementing event domains because they give users the permissions they need to subscribe to topics in your event domain. These roles are focused on event subscriptions and don't grant access for actions such as creating topics.\nThe Event Grid Contributor role allows you to create and manage Event Grid resources.\nPermissions for event subscriptions\nIf you're using an event handler that isn't a WebHook (such as an event hub or queue storage), you must have the\nMicrosoft.EventGrid/EventSubscriptions/Write\npermission on the resource that is the event source. You need this permission because you're writing a new subscription at the scope of the resource. This permissions check prevents an unauthorized user from sending events to your resource.\nThe required resource differs based on whether you're subscribing to a system topic or custom topic. Both types are described in this section.\nTopic Type\nDescription\nSystem topics\nNeed permission to write a new event subscription at the scope of the resource publishing the event. The format of the resource is:\n/subscriptions/{subscription-id}/resourceGroups/{resource-group-name}/providers/{resource-provider}/{resource-type}/{resource-name}\nCustom topics\nNeed permission to write a new event subscription at the scope of the Event Grid topic. The format of the resource is:\n/subscriptions/{subscription-id}/resourceGroups/{resource-group-name}/providers/Microsoft.EventGrid/topics/{topic-name}\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Control access to events"
                },
                {
                  "level": 2,
                  "text": "Built-in roles"
                },
                {
                  "level": 2,
                  "text": "Permissions for event subscriptions"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [
                "/subscriptions/{subscription-id}/resourceGroups/{resource-group-name}/providers/{resource-provider}/{resource-type}/{resource-name}",
                "/subscriptions/{subscription-id}/resourceGroups/{resource-group-name}/providers/Microsoft.EventGrid/topics/{topic-name}"
              ],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 6,
              "title": "Receive events by using webhooks",
              "url": "https://learn.microsoft.com/en-us/training/modules/azure-event-grid/6-webhook-event-delivery",
              "href": "6-webhook-event-delivery",
              "content": "Read in English\nAdd\nAdd to plan\nReceive events by using webhooks\nCompleted\n3 minutes\nWebhooks are one of the many ways to receive events from Azure Event Grid. When a new event is ready, Event Grid service POSTs an HTTP request to the configured endpoint with the event in the request body.\nLike many other services that support webhooks, Event Grid requires you to prove ownership of your Webhook endpoint before it starts delivering events to that endpoint. This requirement prevents a malicious user from flooding your endpoint with events.\nWhen you use any of the following three Azure services, the Azure infrastructure automatically handles this validation:\nAzure Logic Apps with Event Grid Connector\nAzure Automation via webhook\nAzure Functions with Event Grid Trigger\nEndpoint validation with Event Grid events\nIf you're using any other type of endpoint, such as an HTTP trigger based Azure function, your endpoint code needs to participate in a validation handshake with Event Grid. Event Grid supports two ways of validating the subscription.\nSynchronous handshake\n: At the time of event subscription creation, Event Grid sends a subscription validation event to your endpoint. The schema of this event is similar to any other Event Grid event. The data portion of this event includes a\nvalidationCode\nproperty. Your application verifies that the validation request is for an expected event subscription, and returns the validation code in the response synchronously. This handshake mechanism is supported in all Event Grid versions.\nAsynchronous handshake\n: In certain cases, you can't return the ValidationCode in response synchronously. For example, if you use a third-party service (like\nZapier\nor\nIFTTT\n), you can't programmatically respond with the validation code.\nStarting with version 2018-05-01-preview, Event Grid supports a manual validation handshake. If you're creating an event subscription with an SDK or tool that uses API version 2018-05-01-preview or later, Event Grid sends a\nvalidationUrl\nproperty in the data portion of the subscription validation event. To complete the handshake, find that URL in the event data and do a GET request to it. You can use either a REST client or your web browser.\nThe provided URL is valid for\n5 minutes\n. During that time, the provisioning state of the event subscription is\nAwaitingManualAction\n. If you don't complete the manual validation within 5 minutes, the provisioning state is set to\nFailed\n. You have to create the event subscription again before starting the manual validation.\nThis authentication mechanism also requires the webhook endpoint to return an HTTP status code of 200 so that it knows that the POST for the validation event was accepted before it can be put in the manual validation mode. In other words, if the endpoint returns 200 but doesn't return back a validation response synchronously, the mode is transitioned to the manual validation mode. If there's a GET on the validation URL within 5 minutes, the validation handshake is considered to be successful.\nNote\nUsing self-signed certificates for validation isn't supported. Use a signed certificate from a commercial certificate authority (CA) instead.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Receive events by using webhooks"
                },
                {
                  "level": 2,
                  "text": "Endpoint validation with Event Grid events"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [
                "validationCode",
                "validationUrl",
                "AwaitingManualAction"
              ],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                },
                {
                  "url": "https://zapier.com/",
                  "text": "Zapier"
                },
                {
                  "url": "https://ifttt.com/",
                  "text": "IFTTT"
                }
              ]
            },
            {
              "number": 7,
              "title": "Filter events",
              "url": "https://learn.microsoft.com/en-us/training/modules/azure-event-grid/7-event-grid-filtering",
              "href": "7-event-grid-filtering",
              "content": "Read in English\nAdd\nAdd to plan\nFilter events\nCompleted\n3 minutes\nWhen creating an event subscription, you have three options for filtering:\nEvent types\nSubject begins with or ends with\nAdvanced fields and operators\nEvent type filtering\nBy default, all event types for the event source are sent to the endpoint. You can decide to send only certain event types to your endpoint. For example, you can get notified of updates to your resources, but not notified for other operations like deletions. In that case, filter by the\nMicrosoft.Resources.ResourceWriteSuccess\nevent type. Provide an array with the event types, or specify\nAll\nto get all event types for the event source.\nThe JSON syntax for filtering by event type is:\n\"filter\": {\n  \"includedEventTypes\": [\n    \"Microsoft.Resources.ResourceWriteFailure\",\n    \"Microsoft.Resources.ResourceWriteSuccess\"\n  ]\n}\nSubject filtering\nFor simple filtering by subject, specify a starting or ending value for the subject. For example, you can specify the subject ends with\n.txt\nto only get events related to uploading a text file to storage account. Or, you can filter the subject begins with\n/blobServices/default/containers/testcontainer\nto get all events for that container but not other containers in the storage account.\nThe JSON syntax for filtering by subject is:\n\"filter\": {\n  \"subjectBeginsWith\": \"/blobServices/default/containers/mycontainer/log\",\n  \"subjectEndsWith\": \".jpg\"\n}\nAdvanced filtering\nTo filter by values in the data fields and specify the comparison operator, use the advanced filtering option. In advanced filtering, you specify the:\noperator type - The type of comparison.\nkey - The field in the event data that you're using for filtering. It can be a number, boolean, or string.\nvalue or values - The value or values to compare to the key.\nThe JSON syntax for using advanced filters is:\n\"filter\": {\n  \"advancedFilters\": [\n    {\n      \"operatorType\": \"NumberGreaterThanOrEquals\",\n      \"key\": \"Data.Key1\",\n      \"value\": 5\n    },\n    {\n      \"operatorType\": \"StringContains\",\n      \"key\": \"Subject\",\n      \"values\": [\"container1\", \"container2\"]\n    }\n  ]\n}\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Filter events"
                },
                {
                  "level": 2,
                  "text": "Event type filtering"
                },
                {
                  "level": 2,
                  "text": "Subject filtering"
                },
                {
                  "level": 2,
                  "text": "Advanced filtering"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [
                "Microsoft.Resources.ResourceWriteSuccess",
                "\"filter\": {\n  \"includedEventTypes\": [\n    \"Microsoft.Resources.ResourceWriteFailure\",\n    \"Microsoft.Resources.ResourceWriteSuccess\"\n  ]\n}",
                "\"filter\": {\n  \"includedEventTypes\": [\n    \"Microsoft.Resources.ResourceWriteFailure\",\n    \"Microsoft.Resources.ResourceWriteSuccess\"\n  ]\n}",
                "/blobServices/default/containers/testcontainer",
                "\"filter\": {\n  \"subjectBeginsWith\": \"/blobServices/default/containers/mycontainer/log\",\n  \"subjectEndsWith\": \".jpg\"\n}",
                "\"filter\": {\n  \"subjectBeginsWith\": \"/blobServices/default/containers/mycontainer/log\",\n  \"subjectEndsWith\": \".jpg\"\n}",
                "\"filter\": {\n  \"advancedFilters\": [\n    {\n      \"operatorType\": \"NumberGreaterThanOrEquals\",\n      \"key\": \"Data.Key1\",\n      \"value\": 5\n    },\n    {\n      \"operatorType\": \"StringContains\",\n      \"key\": \"Subject\",\n      \"values\": [\"container1\", \"container2\"]\n    }\n  ]\n}",
                "\"filter\": {\n  \"advancedFilters\": [\n    {\n      \"operatorType\": \"NumberGreaterThanOrEquals\",\n      \"key\": \"Data.Key1\",\n      \"value\": 5\n    },\n    {\n      \"operatorType\": \"StringContains\",\n      \"key\": \"Subject\",\n      \"values\": [\"container1\", \"container2\"]\n    }\n  ]\n}"
              ],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 8,
              "title": "Exercise - Route events to a custom endpoint with Event Grid",
              "url": "https://learn.microsoft.com/en-us/training/modules/azure-event-grid/8-event-grid-custom-events",
              "href": "8-event-grid-custom-events",
              "content": "Read in English\nAdd\nAdd to plan\nExercise - Route events to a custom endpoint with Event Grid\nCompleted\n30 minutes\nIn this exercise, you configure Azure Event Grid to send events to an endpoint. You learn how to create Event Grid resources, set up event subscriptions, and verify that your endpoint receives events from Azure services.\nTasks performed in this exercise:\nCreate Azure Event Grid resources\nEnable an Event Grid resource provider\nCreate a topic in Event Grid\nCreate a message endpoint\nSubscribe to the topic\nSend an event with a .NET console app\nClean up resources\nThis exercise takes approximately\n30\nminutes to complete.\nBefore you start\nTo complete the exercise you need:\nAn Azure subscription. If you don't already have one, you can sign up for one\nhttps://azure.microsoft.com/\n.\nGet started\nSelect the\nLaunch Exercise\nbutton to open the exercise instructions in a new browser window. When you're finished with the exercise, return here to:\nComplete the module\nEarn a badge for completing this module\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Exercise - Route events to a custom endpoint with Event Grid"
                },
                {
                  "level": 2,
                  "text": "Before you start"
                },
                {
                  "level": 2,
                  "text": "Get started"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [
                {
                  "src": "../../wwl-azure/azure-event-grid/media/launch-exercise.png",
                  "absolute_url": "https://learn.microsoft.com/en-us/wwl-azure/azure-event-grid/media/launch-exercise.png",
                  "alt_text": "Button to launch exercise.",
                  "title": "",
                  "filename": "launch-exercise.png",
                  "image_type": "icon",
                  "context": {
                    "preceding_heading": "Get started",
                    "following_text": "Was this page helpful?",
                    "figure_caption": "",
                    "parent_section": ""
                  }
                }
              ],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                },
                {
                  "url": "https://azure.microsoft.com/",
                  "text": "https://azure.microsoft.com/"
                }
              ]
            },
            {
              "number": 9,
              "title": "Module assessment",
              "url": "https://learn.microsoft.com/en-us/training/modules/azure-event-grid/9-knowledge-check",
              "href": "9-knowledge-check",
              "content": "Read in English\nAdd\nAdd to plan\nModule assessment\nCompleted\n3 minutes\n1.\nWhich of the following event schema properties requires a value?\nTopic\nData\nSubject\n2.\nWhich of the following Event Grid built-in roles is appropriate for managing Event Grid resources?\nEvent Grid Contributor\nEvent Grid Subscription Contributor\nEvent Grid Data Sender\n3.\nWhat is the purpose of the CloudEvents schema in Azure Event Grid?\nTo provide a proprietary event schema specific to Azure services.\nTo simplify interoperability by providing a common event schema for publishing and consuming cloud-based events.\nTo replace the Event Grid event schema entirely for all event types.\n4.\nWhat happens when Event Grid receives a 400 (Bad Request) or 413 (Request Entity Too Large) response code during event delivery?\nEvent Grid retries the delivery indefinitely until the endpoint responds.\nEvent Grid schedules the event for dead-lettering if a dead-letter location is configured.\nEvent Grid immediately drops the event without further action.\n5.\nWhat is the purpose of the validation handshake in Azure Event Grid when using a custom webhook endpoint?\nTo prove ownership of the webhook endpoint before delivering events\nTo ensure the webhook endpoint is hosted on Azure infrastructure\nTo encrypt the event data being sent to the webhook endpoint\nYou must answer all questions before checking your work.\nYou must answer all questions before checking your work.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Module assessment"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 10,
              "title": "Summary",
              "url": "https://learn.microsoft.com/en-us/training/modules/azure-event-grid/10-summary",
              "href": "10-summary",
              "content": "Read in English\nAdd\nAdd to plan\nSummary\nCompleted\n3 minutes\nIn this module, you learned how to:\nDescribe how Event Grid operates and how it connects to services and event handlers.\nExplain how Event Grid delivers events and how it handles errors.\nImplement authentication and authorization.\nRoute events to a custom endpoint.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Summary"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            }
          ]
        },
        {
          "title": "Explore Azure Event Hubs",
          "url": "https://learn.microsoft.com/en-us/training/modules/azure-event-hubs/",
          "description": "",
          "learning_objectives": [],
          "prerequisites": [],
          "units": [
            {
              "number": 1,
              "title": "Introduction",
              "url": "https://learn.microsoft.com/en-us/training/modules/azure-event-hubs/1-introduction",
              "href": "1-introduction",
              "content": "Read in English\nAdd\nAdd to plan\nIntroduction\nCompleted\n3 minutes\nAzure Event Hubs is a big data streaming platform and event ingestion service. It can receive and process millions of events per second. Data sent to an event hub can be transformed and stored by using any real-time analytics provider or batching/storage adapters.\nAfter completing this module, you'll be able to:\nDescribe the benefits of using Event Hubs and how it captures streaming data.\nExplain how to process events.\nPerform common operations with the Event Hubs client library.\nSend and retrieve events from Azure Event Hubs\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Introduction"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 2,
              "title": "Discover Azure Event Hubs",
              "url": "https://learn.microsoft.com/en-us/training/modules/azure-event-hubs/2-event-hubs-overview",
              "href": "2-event-hubs-overview",
              "content": "Read in English\nAdd\nAdd to plan\nDiscover Azure Event Hubs\nCompleted\n3 minutes\nAzure Event Hubs is a native data-streaming service in the cloud that can stream millions of events per second, with low latency, from any source to any destination. Event Hubs is compatible with Apache Kafka. It enables you to run existing Kafka workloads without any code changes.\nWith Event Hubs, you can ingest, buffer, store, and process your stream in real time to get actionable insights. Event Hubs uses a partitioned consumer model. It enables multiple applications to process the stream concurrently and lets you control the speed of processing. Event Hubs also integrates with Azure Functions for serverless architectures.\nA broad ecosystem is available for the industry-standard AMQP 1.0 protocol. SDKs are available in languages like .NET, Java, Python, and JavaScript, so you can start processing your streams from Event Hubs. All supported client languages provide low-level integration.\nKey capabilities\nLearn about the key capabilities of Azure Event Hubs in the following sections.\nApache Kafka on Azure Event Hubs\nEvent Hubs is a multi-protocol event streaming engine that natively supports Advanced Message Queuing Protocol (AMQP), Apache Kafka, and HTTPS protocols. Because it supports Apache Kafka, you can bring Kafka workloads to Event Hubs without making any code changes. You don't need to set up, configure, or manage your own Kafka clusters or use a Kafka-as-a-service offering that's not native to Azure.\nSchema Registry in Event Hubs\nAzure Schema Registry in Event Hubs provides a centralized repository for managing schemas of event streaming applications. Schema Registry comes free with every Event Hubs namespace. It integrates with your Kafka applications or Event Hubs SDK-based applications.\nReal-time processing of streaming events with Stream Analytics\nEvent Hubs integrates with Azure Stream Analytics to enable real-time stream processing. With the built-in no-code editor, you can develop a Stream Analytics job by using drag-and-drop functionality, without writing any code.\nAlternatively, developers can use the SQL-based Stream Analytics query language to perform real-time stream processing and take advantage of a wide range of functions for analyzing streaming data.\nKey concepts\nEvent Hubs contains the following key components:\nProducer applications\n: These applications can ingest data to an event hub by using Event Hubs SDKs or any Kafka producer client.\nNamespace\n: The management container for one or more event hubs or Kafka topics. The management tasks such as allocating streaming capacity, configuring network security, and enabling geo-disaster recovery are handled at the namespace level.\nEvent Hubs/Kafka topic\n: In Event Hubs, you can organize events into an event hub or a Kafka topic. It's an append-only distributed log, which can comprise one or more partitions.\nPartitions\n: They're used to scale an event hub. They're like lanes in a freeway. If you need more streaming throughput, you can add more partitions.\nConsumer applications\n: These applications can consume data by seeking through the event log and maintaining consumer offset. Consumers can be Kafka consumer clients or Event Hubs SDK clients.\nConsumer group\n: This logical group of consumer instances reads data from an event hub or Kafka topic. It enables multiple consumers to read the same streaming data in an event hub independently at their own pace and with their own offsets.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Discover Azure Event Hubs"
                },
                {
                  "level": 2,
                  "text": "Key capabilities"
                },
                {
                  "level": 3,
                  "text": "Apache Kafka on Azure Event Hubs"
                },
                {
                  "level": 3,
                  "text": "Schema Registry in Event Hubs"
                },
                {
                  "level": 3,
                  "text": "Real-time processing of streaming events with Stream Analytics"
                },
                {
                  "level": 2,
                  "text": "Key concepts"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 3,
              "title": "Explore Event Hubs Capture",
              "url": "https://learn.microsoft.com/en-us/training/modules/azure-event-hubs/3-event-hubs-capture",
              "href": "3-event-hubs-capture",
              "content": "Read in English\nAdd\nAdd to plan\nExplore Event Hubs Capture\nCompleted\n3 minutes\nAzure Event Hubs enables you to automatically capture the streaming data in Event Hubs in an Azure Blob storage or Azure Data Lake Storage account of your choice, with the added flexibility of specifying a time or size interval. Setting up Capture is fast, there are no administrative costs to run it, and it scales automatically with Event Hubs throughput units in the standard tier or processing units in the premium tier.\nEvent Hubs Capture enables you to process real-time and batch-based pipelines on the same stream. This means you can build solutions that grow with your needs over time.\nHow Event Hubs Capture works\nEvent Hubs is a time-retention durable buffer for telemetry ingress, similar to a distributed log. The key to scaling in Event Hubs is the partitioned consumer model. Each partition is an independent segment of data and is consumed independently. Over time this data ages off, based on the configurable retention period. As a result, a given event hub never gets \"too full.\"\nEvent Hubs Capture enables you to specify your own Azure Blob storage account and container, or Azure Data Lake Store account, which are used to store the captured data. These accounts can be in the same region as your event hub or in another region, adding to the flexibility of the Event Hubs Capture feature.\nCaptured data is written in Apache Avro format: a compact, fast, binary format that provides rich data structures with inline schema. This format is widely used in the Hadoop ecosystem, Stream Analytics, and Azure Data Factory. More information about working with Avro is available later in this article.\nCapture windowing\nEvent Hubs Capture enables you to set up a window to control capturing. This window is a minimum size and time configuration with a \"first wins policy,\" meaning that the first trigger encountered causes a capture operation. Each partition captures independently and writes a completed block blob at the time of capture, named for the time at which the capture interval was encountered. The storage naming convention is as follows:\n{Namespace}/{EventHub}/{PartitionId}/{Year}/{Month}/{Day}/{Hour}/{Minute}/{Second}\nNote the date values are padded with zeroes; an example filename might be:\nhttps://mystorageaccount.blob.core.windows.net/mycontainer/mynamespace/myeventhub/0/2017/12/08/03/03/17.avro\nScaling to throughput units\nEvent Hubs traffic is controlled by throughput units. A single throughput unit allows 1 MB per second or 1,000 events per second of ingress and twice that amount of egress. Standard Event Hubs can be configured with 1-20 throughput units, and you can purchase more with a quota increase support request. Usage beyond your purchased throughput units is throttled. Event Hubs Capture copies data directly from the internal Event Hubs storage, bypassing throughput unit egress quotas and saving your egress for other processing readers, such as Stream Analytics or Spark.\nOnce configured, Event Hubs Capture runs automatically when you send your first event, and continues running. To make it easier for your downstream processing to know that the process is working, Event Hubs writes empty files when there's no data. This process provides a predictable cadence and marker that can feed your batch processors.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Explore Event Hubs Capture"
                },
                {
                  "level": 2,
                  "text": "How Event Hubs Capture works"
                },
                {
                  "level": 2,
                  "text": "Capture windowing"
                },
                {
                  "level": 2,
                  "text": "Scaling to throughput units"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [
                "{Namespace}/{EventHub}/{PartitionId}/{Year}/{Month}/{Day}/{Hour}/{Minute}/{Second}",
                "{Namespace}/{EventHub}/{PartitionId}/{Year}/{Month}/{Day}/{Hour}/{Minute}/{Second}",
                "https://mystorageaccount.blob.core.windows.net/mycontainer/mynamespace/myeventhub/0/2017/12/08/03/03/17.avro",
                "https://mystorageaccount.blob.core.windows.net/mycontainer/mynamespace/myeventhub/0/2017/12/08/03/03/17.avro"
              ],
              "images": [
                {
                  "src": "../../wwl-azure/azure-event-hubs/media/event-hubs-capture.png",
                  "absolute_url": "https://learn.microsoft.com/en-us/wwl-azure/azure-event-hubs/media/event-hubs-capture.png",
                  "alt_text": "Image showing capturing of Event Hubs data into Azure Storage or Azure Data Lake Storage",
                  "title": "",
                  "filename": "event-hubs-capture.png",
                  "image_type": "illustration",
                  "context": {
                    "preceding_heading": "Explore Event Hubs Capture",
                    "following_text": "Event Hubs Capture enables you to process real-time and batch-based pipelines on the same stream. This means you can build solutions that grow with your needs over time.",
                    "figure_caption": "",
                    "parent_section": ""
                  }
                }
              ],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 4,
              "title": "Scale your processing application",
              "url": "https://learn.microsoft.com/en-us/training/modules/azure-event-hubs/4-event-processing",
              "href": "4-event-processing",
              "content": "Read in English\nAdd\nAdd to plan\nScale your processing application\nCompleted\n5 minutes\nTo scale your event processing application, you can run multiple instances of the application and have it balance the load among themselves. In the older versions,\nEventProcessorHost\nallowed you to balance the load between multiple instances of your program and checkpoint events when receiving. In the newer versions (5.0 onwards),\nEventProcessorClient\n(.NET and Java), or\nEventHubConsumerClient\n(Python and JavaScript) allows you to do the same.\nNote\nThe key to scale for Event Hubs is the idea of partitioned consumers. In contrast to the competing consumers pattern, the partitioned consumer pattern enables high scale by removing the contention bottleneck and facilitating end to end parallelism.\nExample scenario\nAs an example scenario, consider a home security company that monitors 100,000 homes. Every minute, it gets data from various sensors such as a motion detector, door/window open sensor, glass break detector, and so on, installed in each home. The company provides a web site for residents to monitor the activity of their home in near real time.\nEach sensor pushes data to an event hub. The event hub is configured with 16 partitions. On the consuming end, you need a mechanism that can read these events, consolidate them, and dump the aggregate to a storage blob, which is then projected to a user-friendly web page.\nWhen designing the consumer in a distributed environment, the scenario must handle the following requirements:\nScale:\nCreate multiple consumers, with each consumer taking ownership of reading from a few Event Hubs partitions.\nLoad balance:\nIncrease or reduce the consumers dynamically. For example, when a new sensor type (for example, a carbon monoxide detector) is added to each home, the number of events increases. In that case, the operator (a human) increases the number of consumer instances. Then, the pool of consumers can rebalance the number of partitions they own, to share the load with the newly added consumers.\nSeamless resume on failures:\nIf a consumer (\nconsumer A\n) fails (for example, the virtual machine hosting the consumer suddenly crashes), then other consumers can pick up the partitions owned by\nconsumer A\nand continue. Also, the continuation point, called a\ncheckpoint\nor\noffset\n, should be at the exact point at which\nconsumer A\nfailed, or slightly before that.\nConsume events:\nWhile the previous three points deal with the management of the consumer, there must be code to consume the events and do something useful with it. For example, aggregate it and upload it to blob storage.\nEvent processor or consumer client\nYou don't need to build your own solution to meet these requirements. The Azure Event Hubs SDKs provide this functionality. In .NET or Java SDKs, you use an event processor client (\nEventProcessorClient\n), and in Python and JavaScript SDKs, you use\nEventHubConsumerClient\n.\nFor most production scenarios, we recommend that you use the event processor client for reading and processing events. Event processor clients can work cooperatively within the context of a consumer group for a given event hub. Clients will automatically manage distribution and balancing of work as instances become available or unavailable for the group.\nPartition ownership tracking\nAn event processor instance typically owns and processes events from one or more partitions. Ownership of partitions is evenly distributed among all the active event processor instances associated with an event hub and consumer group combination.\nEach event processor is given a unique identifier and claims ownership of partitions by adding or updating an entry in a checkpoint store. All event processor instances communicate with this store periodically to update its own processing state and to learn about other active instances. This data is then used to balance the load among the active processors.\nReceive messages\nWhen you create an event processor, you specify the functions that process events and errors. Each call to the function that processes events delivers a single event from a specific partition. It's your responsibility to handle this event. If you want to make sure the consumer processes every message at least once, you need to write your own code with retry logic. But be cautious about poisoned messages.\nWe recommend that you do things relatively fast. That is, do as little processing as possible. If you need to write to storage and do some routing, it's better to use two consumer groups and have two event processors.\nCheckpointing\nCheckpointing\nis a process by which an event processor marks or commits the position of the last successfully processed event within a partition. Marking a checkpoint is typically done within the function that processes the events and occurs on a per-partition basis within a consumer group.\nIf an event processor disconnects from a partition, another instance can resume processing the partition at the checkpoint that was previously committed by the last processor of that partition in that consumer group. When the processor connects, it passes the offset to the event hub to specify the location at which to start reading. In this way, you can use checkpointing to both mark events as \"complete\" by downstream applications and to provide resiliency when an event processor goes down. It's possible to return to older data by specifying a lower offset from this checkpointing process.\nThread safety and processor instances\nBy default, the function that processes the events is called sequentially for a given partition. Subsequent events and calls to this function from the same partition queue up behind the scenes as the event pump continues to run in the background on other threads. Events from different partitions can be processed concurrently and any shared state that is accessed across partitions have to be synchronized.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Scale your processing application"
                },
                {
                  "level": 2,
                  "text": "Example scenario"
                },
                {
                  "level": 2,
                  "text": "Event processor or consumer client"
                },
                {
                  "level": 2,
                  "text": "Partition ownership tracking"
                },
                {
                  "level": 2,
                  "text": "Receive messages"
                },
                {
                  "level": 2,
                  "text": "Checkpointing"
                },
                {
                  "level": 2,
                  "text": "Thread safety and processor instances"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [
                "EventProcessorClient",
                "EventHubConsumerClient"
              ],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 5,
              "title": "Control access to events",
              "url": "https://learn.microsoft.com/en-us/training/modules/azure-event-hubs/5-event-hubs-authentication-authorization",
              "href": "5-event-hubs-authentication-authorization",
              "content": "Read in English\nAdd\nAdd to plan\nControl access to events\nCompleted\n3 minutes\nAzure Event Hubs supports both Microsoft Entra ID and shared access signatures (SAS) to handle both authentication and authorization. Azure provides the following Azure built-in roles for authorizing access to Event Hubs data using Microsoft Entra ID and OAuth:\nAzure Event Hubs Data Owner\n: Use this role to give\ncomplete access\nto Event Hubs resources.\nAzure Event Hubs Data Sender\n: Use this role to give\nsend access\nto Event Hubs resources.\nAzure Event Hubs Data Receiver\n: Use this role to give\nreceiving access\nto Event Hubs resources.\nAuthorize access with managed identities\nTo authorize a request to Event Hubs service from a managed identity in your application, you need to configure Azure role-based access control settings for that managed identity. Azure Event Hubs defines Azure roles that encompass permissions for sending and reading from Event Hubs. When the Azure role is assigned to a managed identity, the managed identity is granted access to Event Hubs data at the appropriate scope.\nAuthorize access with Microsoft identity platform\nA key advantage of using Microsoft Entra ID with Event Hubs is that your credentials no longer need to be stored in your code. Instead, you can request an OAuth 2.0 access token from Microsoft identity platform. Microsoft Entra authenticates the security principal (a user, a group, or service principal) running the application. If authentication succeeds, Microsoft Entra ID returns the access token to the application, and the application can then use the access token to authorize requests to Azure Event Hubs.\nAuthorize access to Event Hubs publishers with shared access signatures\nAn event publisher defines a virtual endpoint for an Event Hubs. The publisher can only be used to send messages to an event hub and not receive messages. Typically, an event hub employs one publisher per client. All messages that are sent to any of the publishers of an event hub are enqueued within that event hub. Publishers enable fine-grained access control.\nEach Event Hubs client is assigned a unique token that is uploaded to the client. A client that holds a token can only send to one publisher, and no other publisher. If multiple clients share the same token, then each of them shares the publisher.\nAll tokens are assigned with shared access signature keys. Typically, all tokens are signed with the same key. Clients aren't aware of the key, which prevents clients from manufacturing tokens. Clients operate on the same tokens until they expire.\nAuthorize access to Event Hubs consumers with shared access signatures\nTo authenticate back-end applications that consume from the data generated by Event Hubs producers, Event Hubs token authentication requires its clients to either have the\nmanage\nrights or the\nlisten\nprivileges assigned to its Event Hubs namespace or event hub instance or topic. Data is consumed from Event Hubs using consumer groups. While SAS policy gives you granular scope, this scope is defined only at the entity level and not at the consumer level. It means that the privileges defined at the namespace level or the event hub instance or topic level are to the consumer groups of that entity.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Control access to events"
                },
                {
                  "level": 2,
                  "text": "Authorize access with managed identities"
                },
                {
                  "level": 2,
                  "text": "Authorize access with Microsoft identity platform"
                },
                {
                  "level": 2,
                  "text": "Authorize access to Event Hubs publishers with shared access signatures"
                },
                {
                  "level": 2,
                  "text": "Authorize access to Event Hubs consumers with shared access signatures"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                },
                {
                  "url": "/en-us/azure/role-based-access-control/built-in-roles#azure-event-hubs-data-owner",
                  "text": "Azure Event Hubs Data Owner"
                },
                {
                  "url": "/en-us/azure/role-based-access-control/built-in-roles#azure-event-hubs-data-sender",
                  "text": "Azure Event Hubs Data Sender"
                },
                {
                  "url": "/en-us/azure/role-based-access-control/built-in-roles#azure-event-hubs-data-receiver",
                  "text": "Azure Event Hubs Data Receiver"
                }
              ]
            },
            {
              "number": 6,
              "title": "Perform common operations with the Event Hubs client library",
              "url": "https://learn.microsoft.com/en-us/training/modules/azure-event-hubs/6-event-hubs-programming-guide",
              "href": "6-event-hubs-programming-guide",
              "content": "Read in English\nAdd\nAdd to plan\nPerform common operations with the Event Hubs client library\nCompleted\n5 minutes\nThis unit contains examples of common operations you can perform with the Event Hubs client library (\nAzure.Messaging.EventHubs\n) to interact with an Event Hubs.\nInspect Event Hubs\nMany Event Hubs operations take place within the scope of a specific partition. Because Event Hubs owns the partitions, their names are assigned at the time of creation. To understand what partitions are available, you query the Event Hubs using one of the Event Hubs clients. For illustration, the\nEventHubProducerClient\nis demonstrated in these examples, but the concept and form are common across clients.\nvar connectionString = \"<< CONNECTION STRING FOR THE EVENT HUBS NAMESPACE >>\";\nvar eventHubName = \"<< NAME OF THE EVENT HUB >>\";\n\nawait using (var producer = new EventHubProducerClient(connectionString, eventHubName))\n{\n    string[] partitionIds = await producer.GetPartitionIdsAsync();\n}\nPublish events to Event Hubs\nIn order to publish events, you need to create an\nEventHubProducerClient\n. Producers publish events in batches and might request a specific partition, or allow the Event Hubs service to decide which partition events should be published to. We recommended using automatic routing when the publishing of events needs to be highly available or when event data should be distributed evenly among the partitions. Our example takes advantage of automatic routing.\nvar connectionString = \"<< CONNECTION STRING FOR THE EVENT HUBS NAMESPACE >>\";\nvar eventHubName = \"<< NAME OF THE EVENT HUB >>\";\n\nawait using (var producer = new EventHubProducerClient(connectionString, eventHubName))\n{\n    using EventDataBatch eventBatch = await producer.CreateBatchAsync();\n    eventBatch.TryAdd(new EventData(new BinaryData(\"First\")));\n    eventBatch.TryAdd(new EventData(new BinaryData(\"Second\")));\n\n    await producer.SendAsync(eventBatch);\n}\nRead events from an Event Hubs\nIn order to read events from an Event Hubs, you need to create an\nEventHubConsumerClient\nfor a given consumer group. When an Event Hubs is created, it provides a default consumer group that can be used to get started with exploring Event Hubs. In our example, we focus on reading all events published to the Event Hubs using an iterator.\nNote\nIt is important to note that this approach to consuming is intended to improve the experience of exploring the Event Hubs client library and prototyping. It is recommended that it not be used in production scenarios. For production use, we recommend using the\nEvent Processor Client\n, as it provides a more robust and performant experience.\nvar connectionString = \"<< CONNECTION STRING FOR THE EVENT HUBS NAMESPACE >>\";\nvar eventHubName = \"<< NAME OF THE EVENT HUB >>\";\n\nstring consumerGroup = EventHubConsumerClient.DefaultConsumerGroupName;\n\nawait using (var consumer = new EventHubConsumerClient(consumerGroup, connectionString, eventHubName))\n{\n    using var cancellationSource = new CancellationTokenSource();\n    cancellationSource.CancelAfter(TimeSpan.FromSeconds(45));\n\n    await foreach (PartitionEvent receivedEvent in consumer.ReadEventsAsync(cancellationSource.Token))\n    {\n        // At this point, the loop will wait for events to be available in the Event Hub. When an event\n        // is available, the loop will iterate with the event that was received. Because we did not\n        // specify a maximum wait time, the loop will wait forever unless cancellation is requested using\n        // the cancellation token.\n    }\n}\nRead events from an Event Hubs partition\nTo read from a specific partition, the consumer needs to specify where in the event stream to begin receiving events. In our example, we focus on reading all published events for the first partition of the Event Hubs.\nvar connectionString = \"<< CONNECTION STRING FOR THE EVENT HUBS NAMESPACE >>\";\nvar eventHubName = \"<< NAME OF THE EVENT HUB >>\";\n\nstring consumerGroup = EventHubConsumerClient.DefaultConsumerGroupName;\n\nawait using (var consumer = new EventHubConsumerClient(consumerGroup, connectionString, eventHubName))\n{\n    EventPosition startingPosition = EventPosition.Earliest;\n    string partitionId = (await consumer.GetPartitionIdsAsync()).First();\n\n    using var cancellationSource = new CancellationTokenSource();\n    cancellationSource.CancelAfter(TimeSpan.FromSeconds(45));\n\n    await foreach (PartitionEvent receivedEvent in consumer.ReadEventsFromPartitionAsync(partitionId, startingPosition, cancellationSource.Token))\n    {\n        // At this point, the loop will wait for events to be available in the partition. When an event\n        // is available, the loop will iterate with the event that was received. Because we did not\n        // specify a maximum wait time, the loop will wait forever unless cancellation is requested using\n        // the cancellation token.\n    }\n}\nProcess events using an Event Processor client\nFor most production scenarios, the recommendation is to use\nEventProcessorClient\nfor reading and processing events. Since the\nEventProcessorClient\nhas a dependency on Azure Storage blobs for persistence of its state, you need to provide a\nBlobContainerClient\nfor the processor, which has been configured for the storage account and container that should be used.\nvar cancellationSource = new CancellationTokenSource();\ncancellationSource.CancelAfter(TimeSpan.FromSeconds(45));\n\nvar storageConnectionString = \"<< CONNECTION STRING FOR THE STORAGE ACCOUNT >>\";\nvar blobContainerName = \"<< NAME OF THE BLOB CONTAINER >>\";\n\nvar eventHubsConnectionString = \"<< CONNECTION STRING FOR THE EVENT HUBS NAMESPACE >>\";\nvar eventHubName = \"<< NAME OF THE EVENT HUB >>\";\nvar consumerGroup = \"<< NAME OF THE EVENT HUB CONSUMER GROUP >>\";\n\nTask processEventHandler(ProcessEventArgs eventArgs) => Task.CompletedTask;\nTask processErrorHandler(ProcessErrorEventArgs eventArgs) => Task.CompletedTask;\n\nvar storageClient = new BlobContainerClient(storageConnectionString, blobContainerName);\nvar processor = new EventProcessorClient(storageClient, consumerGroup, eventHubsConnectionString, eventHubName);\n\nprocessor.ProcessEventAsync += processEventHandler;\nprocessor.ProcessErrorAsync += processErrorHandler;\n\nawait processor.StartProcessingAsync();\n\ntry\n{\n    // The processor performs its work in the background; block until cancellation\n    // to allow processing to take place.\n\n    await Task.Delay(Timeout.Infinite, cancellationSource.Token);\n}\ncatch (TaskCanceledException)\n{\n    // This is expected when the delay is canceled.\n}\n\ntry\n{\n    await processor.StopProcessingAsync();\n}\nfinally\n{\n    // To prevent leaks, the handlers should be removed when processing is complete.\n\n    processor.ProcessEventAsync -= processEventHandler;\n    processor.ProcessErrorAsync -= processErrorHandler;\n}\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Perform common operations with the Event Hubs client library"
                },
                {
                  "level": 2,
                  "text": "Inspect Event Hubs"
                },
                {
                  "level": 2,
                  "text": "Publish events to Event Hubs"
                },
                {
                  "level": 2,
                  "text": "Read events from an Event Hubs"
                },
                {
                  "level": 2,
                  "text": "Read events from an Event Hubs partition"
                },
                {
                  "level": 2,
                  "text": "Process events using an Event Processor client"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [
                "Azure.Messaging.EventHubs",
                "EventHubProducerClient",
                "var connectionString = \"<< CONNECTION STRING FOR THE EVENT HUBS NAMESPACE >>\";\nvar eventHubName = \"<< NAME OF THE EVENT HUB >>\";\n\nawait using (var producer = new EventHubProducerClient(connectionString, eventHubName))\n{\n    string[] partitionIds = await producer.GetPartitionIdsAsync();\n}",
                "var connectionString = \"<< CONNECTION STRING FOR THE EVENT HUBS NAMESPACE >>\";\nvar eventHubName = \"<< NAME OF THE EVENT HUB >>\";\n\nawait using (var producer = new EventHubProducerClient(connectionString, eventHubName))\n{\n    string[] partitionIds = await producer.GetPartitionIdsAsync();\n}",
                "EventHubProducerClient",
                "var connectionString = \"<< CONNECTION STRING FOR THE EVENT HUBS NAMESPACE >>\";\nvar eventHubName = \"<< NAME OF THE EVENT HUB >>\";\n\nawait using (var producer = new EventHubProducerClient(connectionString, eventHubName))\n{\n    using EventDataBatch eventBatch = await producer.CreateBatchAsync();\n    eventBatch.TryAdd(new EventData(new BinaryData(\"First\")));\n    eventBatch.TryAdd(new EventData(new BinaryData(\"Second\")));\n\n    await producer.SendAsync(eventBatch);\n}",
                "var connectionString = \"<< CONNECTION STRING FOR THE EVENT HUBS NAMESPACE >>\";\nvar eventHubName = \"<< NAME OF THE EVENT HUB >>\";\n\nawait using (var producer = new EventHubProducerClient(connectionString, eventHubName))\n{\n    using EventDataBatch eventBatch = await producer.CreateBatchAsync();\n    eventBatch.TryAdd(new EventData(new BinaryData(\"First\")));\n    eventBatch.TryAdd(new EventData(new BinaryData(\"Second\")));\n\n    await producer.SendAsync(eventBatch);\n}",
                "EventHubConsumerClient",
                "var connectionString = \"<< CONNECTION STRING FOR THE EVENT HUBS NAMESPACE >>\";\nvar eventHubName = \"<< NAME OF THE EVENT HUB >>\";\n\nstring consumerGroup = EventHubConsumerClient.DefaultConsumerGroupName;\n\nawait using (var consumer = new EventHubConsumerClient(consumerGroup, connectionString, eventHubName))\n{\n    using var cancellationSource = new CancellationTokenSource();\n    cancellationSource.CancelAfter(TimeSpan.FromSeconds(45));\n\n    await foreach (PartitionEvent receivedEvent in consumer.ReadEventsAsync(cancellationSource.Token))\n    {\n        // At this point, the loop will wait for events to be available in the Event Hub. When an event\n        // is available, the loop will iterate with the event that was received. Because we did not\n        // specify a maximum wait time, the loop will wait forever unless cancellation is requested using\n        // the cancellation token.\n    }\n}",
                "var connectionString = \"<< CONNECTION STRING FOR THE EVENT HUBS NAMESPACE >>\";\nvar eventHubName = \"<< NAME OF THE EVENT HUB >>\";\n\nstring consumerGroup = EventHubConsumerClient.DefaultConsumerGroupName;\n\nawait using (var consumer = new EventHubConsumerClient(consumerGroup, connectionString, eventHubName))\n{\n    using var cancellationSource = new CancellationTokenSource();\n    cancellationSource.CancelAfter(TimeSpan.FromSeconds(45));\n\n    await foreach (PartitionEvent receivedEvent in consumer.ReadEventsAsync(cancellationSource.Token))\n    {\n        // At this point, the loop will wait for events to be available in the Event Hub. When an event\n        // is available, the loop will iterate with the event that was received. Because we did not\n        // specify a maximum wait time, the loop will wait forever unless cancellation is requested using\n        // the cancellation token.\n    }\n}",
                "var connectionString = \"<< CONNECTION STRING FOR THE EVENT HUBS NAMESPACE >>\";\nvar eventHubName = \"<< NAME OF THE EVENT HUB >>\";\n\nstring consumerGroup = EventHubConsumerClient.DefaultConsumerGroupName;\n\nawait using (var consumer = new EventHubConsumerClient(consumerGroup, connectionString, eventHubName))\n{\n    EventPosition startingPosition = EventPosition.Earliest;\n    string partitionId = (await consumer.GetPartitionIdsAsync()).First();\n\n    using var cancellationSource = new CancellationTokenSource();\n    cancellationSource.CancelAfter(TimeSpan.FromSeconds(45));\n\n    await foreach (PartitionEvent receivedEvent in consumer.ReadEventsFromPartitionAsync(partitionId, startingPosition, cancellationSource.Token))\n    {\n        // At this point, the loop will wait for events to be available in the partition. When an event\n        // is available, the loop will iterate with the event that was received. Because we did not\n        // specify a maximum wait time, the loop will wait forever unless cancellation is requested using\n        // the cancellation token.\n    }\n}",
                "var connectionString = \"<< CONNECTION STRING FOR THE EVENT HUBS NAMESPACE >>\";\nvar eventHubName = \"<< NAME OF THE EVENT HUB >>\";\n\nstring consumerGroup = EventHubConsumerClient.DefaultConsumerGroupName;\n\nawait using (var consumer = new EventHubConsumerClient(consumerGroup, connectionString, eventHubName))\n{\n    EventPosition startingPosition = EventPosition.Earliest;\n    string partitionId = (await consumer.GetPartitionIdsAsync()).First();\n\n    using var cancellationSource = new CancellationTokenSource();\n    cancellationSource.CancelAfter(TimeSpan.FromSeconds(45));\n\n    await foreach (PartitionEvent receivedEvent in consumer.ReadEventsFromPartitionAsync(partitionId, startingPosition, cancellationSource.Token))\n    {\n        // At this point, the loop will wait for events to be available in the partition. When an event\n        // is available, the loop will iterate with the event that was received. Because we did not\n        // specify a maximum wait time, the loop will wait forever unless cancellation is requested using\n        // the cancellation token.\n    }\n}",
                "EventProcessorClient",
                "EventProcessorClient",
                "BlobContainerClient",
                "var cancellationSource = new CancellationTokenSource();\ncancellationSource.CancelAfter(TimeSpan.FromSeconds(45));\n\nvar storageConnectionString = \"<< CONNECTION STRING FOR THE STORAGE ACCOUNT >>\";\nvar blobContainerName = \"<< NAME OF THE BLOB CONTAINER >>\";\n\nvar eventHubsConnectionString = \"<< CONNECTION STRING FOR THE EVENT HUBS NAMESPACE >>\";\nvar eventHubName = \"<< NAME OF THE EVENT HUB >>\";\nvar consumerGroup = \"<< NAME OF THE EVENT HUB CONSUMER GROUP >>\";\n\nTask processEventHandler(ProcessEventArgs eventArgs) => Task.CompletedTask;\nTask processErrorHandler(ProcessErrorEventArgs eventArgs) => Task.CompletedTask;\n\nvar storageClient = new BlobContainerClient(storageConnectionString, blobContainerName);\nvar processor = new EventProcessorClient(storageClient, consumerGroup, eventHubsConnectionString, eventHubName);\n\nprocessor.ProcessEventAsync += processEventHandler;\nprocessor.ProcessErrorAsync += processErrorHandler;\n\nawait processor.StartProcessingAsync();\n\ntry\n{\n    // The processor performs its work in the background; block until cancellation\n    // to allow processing to take place.\n\n    await Task.Delay(Timeout.Infinite, cancellationSource.Token);\n}\ncatch (TaskCanceledException)\n{\n    // This is expected when the delay is canceled.\n}\n\ntry\n{\n    await processor.StopProcessingAsync();\n}\nfinally\n{\n    // To prevent leaks, the handlers should be removed when processing is complete.\n\n    processor.ProcessEventAsync -= processEventHandler;\n    processor.ProcessErrorAsync -= processErrorHandler;\n}",
                "var cancellationSource = new CancellationTokenSource();\ncancellationSource.CancelAfter(TimeSpan.FromSeconds(45));\n\nvar storageConnectionString = \"<< CONNECTION STRING FOR THE STORAGE ACCOUNT >>\";\nvar blobContainerName = \"<< NAME OF THE BLOB CONTAINER >>\";\n\nvar eventHubsConnectionString = \"<< CONNECTION STRING FOR THE EVENT HUBS NAMESPACE >>\";\nvar eventHubName = \"<< NAME OF THE EVENT HUB >>\";\nvar consumerGroup = \"<< NAME OF THE EVENT HUB CONSUMER GROUP >>\";\n\nTask processEventHandler(ProcessEventArgs eventArgs) => Task.CompletedTask;\nTask processErrorHandler(ProcessErrorEventArgs eventArgs) => Task.CompletedTask;\n\nvar storageClient = new BlobContainerClient(storageConnectionString, blobContainerName);\nvar processor = new EventProcessorClient(storageClient, consumerGroup, eventHubsConnectionString, eventHubName);\n\nprocessor.ProcessEventAsync += processEventHandler;\nprocessor.ProcessErrorAsync += processErrorHandler;\n\nawait processor.StartProcessingAsync();\n\ntry\n{\n    // The processor performs its work in the background; block until cancellation\n    // to allow processing to take place.\n\n    await Task.Delay(Timeout.Infinite, cancellationSource.Token);\n}\ncatch (TaskCanceledException)\n{\n    // This is expected when the delay is canceled.\n}\n\ntry\n{\n    await processor.StopProcessingAsync();\n}\nfinally\n{\n    // To prevent leaks, the handlers should be removed when processing is complete.\n\n    processor.ProcessEventAsync -= processEventHandler;\n    processor.ProcessErrorAsync -= processErrorHandler;\n}"
              ],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                },
                {
                  "url": "https://github.com/Azure/azure-sdk-for-net/blob/main/sdk/eventhub/Azure.Messaging.EventHubs.Processor",
                  "text": "Event Processor Client"
                }
              ]
            },
            {
              "number": 7,
              "title": "Module assessment",
              "url": "https://learn.microsoft.com/en-us/training/modules/azure-event-hubs/7-knowledge-check",
              "href": "7-knowledge-check",
              "content": "Read in English\nAdd\nAdd to plan\nModule assessment\nCompleted\n3 minutes\n1.\nWhich of the following Event Hubs concepts represents an ordered sequence of events that is held in an Event Hubs?\nConsumer group\nPartition\nEvent Hubs producer\n2.\nWhich of the following options represents when an event processor marks or commits the position of the last successfully processed event within a partition?\nCheckpointing\nScale\nLoad balance\n3.\nWhat is a key advantage of using Microsoft Entra ID with Azure Event Hubs?\nIt allows storing credentials directly in the application code for easier access.\nIt eliminates the need for OAuth 2.0 tokens for authentication.\nIt removes the need to store credentials in the application code by using OAuth 2.0 tokens.\n4.\nWhat is the purpose of the EventHubProducerClient in the Azure Event Hubs client library?\nTo process events from an event hub using a consumer group.\nTo publish events to an event hub, either to specific partitions or using automatic routing.\nTo manage and monitor the partitions within an event hub.\nYou must answer all questions before checking your work.\nYou must answer all questions before checking your work.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Module assessment"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 8,
              "title": "Summary",
              "url": "https://learn.microsoft.com/en-us/training/modules/azure-event-hubs/8-summary",
              "href": "8-summary",
              "content": "Read in English\nAdd\nAdd to plan\nSummary\nCompleted\n3 minutes\nIn this module, you learned how to:\nDescribe the benefits of using Event Hubs and how it captures streaming data.\nExplain how to process events.\nPerform common operations with the Event Hubs client library.\nSend and retrieve events from Azure Event Hubs\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Summary"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "title": "Develop message-based solutions",
      "url": "https://learn.microsoft.com/en-us/training/paths/az-204-develop-message-based-solutions/",
      "learn_uid": "learn.wwl.az-204-develop-message-based-solutions",
      "modules": [
        {
          "title": "Discover Azure message queues",
          "url": "https://learn.microsoft.com/en-us/training/modules/discover-azure-message-queue/",
          "description": "",
          "learning_objectives": [],
          "prerequisites": [],
          "units": [
            {
              "number": 1,
              "title": "Introduction",
              "url": "https://learn.microsoft.com/en-us/training/modules/discover-azure-message-queue/1-introduction",
              "href": "1-introduction",
              "content": "Read in English\nAdd\nAdd to plan\nIntroduction\nCompleted\n3 minutes\nAzure supports two types of queue mechanisms:\nService Bus queues\nand\nStorage queues\n.\nService Bus queues are part of a broader Azure messaging infrastructure that supports queuing, publish/subscribe, and more advanced integration patterns. They're designed to integrate applications or application components that might span multiple communication protocols, data contracts, trust domains, or network environments.\nStorage queues are part of the Azure Storage infrastructure. They allow you to store large numbers of messages. You access messages from anywhere in the world via authenticated calls using HTTP or HTTPS. A queue might contain millions of messages, up to the total capacity limit of a storage account. Queues are commonly used to create a backlog of work to process asynchronously.\nAfter completing this module, you'll be able to:\nChoose the appropriate queue mechanism for your solution.\nExplain how the messaging entities that form the core capabilities of Service Bus operate.\nSend and receive message from a Service Bus queue by using .NET.\nIdentify the key components of Azure Queue Storage\nCreate queues and manage messages in Azure Queue Storage by using .NET.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Introduction"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 2,
              "title": "Choose a message queue solution",
              "url": "https://learn.microsoft.com/en-us/training/modules/discover-azure-message-queue/2-choose-queue-solution",
              "href": "2-choose-queue-solution",
              "content": "Read in English\nAdd\nAdd to plan\nChoose a message queue solution\nCompleted\n3 minutes\nStorage queues and Service Bus queues have a slightly different feature set. You can choose either one or both, depending on the needs of your particular solution.\nWhen determining which queuing technology fits the purpose of a given solution, solution architects and developers should consider these recommendations.\nConsider using Service Bus queues\nAs a solution architect/developer,\nyou should consider using Service Bus queues\nwhen:\nYour solution needs to receive messages without having to poll the queue. With Service Bus, you can achieve it by using a long-polling receive operation using the TCP-based protocols that Service Bus supports.\nYour solution requires the queue to provide a guaranteed first-in-first-out (FIFO) ordered delivery.\nYour solution needs to support automatic duplicate detection.\nYou want your application to process messages as parallel long-running streams (messages are associated with a stream using the\nsession ID\nproperty on the message). In this model, each node in the consuming application competes for streams, as opposed to messages. When a stream is given to a consuming node, the node can examine the state of the application stream state using transactions.\nYour solution requires transactional behavior and atomicity when sending or receiving multiple messages from a queue.\nYour application handles messages that can exceed 64 KB but won't likely approach the 256 KB or 1-MB limit, depending on the chosen service tier (although Service Bus queues can handle messages up to 100 MB).\nYou deal with a requirement to provide a role-based access model to the queues, and different rights/permissions for senders and receivers.\nConsider using Storage queues\nAs a solution architect/developer,\nyou should consider using Storage queues\nwhen:\nYour application must store over 80 gigabytes of messages in a queue.\nYour application wants to track progress for processing a message in the queue. It's useful if the worker processing a message crashes. Another worker can then use that information to continue from where the prior worker left off.\nYou require server side logs of all of the transactions executed against your queues.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Choose a message queue solution"
                },
                {
                  "level": 2,
                  "text": "Consider using Service Bus queues"
                },
                {
                  "level": 2,
                  "text": "Consider using Storage queues"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 3,
              "title": "Explore Azure Service Bus",
              "url": "https://learn.microsoft.com/en-us/training/modules/discover-azure-message-queue/3-azure-service-bus-overview",
              "href": "3-azure-service-bus-overview",
              "content": "Read in English\nAdd\nAdd to plan\nExplore Azure Service Bus\nCompleted\n3 minutes\nAzure Service Bus is a fully managed enterprise message broker with message queues and publish-subscribe topics. Service Bus is used to decouple applications and services. Data is transferred between different applications and services using\nmessages\n. A message is a container decorated with metadata, and contains data. The data can be any kind of information, including structured data encoded with the common formats such as the following ones: JSON, XML, Apache Avro, and Plain Text.\nSome common messaging scenarios are:\nMessaging\n. Transfer business data, such as sales or purchase orders, journals, or inventory movements.\nDecouple applications\n. Improve reliability and scalability of applications and services. Client and service don't have to be online at the same time.\nTopics and subscriptions\n. Enable 1:\nn\nrelationships between publishers and subscribers.\nMessage sessions\n. Implement workflows that require message ordering or message deferral.\nService Bus tiers\nService Bus offers basic, standard, and premium tiers. The\npremium\ntier of Service Bus Messaging addresses common customer requests around scale, performance, and availability for mission-critical applications. The premium tier is recommended for production scenarios. Although the feature sets are nearly identical, these two tiers of Service Bus Messaging are designed to serve different use cases. For more information on the available tiers, visit\nService Bus pricing\n.\nSome high-level differences between the premium and standard tiers are highlighted in the following table.\nPremium\nStandard\nHigh throughput\nVariable throughput\nPredictable performance\nVariable latency\nFixed pricing\nPay as you go variable pricing\nAbility to scale workload up and down\nN/A\nMessage size up to 100 MB\nMessage size up to 256 KB\nAdvanced features\nService Bus includes advanced features that enable you to solve more complex messaging problems. The following table describes several of these features.\nFeature\nDescription\nMessage sessions\nTo create a first-in, first-out (FIFO) guarantee in Service Bus, use sessions. Message sessions enable exclusive, ordered handling of unbounded sequences of related messages.\nAutoforwarding\nThe autoforwarding feature chains a queue or subscription to another queue or topic that is in the same namespace.\nDead-letter queue\nService Bus supports a dead-letter queue (DLQ). A DLQ holds messages that can't be delivered to any receiver. Service Bus lets you remove messages from the DLQ and inspect them.\nScheduled delivery\nYou can submit messages to a queue or topic for delayed processing. You can schedule a job to become available for processing by a system at a certain time.\nMessage deferral\nA queue or subscription client can defer retrieval of a message until a later time. The message remains in the queue or subscription, but is set aside.\nTransactions\nA transaction groups two or more operations together into an\nexecution scope\n. Service Bus supports grouping operations against a single messaging entity within the scope of a single transaction. A message entity can be a queue, topic, or subscription.\nFiltering and actions\nSubscribers can define which messages they want to receive from a topic. These messages are specified in the form of one or more named subscription rules.\nAutodelete on idle\nAutodelete on idle enables you to specify an idle interval after which a queue is automatically deleted. The minimum duration is 5 minutes.\nDuplicate detection\nAn error could cause the client to have a doubt about the outcome of a send operation. Duplicate detection enables the sender to resend the same message, or for the queue or topic to discard any duplicate copies.\nSecurity protocols\nService Bus supports security protocols such as Shared Access Signatures (SAS), Role Based Access Control (RBAC) and Managed identities for Azure resources.\nGeo-disaster recovery\nWhen Azure regions or datacenters experience downtime, Geo-disaster recovery enables data processing to continue operating in a different region or datacenter.\nSecurity\nService Bus supports standard AMQP 1.0 and HTTP/REST protocols.\nCompliance with standards and protocols\nThe primary wire protocol for Service Bus is\nAdvanced Messaging Queueing Protocol (AMQP) 1.0\n, an open ISO/IEC standard. It allows customers to write applications that work against Service Bus and on-premises brokers such as ActiveMQ or RabbitMQ. The\nAMQP protocol guide\nprovides detailed information in case you want to build such an abstraction.\nService Bus Premium is fully compliant with the Java/Jakarta EE\nJava Message Service (JMS) 2.0\nAPI.\nClient libraries\nFully supported Service Bus client libraries are available via the Azure SDK.\nAzure Service Bus for .NET\nAzure Service Bus libraries for Java\nAzure Service Bus provider for Java JMS 2.0\nAzure Service Bus Modules for JavaScript and TypeScript\nAzure Service Bus libraries for Python\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Explore Azure Service Bus"
                },
                {
                  "level": 2,
                  "text": "Service Bus tiers"
                },
                {
                  "level": 2,
                  "text": "Advanced features"
                },
                {
                  "level": 2,
                  "text": "Compliance with standards and protocols"
                },
                {
                  "level": 2,
                  "text": "Client libraries"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                },
                {
                  "url": "https://azure.microsoft.com/pricing/details/service-bus/",
                  "text": "Service Bus pricing"
                },
                {
                  "url": "/en-us/azure/service-bus-messaging/service-bus-amqp-overview",
                  "text": "Advanced Messaging Queueing Protocol (AMQP) 1.0"
                },
                {
                  "url": "/en-us/azure/service-bus-messaging/service-bus-amqp-protocol-guide",
                  "text": "AMQP protocol guide"
                },
                {
                  "url": "/en-us/azure/service-bus-messaging/how-to-use-java-message-service-20",
                  "text": "Java Message Service (JMS) 2.0"
                },
                {
                  "url": "/en-us/dotnet/api/overview/azure/service-bus",
                  "text": "Azure Service Bus for .NET"
                },
                {
                  "url": "/en-us/java/api/overview/azure/servicebus",
                  "text": "Azure Service Bus libraries for Java"
                },
                {
                  "url": "/en-us/azure/service-bus-messaging/how-to-use-java-message-service-20",
                  "text": "Azure Service Bus provider for Java JMS 2.0"
                },
                {
                  "url": "/en-us/javascript/api/overview/azure/service-bus",
                  "text": "Azure Service Bus Modules for JavaScript and TypeScript"
                },
                {
                  "url": "/en-us/python/api/overview/azure/servicebus",
                  "text": "Azure Service Bus libraries for Python"
                }
              ]
            },
            {
              "number": 4,
              "title": "Discover Service Bus queues, topics, and subscriptions",
              "url": "https://learn.microsoft.com/en-us/training/modules/discover-azure-message-queue/4-queues-topics-subscriptions",
              "href": "4-queues-topics-subscriptions",
              "content": "Read in English\nAdd\nAdd to plan\nDiscover Service Bus queues, topics, and subscriptions\nCompleted\n3 minutes\nThe messaging entities that form the core of the messaging capabilities in Service Bus are\nqueues\n,\ntopics and subscriptions\n, and rules/actions.\nQueues\nQueues offer\nFirst In, First Out\n(FIFO) message delivery to one or more competing consumers. That is, receivers typically receive and process messages in the order in which they were added to the queue. And, only one message consumer receives and processes each message. Because messages are stored durably in the queue, producers (senders) and consumers (receivers) don't have to process messages concurrently.\nA related benefit is\nload-leveling\n, which enables producers and consumers to send and receive messages at different rates. In many applications, the system load varies over time. However, the processing time required for each unit of work is typically constant. Intermediating message producers and consumers with a queue means that the consuming application only has to be able to handle average load instead of peak load.\nUsing queues to intermediate between message producers and consumers provides an inherent loose coupling between the components. Because producers and consumers aren't aware of each other, a consumer can be upgraded without having any effect on the producer.\nYou can create queues using the Azure portal, PowerShell, CLI, or Resource Manager templates. Then, send and receive messages using clients written in C#, Java, Python, and JavaScript.\nReceive modes\nYou can specify two different modes in which Service Bus receives messages:\nReceive and delete\nor\nPeek lock\n.\nReceive and delete\nIn this mode, when Service Bus receives the request from the consumer, it marks the message as consumed and returns it to the consumer application. This mode is the simplest model. It works best for scenarios in which the application can tolerate not processing a message if a failure occurs. For example, consider a scenario in which the consumer issues the receive request and then crashes before processing it. As Service Bus marks the message as consumed, the application begins consuming messages upon restart. It misses the message that it consumed before the crash.\nPeek lock\nIn this mode, the receive operation becomes two-stage, which makes it possible to support applications that can't tolerate missing messages.\nFinds the next message to be consumed,\nlocks\nit to prevent other consumers from receiving it, and then, return the message to the application.\nAfter the application finishes processing the message, it requests the Service Bus service to complete the second stage of the receive process. Then, the service\nmarks the message as consumed\n.\nIf the application is unable to process the message for some reason, it can request the Service Bus service to\nabandon\nthe message. Service Bus\nunlocks\nthe message and makes it available to be received again, either by the same consumer or by another competing consumer. Secondly, there's a\ntimeout\nassociated with the lock. If the application fails to process the message before the lock timeout expires, Service Bus unlocks the message and makes it available to be received again.\nTopics and subscriptions\nA queue allows processing of a message by a single consumer. In contrast to queues, topics and subscriptions provide a one-to-many form of communication in a\npublish and subscribe\npattern. It's useful for scaling to large numbers of recipients. Each published message is made available to each subscription registered with the topic. Publisher sends a message to a topic and one or more subscribers receive a copy of the message.\nThe subscriptions can use more filters to restrict the messages that they want to receive. Publishers send messages to a topic in the same way that they send messages to a queue. But, consumers don't receive messages directly from the topic. Instead, consumers receive messages from subscriptions of the topic. A topic subscription resembles a virtual queue that receives copies of the messages that are sent to the topic. Consumers receive messages from a subscription identically to the way they receive messages from a queue.\nThe message-sending functionality of a queue maps directly to a topic and its message-receiving functionality maps to a subscription. Among other things, this feature means that subscriptions support the same patterns described earlier in this section regarding queues: competing consumer, temporal decoupling, load leveling, and load balancing.\nRules and actions\nIn many scenarios, messages that have specific characteristics must be processed in different ways. To enable this processing, you can configure subscriptions to find messages that have desired properties and then perform certain modifications to those properties. While Service Bus subscriptions see all messages sent to the topic, you can only copy a subset of those messages to the virtual subscription queue. This filtering is accomplished using subscription filters. Such modifications are called\nfilter actions\n. When a subscription is created, you can supply a filter expression that operates on the properties of the message. The properties can be both the system properties (for example,\nLabel\n) and custom application properties (for example,\nStoreName\n.) The SQL filter expression is optional in this case. Without a SQL filter expression, any filter action defined on a subscription is performed on all the messages for that subscription.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Discover Service Bus queues, topics, and subscriptions"
                },
                {
                  "level": 2,
                  "text": "Queues"
                },
                {
                  "level": 2,
                  "text": "Receive modes"
                },
                {
                  "level": 3,
                  "text": "Receive and delete"
                },
                {
                  "level": 3,
                  "text": "Peek lock"
                },
                {
                  "level": 2,
                  "text": "Topics and subscriptions"
                },
                {
                  "level": 3,
                  "text": "Rules and actions"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 5,
              "title": "Explore Service Bus message payloads and serialization",
              "url": "https://learn.microsoft.com/en-us/training/modules/discover-azure-message-queue/5-messages-payloads-serialization",
              "href": "5-messages-payloads-serialization",
              "content": "Read in English\nAdd\nAdd to plan\nExplore Service Bus message payloads and serialization\nCompleted\n3 minutes\nMessages carry a payload and metadata. The metadata is in the form of key-value pair properties, and describes the payload, and gives handling instructions to Service Bus and applications. Occasionally, that metadata alone is sufficient to carry the information that the sender wants to communicate to receivers, and the payload remains empty.\nThe object model of the official Service Bus clients for .NET and Java maps to and from the wire protocols Service Bus supports.\nA Service Bus message consists of a binary payload section that Service Bus never handles in any form on the service-side, and two sets of properties. The\nbroker properties\nare system defined. These predefined properties either control message-level functionality inside the broker, or they map to common and standardized metadata items. The\nuser properties\nare a collection of key-value pairs defined and set by the application.\nMessage routing and correlation\nA subset of the broker properties, specifically\nTo\n,\nReplyTo\n,\nReplyToSessionId\n,\nMessageId\n,\nCorrelationId\n, and\nSessionId\n, help applications route messages to particular destinations. The following patterns describe the routing:\nSimple request/reply:\nA publisher sends a message into a queue and expects a reply from the message consumer. The publisher owns a queue to receive the replies. The address of that queue is contained in the\nReplyTo\nproperty of the outbound message. When the consumer responds, it copies the\nMessageId\nof the handled message into the\nCorrelationId\nproperty of the reply message and delivers the message to the destination indicated by the\nReplyTo\nproperty. One message can yield multiple replies, depending on the application context.\nMulticast request/reply:\nAs a variation of the prior pattern, a publisher sends the message into a topic and multiple subscribers become eligible to consume the message. Each of the subscribers might respond in the fashion described previously. If\nReplyTo\npoints to a topic, such a set of discovery responses can be distributed to an audience.\nMultiplexing:\nThis session feature enables multiplexing of streams of related messages through a single queue or subscription such that each session (or group) of related messages, identified by matching\nSessionId\nvalues, are routed to a specific receiver while the receiver holds the session under lock. Learn more about the details of sessions\nhere\n.\nMultiplexed request/reply:\nThis session feature enables multiplexed replies, allowing several publishers to share a reply queue. By setting\nReplyToSessionId\n, the publisher can instruct one or more consumers to copy that value into the\nSessionId\nproperty of the reply message. The publishing queue or topic doesn't need to be session-aware. When the message is sent the publisher can wait for a session with the given\nSessionId\nto materialize on the queue by conditionally accepting a session receiver.\nPayload serialization\nWhen in transit or stored inside of Service Bus, the payload is always an opaque, binary block. The\nContentType\nproperty enables applications to describe the payload, with the suggested format for the property values being a MIME content-type description according to IETF RFC2045; for example,\napplication/json;charset=utf-8\n.\nUnlike the Java or .NET Standard variants, the .NET Framework version of the Service Bus API supports creating\nBrokeredMessage\ninstances by passing arbitrary .NET objects into the constructor.\nThe legacy SBMP protocol serializes objects with the default binary serializer, or with a serializer that is externally supplied. The AMQP protocol serializes objects into an AMQP object. The receiver can retrieve those objects with the\nGetBody<T>()\nmethod, supplying the expected type. With AMQP, the objects are serialized into an AMQP graph of\nArrayList\nand\nIDictionary<string,object>\nobjects, and any AMQP client can decode them.\nWhile this hidden serialization magic is convenient, if applications should take explicit control of object serialization and turn their object graphs into streams before including them into a message, they should do the reverse operation on the receiver side. While AMQP has a powerful binary encoding model, it's tied to the AMQP messaging ecosystem and HTTP clients have trouble decoding such payloads.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Explore Service Bus message payloads and serialization"
                },
                {
                  "level": 2,
                  "text": "Message routing and correlation"
                },
                {
                  "level": 2,
                  "text": "Payload serialization"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [
                "ReplyToSessionId",
                "CorrelationId",
                "CorrelationId",
                "ReplyToSessionId",
                "ContentType",
                "application/json;charset=utf-8",
                "BrokeredMessage",
                "GetBody<T>()",
                "IDictionary<string,object>"
              ],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                },
                {
                  "url": "/en-us/azure/service-bus-messaging/message-sessions",
                  "text": "here"
                }
              ]
            },
            {
              "number": 6,
              "title": "Exercise - Send and receive messages from Azure Service Bus",
              "url": "https://learn.microsoft.com/en-us/training/modules/discover-azure-message-queue/6-send-receive-messages-service-bus",
              "href": "6-send-receive-messages-service-bus",
              "content": "Read in English\nAdd\nAdd to plan\nExercise - Send and receive messages from Azure Service Bus\nCompleted\n30 minutes\nIn this exercise, you create and configure Azure Service Bus resources, then build a .NET app to send and receive messages using the\nAzure.Messaging.ServiceBus\nSDK. You learn how to provision a Service Bus namespace and queue, assign permissions, and interact with messages programmatically.\nTasks performed in this exercise:\nCreate Azure Service Bus resources\nAssign a role to your Microsoft Entra user name\nCreate a .NET console app to send and receive messages\nClean up resources\nThis exercise takes approximately\n30\nminutes to complete.\nBefore you start\nTo complete the exercise, you need:\nAn Azure subscription. If you don't already have one, you can sign up for one\nhttps://azure.microsoft.com/\n.\nGet started\nSelect the\nLaunch Exercise\nbutton to open the exercise instructions in a new browser window. When you're finished with the exercise, return here to:\nComplete the module\nEarn a badge for completing this module\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Exercise - Send and receive messages from Azure Service Bus"
                },
                {
                  "level": 2,
                  "text": "Before you start"
                },
                {
                  "level": 2,
                  "text": "Get started"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [
                {
                  "src": "../../wwl-azure/discover-azure-message-queue/media/launch-exercise.png",
                  "absolute_url": "https://learn.microsoft.com/en-us/wwl-azure/discover-azure-message-queue/media/launch-exercise.png",
                  "alt_text": "Button to launch exercise.",
                  "title": "",
                  "filename": "launch-exercise.png",
                  "image_type": "icon",
                  "context": {
                    "preceding_heading": "Get started",
                    "following_text": "Was this page helpful?",
                    "figure_caption": "",
                    "parent_section": ""
                  }
                }
              ],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                },
                {
                  "url": "https://azure.microsoft.com/",
                  "text": "https://azure.microsoft.com/"
                }
              ]
            },
            {
              "number": 7,
              "title": "Explore Azure Queue Storage",
              "url": "https://learn.microsoft.com/en-us/training/modules/discover-azure-message-queue/7-azure-queue-storage-overview",
              "href": "7-azure-queue-storage-overview",
              "content": "Read in English\nAdd\nAdd to plan\nExplore Azure Queue Storage\nCompleted\n3 minutes\nAzure Queue Storage is a service for storing large numbers of messages. You access messages from anywhere in the world via authenticated calls using HTTP or HTTPS. A queue message can be up to 64 KB in size. A queue might contain millions of messages, up to the total capacity limit of a storage account. Queues are commonly used to create a backlog of work to process asynchronously.\nThe Queue service contains the following components:\nURL format:\nQueues are addressable using the URL format\nhttps://<storage account>.queue.core.windows.net/<queue>\n. For example, the following URL addresses a queue in the diagram above\nhttps://myaccount.queue.core.windows.net/images-to-download\nStorage account:\nAll access to Azure Storage is done through a storage account.\nQueue:\nA queue contains a set of messages. All messages must be in a queue. The queue name must be all lowercase.\nMessage:\nA message, in any format, of up to 64 KB. Before version 2017-07-29, the maximum time-to-live allowed is seven days. For version 2017-07-29 or later, the maximum time-to-live can be any positive number, or -1 indicating that the message doesn't expire. If this parameter is omitted, the default time-to-live is seven days.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Explore Azure Queue Storage"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [
                "https://<storage account>.queue.core.windows.net/<queue>",
                "https://myaccount.queue.core.windows.net/images-to-download"
              ],
              "images": [
                {
                  "src": "../../wwl-azure/discover-azure-message-queue/media/queue-storage-service-components.png",
                  "absolute_url": "https://learn.microsoft.com/en-us/wwl-azure/discover-azure-message-queue/media/queue-storage-service-components.png",
                  "alt_text": "Image showing components of the queue service",
                  "title": "",
                  "filename": "queue-storage-service-components.png",
                  "image_type": "illustration",
                  "context": {
                    "preceding_heading": "Explore Azure Queue Storage",
                    "following_text": "URL format:Queues are addressable using the URL formathttps://<storage account>.queue.core.windows.net/<queue>. For example, the following URL addresses a queue in the diagram abovehttps://myaccount.q",
                    "figure_caption": "",
                    "parent_section": ""
                  }
                }
              ],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 8,
              "title": "Create and manage Azure Queue Storage and messages by using .NET",
              "url": "https://learn.microsoft.com/en-us/training/modules/discover-azure-message-queue/8-queue-storage-code-examples",
              "href": "8-queue-storage-code-examples",
              "content": "Read in English\nAdd\nAdd to plan\nCreate and manage Azure Queue Storage and messages by using .NET\nCompleted\n3 minutes\nIn this unit we're covering how to create queues and manage messages in Azure Queue Storage by showing code snippets from a .NET project.\nThe code examples rely on the following NuGet packages:\nAzure.Core library for .NET\n: This package provides shared primitives, abstractions, and helpers for modern .NET Azure SDK client libraries.\nAzure.Storage.Common client library for .NET\n: This package provides infrastructure shared by the other Azure Storage client libraries.\nAzure.Storage.Queues client library for .NET\n: This package enables working with Azure Queue Storage for storing messages that accessed by a client.\nSystem.Configuration.ConfigurationManager library for .NET\n: This package provides access to configuration files for client applications.\nCreate the Queue service client\nThe\nQueueClient\nclass enables you to retrieve queues stored in Queue storage. Here's one way to create the service client:\nQueueClient queueClient = new QueueClient(connectionString, queueName);\nCreate a queue\nThis example shows how to create a queue if it doesn't already exist:\n// Get the connection string from app settings\nstring connectionString = ConfigurationManager.AppSettings[\"StorageConnectionString\"];\n\n// Instantiate a QueueClient which will be used to create and manipulate the queue\nQueueClient queueClient = new QueueClient(connectionString, queueName);\n\n// Create the queue\nqueueClient.CreateIfNotExists();\nInsert a message into a queue\nTo insert a message into an existing queue, call the\nSendMessage\nmethod. A message can be either a string (in UTF-8 format) or a byte array. The following code creates a queue (if it doesn't exist) and inserts a message:\n// Get the connection string from app settings\nstring connectionString = ConfigurationManager.AppSettings[\"StorageConnectionString\"];\n\n// Instantiate a QueueClient which will be used to create and manipulate the queue\nQueueClient queueClient = new QueueClient(connectionString, queueName);\n\n// Create the queue if it doesn't already exist\nqueueClient.CreateIfNotExists();\n\nif (queueClient.Exists())\n{\n    // Send a message to the queue\n    queueClient.SendMessage(message);\n}\nPeek at the next message\nYou can peek at the messages in the queue without removing them from the queue by calling the\nPeekMessages\nmethod. If you don't pass a value for the\nmaxMessages\nparameter, the default is to peek at one message.\n// Get the connection string from app settings\nstring connectionString = ConfigurationManager.AppSettings[\"StorageConnectionString\"];\n\n// Instantiate a QueueClient which will be used to manipulate the queue\nQueueClient queueClient = new QueueClient(connectionString, queueName);\n\nif (queueClient.Exists())\n{ \n    // Peek at the next message\n    PeekedMessage[] peekedMessage = queueClient.PeekMessages();\n}\nChange the contents of a queued message\nYou can change the contents of a message in-place in the queue. If the message represents a work task, you could use this feature to update the status of the work task. The following code updates the queue message with new contents, and sets the visibility timeout to extend another 60 seconds. This saves the state of work associated with the message, and gives the client another minute to continue working on the message.\n// Get the connection string from app settings\nstring connectionString = ConfigurationManager.AppSettings[\"StorageConnectionString\"];\n\n// Instantiate a QueueClient which will be used to manipulate the queue\nQueueClient queueClient = new QueueClient(connectionString, queueName);\n\nif (queueClient.Exists())\n{\n    // Get the message from the queue\n    QueueMessage[] message = queueClient.ReceiveMessages();\n\n    // Update the message contents\n    queueClient.UpdateMessage(message[0].MessageId, \n            message[0].PopReceipt, \n            \"Updated contents\",\n            TimeSpan.FromSeconds(60.0)  // Make it invisible for another 60 seconds\n        );\n}\nDequeue the next message\nDequeue a message from a queue in two steps. When you call\nReceiveMessages\n, you get the next message in a queue. A message returned from\nReceiveMessages\nbecomes invisible to any other code reading messages from this queue. By default, this message stays invisible for 30 seconds. To finish removing the message from the queue, you must also call\nDeleteMessage\n. This two-step process of removing a message assures that if your code fails to process a message due to hardware or software failure, another instance of your code can get the same message and try again. Your code calls\nDeleteMessage\nright after the message has been processed.\n// Get the connection string from app settings\nstring connectionString = ConfigurationManager.AppSettings[\"StorageConnectionString\"];\n\n// Instantiate a QueueClient which will be used to manipulate the queue\nQueueClient queueClient = new QueueClient(connectionString, queueName);\n\nif (queueClient.Exists())\n{\n    // Get the next message\n    QueueMessage[] retrievedMessage = queueClient.ReceiveMessages();\n\n    // Process (i.e. print) the message in less than 30 seconds\n    Console.WriteLine($\"Dequeued message: '{retrievedMessage[0].Body}'\");\n\n    // Delete the message\n    queueClient.DeleteMessage(retrievedMessage[0].MessageId, retrievedMessage[0].PopReceipt);\n}\nGet the queue length\nYou can get an estimate of the number of messages in a queue. The\nGetProperties\nmethod returns queue properties including the message count. The\nApproximateMessagesCount\nproperty contains the approximate number of messages in the queue. This number isn't lower than the actual number of messages in the queue, but could be higher.\n/// Instantiate a QueueClient which will be used to manipulate the queue\nQueueClient queueClient = new QueueClient(connectionString, queueName);\n\nif (queueClient.Exists())\n{\n    QueueProperties properties = queueClient.GetProperties();\n\n    // Retrieve the cached approximate message count.\n    int cachedMessagesCount = properties.ApproximateMessagesCount;\n\n    // Display number of messages.\n    Console.WriteLine($\"Number of messages in queue: {cachedMessagesCount}\");\n}\nDelete a queue\nTo delete a queue and all the messages contained in it, call the\nDelete\nmethod on the queue object.\n/// Get the connection string from app settings\nstring connectionString = ConfigurationManager.AppSettings[\"StorageConnectionString\"];\n\n// Instantiate a QueueClient which will be used to manipulate the queue\nQueueClient queueClient = new QueueClient(connectionString, queueName);\n\nif (queueClient.Exists())\n{\n    // Delete the queue\n    queueClient.Delete();\n}\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Create and manage Azure Queue Storage and messages by using .NET"
                },
                {
                  "level": 2,
                  "text": "Create the Queue service client"
                },
                {
                  "level": 2,
                  "text": "Create a queue"
                },
                {
                  "level": 2,
                  "text": "Insert a message into a queue"
                },
                {
                  "level": 2,
                  "text": "Peek at the next message"
                },
                {
                  "level": 2,
                  "text": "Change the contents of a queued message"
                },
                {
                  "level": 2,
                  "text": "Dequeue the next message"
                },
                {
                  "level": 2,
                  "text": "Get the queue length"
                },
                {
                  "level": 2,
                  "text": "Delete a queue"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [
                "QueueClient",
                "QueueClient queueClient = new QueueClient(connectionString, queueName);",
                "QueueClient queueClient = new QueueClient(connectionString, queueName);",
                "// Get the connection string from app settings\nstring connectionString = ConfigurationManager.AppSettings[\"StorageConnectionString\"];\n\n// Instantiate a QueueClient which will be used to create and manipulate the queue\nQueueClient queueClient = new QueueClient(connectionString, queueName);\n\n// Create the queue\nqueueClient.CreateIfNotExists();",
                "// Get the connection string from app settings\nstring connectionString = ConfigurationManager.AppSettings[\"StorageConnectionString\"];\n\n// Instantiate a QueueClient which will be used to create and manipulate the queue\nQueueClient queueClient = new QueueClient(connectionString, queueName);\n\n// Create the queue\nqueueClient.CreateIfNotExists();",
                "SendMessage",
                "// Get the connection string from app settings\nstring connectionString = ConfigurationManager.AppSettings[\"StorageConnectionString\"];\n\n// Instantiate a QueueClient which will be used to create and manipulate the queue\nQueueClient queueClient = new QueueClient(connectionString, queueName);\n\n// Create the queue if it doesn't already exist\nqueueClient.CreateIfNotExists();\n\nif (queueClient.Exists())\n{\n    // Send a message to the queue\n    queueClient.SendMessage(message);\n}",
                "// Get the connection string from app settings\nstring connectionString = ConfigurationManager.AppSettings[\"StorageConnectionString\"];\n\n// Instantiate a QueueClient which will be used to create and manipulate the queue\nQueueClient queueClient = new QueueClient(connectionString, queueName);\n\n// Create the queue if it doesn't already exist\nqueueClient.CreateIfNotExists();\n\nif (queueClient.Exists())\n{\n    // Send a message to the queue\n    queueClient.SendMessage(message);\n}",
                "PeekMessages",
                "maxMessages",
                "// Get the connection string from app settings\nstring connectionString = ConfigurationManager.AppSettings[\"StorageConnectionString\"];\n\n// Instantiate a QueueClient which will be used to manipulate the queue\nQueueClient queueClient = new QueueClient(connectionString, queueName);\n\nif (queueClient.Exists())\n{ \n    // Peek at the next message\n    PeekedMessage[] peekedMessage = queueClient.PeekMessages();\n}",
                "// Get the connection string from app settings\nstring connectionString = ConfigurationManager.AppSettings[\"StorageConnectionString\"];\n\n// Instantiate a QueueClient which will be used to manipulate the queue\nQueueClient queueClient = new QueueClient(connectionString, queueName);\n\nif (queueClient.Exists())\n{ \n    // Peek at the next message\n    PeekedMessage[] peekedMessage = queueClient.PeekMessages();\n}",
                "// Get the connection string from app settings\nstring connectionString = ConfigurationManager.AppSettings[\"StorageConnectionString\"];\n\n// Instantiate a QueueClient which will be used to manipulate the queue\nQueueClient queueClient = new QueueClient(connectionString, queueName);\n\nif (queueClient.Exists())\n{\n    // Get the message from the queue\n    QueueMessage[] message = queueClient.ReceiveMessages();\n\n    // Update the message contents\n    queueClient.UpdateMessage(message[0].MessageId, \n            message[0].PopReceipt, \n            \"Updated contents\",\n            TimeSpan.FromSeconds(60.0)  // Make it invisible for another 60 seconds\n        );\n}",
                "// Get the connection string from app settings\nstring connectionString = ConfigurationManager.AppSettings[\"StorageConnectionString\"];\n\n// Instantiate a QueueClient which will be used to manipulate the queue\nQueueClient queueClient = new QueueClient(connectionString, queueName);\n\nif (queueClient.Exists())\n{\n    // Get the message from the queue\n    QueueMessage[] message = queueClient.ReceiveMessages();\n\n    // Update the message contents\n    queueClient.UpdateMessage(message[0].MessageId, \n            message[0].PopReceipt, \n            \"Updated contents\",\n            TimeSpan.FromSeconds(60.0)  // Make it invisible for another 60 seconds\n        );\n}",
                "ReceiveMessages",
                "ReceiveMessages",
                "DeleteMessage",
                "DeleteMessage",
                "// Get the connection string from app settings\nstring connectionString = ConfigurationManager.AppSettings[\"StorageConnectionString\"];\n\n// Instantiate a QueueClient which will be used to manipulate the queue\nQueueClient queueClient = new QueueClient(connectionString, queueName);\n\nif (queueClient.Exists())\n{\n    // Get the next message\n    QueueMessage[] retrievedMessage = queueClient.ReceiveMessages();\n\n    // Process (i.e. print) the message in less than 30 seconds\n    Console.WriteLine($\"Dequeued message: '{retrievedMessage[0].Body}'\");\n\n    // Delete the message\n    queueClient.DeleteMessage(retrievedMessage[0].MessageId, retrievedMessage[0].PopReceipt);\n}",
                "// Get the connection string from app settings\nstring connectionString = ConfigurationManager.AppSettings[\"StorageConnectionString\"];\n\n// Instantiate a QueueClient which will be used to manipulate the queue\nQueueClient queueClient = new QueueClient(connectionString, queueName);\n\nif (queueClient.Exists())\n{\n    // Get the next message\n    QueueMessage[] retrievedMessage = queueClient.ReceiveMessages();\n\n    // Process (i.e. print) the message in less than 30 seconds\n    Console.WriteLine($\"Dequeued message: '{retrievedMessage[0].Body}'\");\n\n    // Delete the message\n    queueClient.DeleteMessage(retrievedMessage[0].MessageId, retrievedMessage[0].PopReceipt);\n}",
                "GetProperties",
                "ApproximateMessagesCount",
                "/// Instantiate a QueueClient which will be used to manipulate the queue\nQueueClient queueClient = new QueueClient(connectionString, queueName);\n\nif (queueClient.Exists())\n{\n    QueueProperties properties = queueClient.GetProperties();\n\n    // Retrieve the cached approximate message count.\n    int cachedMessagesCount = properties.ApproximateMessagesCount;\n\n    // Display number of messages.\n    Console.WriteLine($\"Number of messages in queue: {cachedMessagesCount}\");\n}",
                "/// Instantiate a QueueClient which will be used to manipulate the queue\nQueueClient queueClient = new QueueClient(connectionString, queueName);\n\nif (queueClient.Exists())\n{\n    QueueProperties properties = queueClient.GetProperties();\n\n    // Retrieve the cached approximate message count.\n    int cachedMessagesCount = properties.ApproximateMessagesCount;\n\n    // Display number of messages.\n    Console.WriteLine($\"Number of messages in queue: {cachedMessagesCount}\");\n}",
                "/// Get the connection string from app settings\nstring connectionString = ConfigurationManager.AppSettings[\"StorageConnectionString\"];\n\n// Instantiate a QueueClient which will be used to manipulate the queue\nQueueClient queueClient = new QueueClient(connectionString, queueName);\n\nif (queueClient.Exists())\n{\n    // Delete the queue\n    queueClient.Delete();\n}",
                "/// Get the connection string from app settings\nstring connectionString = ConfigurationManager.AppSettings[\"StorageConnectionString\"];\n\n// Instantiate a QueueClient which will be used to manipulate the queue\nQueueClient queueClient = new QueueClient(connectionString, queueName);\n\nif (queueClient.Exists())\n{\n    // Delete the queue\n    queueClient.Delete();\n}"
              ],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                },
                {
                  "url": "https://www.nuget.org/packages/azure.core/",
                  "text": "Azure.Core library for .NET"
                },
                {
                  "url": "https://www.nuget.org/packages/azure.storage.common/",
                  "text": "Azure.Storage.Common client library for .NET"
                },
                {
                  "url": "https://www.nuget.org/packages/azure.storage.queues/",
                  "text": "Azure.Storage.Queues client library for .NET"
                },
                {
                  "url": "https://www.nuget.org/packages/system.configuration.configurationmanager/",
                  "text": "System.Configuration.ConfigurationManager library for .NET"
                }
              ]
            },
            {
              "number": 9,
              "title": "Module assessment",
              "url": "https://learn.microsoft.com/en-us/training/modules/discover-azure-message-queue/9-knowledge-check",
              "href": "9-knowledge-check",
              "content": "Read in English\nAdd\nAdd to plan\nModule assessment\nCompleted\n3 minutes\n1.\nWhat is a key consideration when choosing to use Service Bus queues over Storage queues?\nYour solution requires the queue to provide a guaranteed first-in-first-out (FIFO) ordered delivery.\nYour application must store over 80 gigabytes of messages in a queue.\nYou require server side logs of all of the transactions executed against your queues.\n2.\nWhat is the main difference between Service Bus queues and topics with subscriptions?\nQueues allow processing of a message by a single consumer, while topics with subscriptions provide a one-to-many form of communication.\nQueues allow processing of a message by multiple consumers, while topics with subscriptions provide a one-to-one form of communication.\nTopics with subscriptions allow processing of a message by a single consumer, while queues provide a one-to-many form of communication.\n3.\nWhat is the role of the\nContentType\nproperty in Service Bus message payloads?\nIt encrypts the payload for secure transmission.\nIt determines the size of the payload.\nIt enables applications to describe the payload, with the suggested format for the property values being a MIME content-type description.\n4.\nWhat is the purpose of the 'QueueClient' class in Azure Queue Storage when using .NET?\nIt manages the configuration files for client applications.\nIt retrieves and manipulates queues stored in Azure Queue Storage.\nIt creates and manage messages within a specific queue.\nYou must answer all questions before checking your work.\nYou must answer all questions before checking your work.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Module assessment"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [
                "ContentType"
              ],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            },
            {
              "number": 10,
              "title": "Summary",
              "url": "https://learn.microsoft.com/en-us/training/modules/discover-azure-message-queue/10-summary",
              "href": "10-summary",
              "content": "Read in English\nAdd\nAdd to plan\nSummary\nCompleted\n3 minutes\nIn this module, you learned how to:\nChoose the appropriate queue mechanism for your solution.\nExplain how the messaging entities that form the core capabilities of Service Bus operate.\nSend and receive message from a Service Bus queue by using .NET.\nIdentify the key components of Azure Queue Storage\nCreate queues and manage messages in Azure Queue Storage by using .NET.\nFeedback\nWas this page helpful?\nYes\nNo",
              "headings": [
                {
                  "level": 1,
                  "text": "Summary"
                },
                {
                  "level": 2,
                  "text": "Feedback"
                }
              ],
              "code_blocks": [],
              "images": [],
              "links": [
                {
                  "url": "#",
                  "text": "Read in English"
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "title": "Az 204 Instrument Solutions To Support Monitoring Logging",
      "url": "https://learn.microsoft.com/en-us/training/paths/az-204-instrument-solutions-to-support-monitoring-logging/",
      "learn_uid": "learn.wwl.az-204-instrument-solutions-to-support-monitoring-logging"
    }
  ]
}